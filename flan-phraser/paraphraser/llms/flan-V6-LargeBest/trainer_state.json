{
  "best_metric": 0.7422646284103394,
  "best_model_checkpoint": "/kaggle/working/flan-peft-train-V3/checkpoint-10970",
  "epoch": 4.999430329269682,
  "eval_steps": 500,
  "global_step": 10970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002278682921271505,
      "grad_norm": 0.08369752764701843,
      "learning_rate": 0.0009995442114858707,
      "loss": 1.1366,
      "step": 5
    },
    {
      "epoch": 0.00455736584254301,
      "grad_norm": 0.09155458211898804,
      "learning_rate": 0.0009990884229717412,
      "loss": 1.043,
      "step": 10
    },
    {
      "epoch": 0.006836048763814515,
      "grad_norm": 0.08511107414960861,
      "learning_rate": 0.0009986326344576116,
      "loss": 0.9872,
      "step": 15
    },
    {
      "epoch": 0.00911473168508602,
      "grad_norm": 0.10077137500047684,
      "learning_rate": 0.0009981768459434823,
      "loss": 1.127,
      "step": 20
    },
    {
      "epoch": 0.011393414606357526,
      "grad_norm": 0.12256806343793869,
      "learning_rate": 0.0009977210574293527,
      "loss": 0.9853,
      "step": 25
    },
    {
      "epoch": 0.01367209752762903,
      "grad_norm": 0.12184912711381912,
      "learning_rate": 0.0009972652689152234,
      "loss": 1.0375,
      "step": 30
    },
    {
      "epoch": 0.015950780448900536,
      "grad_norm": 0.12439608573913574,
      "learning_rate": 0.0009968094804010939,
      "loss": 1.0178,
      "step": 35
    },
    {
      "epoch": 0.01822946337017204,
      "grad_norm": 0.10256876051425934,
      "learning_rate": 0.0009963536918869646,
      "loss": 1.0438,
      "step": 40
    },
    {
      "epoch": 0.020508146291443544,
      "grad_norm": 0.15623700618743896,
      "learning_rate": 0.000995897903372835,
      "loss": 1.0623,
      "step": 45
    },
    {
      "epoch": 0.022786829212715052,
      "grad_norm": 0.1048489362001419,
      "learning_rate": 0.0009954421148587057,
      "loss": 1.0141,
      "step": 50
    },
    {
      "epoch": 0.025065512133986556,
      "grad_norm": 0.09730970859527588,
      "learning_rate": 0.0009949863263445761,
      "loss": 0.9889,
      "step": 55
    },
    {
      "epoch": 0.02734419505525806,
      "grad_norm": 0.1007462665438652,
      "learning_rate": 0.0009945305378304466,
      "loss": 1.0402,
      "step": 60
    },
    {
      "epoch": 0.029622877976529564,
      "grad_norm": 0.09513722360134125,
      "learning_rate": 0.0009940747493163173,
      "loss": 1.0402,
      "step": 65
    },
    {
      "epoch": 0.03190156089780107,
      "grad_norm": 0.09411641955375671,
      "learning_rate": 0.0009936189608021877,
      "loss": 0.9944,
      "step": 70
    },
    {
      "epoch": 0.03418024381907257,
      "grad_norm": 0.10454647243022919,
      "learning_rate": 0.0009931631722880584,
      "loss": 0.9613,
      "step": 75
    },
    {
      "epoch": 0.03645892674034408,
      "grad_norm": 0.09802419692277908,
      "learning_rate": 0.0009927073837739289,
      "loss": 0.9634,
      "step": 80
    },
    {
      "epoch": 0.03873760966161559,
      "grad_norm": 0.09592941403388977,
      "learning_rate": 0.0009922515952597996,
      "loss": 1.0765,
      "step": 85
    },
    {
      "epoch": 0.04101629258288709,
      "grad_norm": 0.10440418869256973,
      "learning_rate": 0.00099179580674567,
      "loss": 0.9624,
      "step": 90
    },
    {
      "epoch": 0.043294975504158596,
      "grad_norm": 0.11446361243724823,
      "learning_rate": 0.0009913400182315407,
      "loss": 1.0746,
      "step": 95
    },
    {
      "epoch": 0.045573658425430104,
      "grad_norm": 0.10978246480226517,
      "learning_rate": 0.0009908842297174111,
      "loss": 1.0577,
      "step": 100
    },
    {
      "epoch": 0.047852341346701605,
      "grad_norm": 0.10599663108587265,
      "learning_rate": 0.0009904284412032816,
      "loss": 0.9631,
      "step": 105
    },
    {
      "epoch": 0.05013102426797311,
      "grad_norm": 0.10412118583917618,
      "learning_rate": 0.0009899726526891523,
      "loss": 0.9564,
      "step": 110
    },
    {
      "epoch": 0.05240970718924462,
      "grad_norm": 0.10239516198635101,
      "learning_rate": 0.000989516864175023,
      "loss": 0.9482,
      "step": 115
    },
    {
      "epoch": 0.05468839011051612,
      "grad_norm": 0.11652769148349762,
      "learning_rate": 0.0009890610756608934,
      "loss": 0.9874,
      "step": 120
    },
    {
      "epoch": 0.05696707303178763,
      "grad_norm": 0.09128254652023315,
      "learning_rate": 0.0009886052871467639,
      "loss": 0.9741,
      "step": 125
    },
    {
      "epoch": 0.05924575595305913,
      "grad_norm": 0.1035212054848671,
      "learning_rate": 0.0009881494986326345,
      "loss": 0.9845,
      "step": 130
    },
    {
      "epoch": 0.061524438874330636,
      "grad_norm": 0.1098245158791542,
      "learning_rate": 0.000987693710118505,
      "loss": 1.0354,
      "step": 135
    },
    {
      "epoch": 0.06380312179560214,
      "grad_norm": 0.0954827293753624,
      "learning_rate": 0.0009872379216043757,
      "loss": 0.9822,
      "step": 140
    },
    {
      "epoch": 0.06608180471687365,
      "grad_norm": 0.10910303145647049,
      "learning_rate": 0.0009867821330902461,
      "loss": 1.003,
      "step": 145
    },
    {
      "epoch": 0.06836048763814515,
      "grad_norm": 0.0921766459941864,
      "learning_rate": 0.0009863263445761166,
      "loss": 0.9098,
      "step": 150
    },
    {
      "epoch": 0.07063917055941665,
      "grad_norm": 0.09163478761911392,
      "learning_rate": 0.0009858705560619873,
      "loss": 1.0024,
      "step": 155
    },
    {
      "epoch": 0.07291785348068816,
      "grad_norm": 0.11847501248121262,
      "learning_rate": 0.000985414767547858,
      "loss": 1.0631,
      "step": 160
    },
    {
      "epoch": 0.07519653640195967,
      "grad_norm": 0.1052233949303627,
      "learning_rate": 0.0009849589790337284,
      "loss": 0.9663,
      "step": 165
    },
    {
      "epoch": 0.07747521932323118,
      "grad_norm": 0.1054283007979393,
      "learning_rate": 0.0009845031905195989,
      "loss": 0.9473,
      "step": 170
    },
    {
      "epoch": 0.07975390224450268,
      "grad_norm": 0.10878656804561615,
      "learning_rate": 0.0009840474020054695,
      "loss": 0.987,
      "step": 175
    },
    {
      "epoch": 0.08203258516577418,
      "grad_norm": 0.10643110424280167,
      "learning_rate": 0.00098359161349134,
      "loss": 0.9581,
      "step": 180
    },
    {
      "epoch": 0.08431126808704568,
      "grad_norm": 0.10773534327745438,
      "learning_rate": 0.0009831358249772105,
      "loss": 0.9321,
      "step": 185
    },
    {
      "epoch": 0.08658995100831719,
      "grad_norm": 0.11324618756771088,
      "learning_rate": 0.0009826800364630811,
      "loss": 0.9379,
      "step": 190
    },
    {
      "epoch": 0.0888686339295887,
      "grad_norm": 0.09794645756483078,
      "learning_rate": 0.0009822242479489518,
      "loss": 1.0033,
      "step": 195
    },
    {
      "epoch": 0.09114731685086021,
      "grad_norm": 0.10352832078933716,
      "learning_rate": 0.0009817684594348223,
      "loss": 0.9376,
      "step": 200
    },
    {
      "epoch": 0.0934259997721317,
      "grad_norm": 0.1152883768081665,
      "learning_rate": 0.000981312670920693,
      "loss": 1.0092,
      "step": 205
    },
    {
      "epoch": 0.09570468269340321,
      "grad_norm": 0.10217010974884033,
      "learning_rate": 0.0009808568824065634,
      "loss": 0.9517,
      "step": 210
    },
    {
      "epoch": 0.09798336561467472,
      "grad_norm": 0.10455787926912308,
      "learning_rate": 0.0009804010938924339,
      "loss": 1.008,
      "step": 215
    },
    {
      "epoch": 0.10026204853594622,
      "grad_norm": 0.11113252490758896,
      "learning_rate": 0.0009799453053783045,
      "loss": 0.96,
      "step": 220
    },
    {
      "epoch": 0.10254073145721773,
      "grad_norm": 0.09361538290977478,
      "learning_rate": 0.000979489516864175,
      "loss": 0.9398,
      "step": 225
    },
    {
      "epoch": 0.10481941437848924,
      "grad_norm": 0.1211777850985527,
      "learning_rate": 0.0009790337283500455,
      "loss": 0.942,
      "step": 230
    },
    {
      "epoch": 0.10709809729976073,
      "grad_norm": 0.1100122332572937,
      "learning_rate": 0.0009785779398359161,
      "loss": 0.9332,
      "step": 235
    },
    {
      "epoch": 0.10937678022103224,
      "grad_norm": 0.09881787747144699,
      "learning_rate": 0.0009781221513217868,
      "loss": 0.9718,
      "step": 240
    },
    {
      "epoch": 0.11165546314230375,
      "grad_norm": 0.09869951754808426,
      "learning_rate": 0.0009776663628076573,
      "loss": 0.9407,
      "step": 245
    },
    {
      "epoch": 0.11393414606357526,
      "grad_norm": 0.11816848069429398,
      "learning_rate": 0.0009772105742935277,
      "loss": 0.975,
      "step": 250
    },
    {
      "epoch": 0.11621282898484676,
      "grad_norm": 0.11892013251781464,
      "learning_rate": 0.0009767547857793984,
      "loss": 1.0304,
      "step": 255
    },
    {
      "epoch": 0.11849151190611826,
      "grad_norm": 0.10465247929096222,
      "learning_rate": 0.000976298997265269,
      "loss": 0.9437,
      "step": 260
    },
    {
      "epoch": 0.12077019482738977,
      "grad_norm": 0.11100976169109344,
      "learning_rate": 0.0009758432087511395,
      "loss": 0.9398,
      "step": 265
    },
    {
      "epoch": 0.12304887774866127,
      "grad_norm": 0.10800252109766006,
      "learning_rate": 0.00097538742023701,
      "loss": 0.9436,
      "step": 270
    },
    {
      "epoch": 0.12532756066993278,
      "grad_norm": 0.11363852769136429,
      "learning_rate": 0.0009749316317228806,
      "loss": 0.9428,
      "step": 275
    },
    {
      "epoch": 0.1276062435912043,
      "grad_norm": 0.09550884366035461,
      "learning_rate": 0.0009744758432087511,
      "loss": 0.9467,
      "step": 280
    },
    {
      "epoch": 0.1298849265124758,
      "grad_norm": 0.10154449939727783,
      "learning_rate": 0.0009740200546946217,
      "loss": 0.9374,
      "step": 285
    },
    {
      "epoch": 0.1321636094337473,
      "grad_norm": 0.10747954249382019,
      "learning_rate": 0.0009735642661804924,
      "loss": 0.9827,
      "step": 290
    },
    {
      "epoch": 0.1344422923550188,
      "grad_norm": 0.10706672072410583,
      "learning_rate": 0.0009731084776663628,
      "loss": 0.9529,
      "step": 295
    },
    {
      "epoch": 0.1367209752762903,
      "grad_norm": 0.10651249438524246,
      "learning_rate": 0.0009726526891522334,
      "loss": 0.9542,
      "step": 300
    },
    {
      "epoch": 0.1389996581975618,
      "grad_norm": 0.11839815229177475,
      "learning_rate": 0.000972196900638104,
      "loss": 0.898,
      "step": 305
    },
    {
      "epoch": 0.1412783411188333,
      "grad_norm": 0.10431333631277084,
      "learning_rate": 0.0009717411121239745,
      "loss": 1.0033,
      "step": 310
    },
    {
      "epoch": 0.1435570240401048,
      "grad_norm": 0.12134947627782822,
      "learning_rate": 0.000971285323609845,
      "loss": 0.9281,
      "step": 315
    },
    {
      "epoch": 0.14583570696137632,
      "grad_norm": 0.12386948615312576,
      "learning_rate": 0.0009708295350957156,
      "loss": 0.9595,
      "step": 320
    },
    {
      "epoch": 0.14811438988264783,
      "grad_norm": 0.10786985605955124,
      "learning_rate": 0.0009703737465815861,
      "loss": 0.939,
      "step": 325
    },
    {
      "epoch": 0.15039307280391934,
      "grad_norm": 0.09655681997537613,
      "learning_rate": 0.0009699179580674568,
      "loss": 0.9382,
      "step": 330
    },
    {
      "epoch": 0.15267175572519084,
      "grad_norm": 0.11886575073003769,
      "learning_rate": 0.0009694621695533273,
      "loss": 0.9657,
      "step": 335
    },
    {
      "epoch": 0.15495043864646235,
      "grad_norm": 0.11680003255605698,
      "learning_rate": 0.0009690063810391978,
      "loss": 0.9916,
      "step": 340
    },
    {
      "epoch": 0.15722912156773386,
      "grad_norm": 0.11795920133590698,
      "learning_rate": 0.0009685505925250684,
      "loss": 0.9564,
      "step": 345
    },
    {
      "epoch": 0.15950780448900537,
      "grad_norm": 0.10779263824224472,
      "learning_rate": 0.000968094804010939,
      "loss": 0.9057,
      "step": 350
    },
    {
      "epoch": 0.16178648741027685,
      "grad_norm": 0.1193455159664154,
      "learning_rate": 0.0009676390154968095,
      "loss": 0.9964,
      "step": 355
    },
    {
      "epoch": 0.16406517033154835,
      "grad_norm": 0.10625666379928589,
      "learning_rate": 0.00096718322698268,
      "loss": 0.9513,
      "step": 360
    },
    {
      "epoch": 0.16634385325281986,
      "grad_norm": 0.11565297842025757,
      "learning_rate": 0.0009667274384685507,
      "loss": 0.9919,
      "step": 365
    },
    {
      "epoch": 0.16862253617409137,
      "grad_norm": 0.15058274567127228,
      "learning_rate": 0.0009662716499544212,
      "loss": 0.8935,
      "step": 370
    },
    {
      "epoch": 0.17090121909536288,
      "grad_norm": 0.11679674685001373,
      "learning_rate": 0.0009658158614402918,
      "loss": 0.9716,
      "step": 375
    },
    {
      "epoch": 0.17317990201663438,
      "grad_norm": 0.1397944688796997,
      "learning_rate": 0.0009653600729261622,
      "loss": 0.9802,
      "step": 380
    },
    {
      "epoch": 0.1754585849379059,
      "grad_norm": 0.14461687207221985,
      "learning_rate": 0.0009649042844120328,
      "loss": 0.9736,
      "step": 385
    },
    {
      "epoch": 0.1777372678591774,
      "grad_norm": 0.1175338625907898,
      "learning_rate": 0.0009644484958979034,
      "loss": 1.0065,
      "step": 390
    },
    {
      "epoch": 0.1800159507804489,
      "grad_norm": 0.13391362130641937,
      "learning_rate": 0.000963992707383774,
      "loss": 0.9127,
      "step": 395
    },
    {
      "epoch": 0.18229463370172042,
      "grad_norm": 0.1148635521531105,
      "learning_rate": 0.0009635369188696444,
      "loss": 0.9572,
      "step": 400
    },
    {
      "epoch": 0.18457331662299192,
      "grad_norm": 0.1216767430305481,
      "learning_rate": 0.0009630811303555151,
      "loss": 0.972,
      "step": 405
    },
    {
      "epoch": 0.1868519995442634,
      "grad_norm": 0.11752130836248398,
      "learning_rate": 0.0009626253418413856,
      "loss": 0.9122,
      "step": 410
    },
    {
      "epoch": 0.1891306824655349,
      "grad_norm": 0.11848433315753937,
      "learning_rate": 0.0009621695533272562,
      "loss": 0.9398,
      "step": 415
    },
    {
      "epoch": 0.19140936538680642,
      "grad_norm": 0.11063068360090256,
      "learning_rate": 0.0009617137648131268,
      "loss": 0.9662,
      "step": 420
    },
    {
      "epoch": 0.19368804830807793,
      "grad_norm": 0.12610915303230286,
      "learning_rate": 0.0009612579762989972,
      "loss": 0.9613,
      "step": 425
    },
    {
      "epoch": 0.19596673122934943,
      "grad_norm": 0.11474351584911346,
      "learning_rate": 0.0009608021877848678,
      "loss": 0.9897,
      "step": 430
    },
    {
      "epoch": 0.19824541415062094,
      "grad_norm": 0.10608746856451035,
      "learning_rate": 0.0009603463992707384,
      "loss": 0.9024,
      "step": 435
    },
    {
      "epoch": 0.20052409707189245,
      "grad_norm": 0.12237820774316788,
      "learning_rate": 0.0009598906107566089,
      "loss": 0.9772,
      "step": 440
    },
    {
      "epoch": 0.20280277999316396,
      "grad_norm": 0.14569999277591705,
      "learning_rate": 0.0009594348222424795,
      "loss": 1.0101,
      "step": 445
    },
    {
      "epoch": 0.20508146291443546,
      "grad_norm": 0.1156698614358902,
      "learning_rate": 0.0009589790337283501,
      "loss": 0.9497,
      "step": 450
    },
    {
      "epoch": 0.20736014583570697,
      "grad_norm": 0.11267761886119843,
      "learning_rate": 0.0009585232452142206,
      "loss": 0.9285,
      "step": 455
    },
    {
      "epoch": 0.20963882875697848,
      "grad_norm": 0.13181933760643005,
      "learning_rate": 0.0009580674567000912,
      "loss": 0.9983,
      "step": 460
    },
    {
      "epoch": 0.21191751167824996,
      "grad_norm": 0.10774251073598862,
      "learning_rate": 0.0009576116681859617,
      "loss": 0.9343,
      "step": 465
    },
    {
      "epoch": 0.21419619459952147,
      "grad_norm": 0.10888870060443878,
      "learning_rate": 0.0009571558796718322,
      "loss": 0.9638,
      "step": 470
    },
    {
      "epoch": 0.21647487752079297,
      "grad_norm": 0.1052594855427742,
      "learning_rate": 0.0009567000911577028,
      "loss": 0.9037,
      "step": 475
    },
    {
      "epoch": 0.21875356044206448,
      "grad_norm": 0.13200430572032928,
      "learning_rate": 0.0009562443026435735,
      "loss": 0.9722,
      "step": 480
    },
    {
      "epoch": 0.221032243363336,
      "grad_norm": 0.1185946837067604,
      "learning_rate": 0.000955788514129444,
      "loss": 0.9324,
      "step": 485
    },
    {
      "epoch": 0.2233109262846075,
      "grad_norm": 0.112062469124794,
      "learning_rate": 0.0009553327256153145,
      "loss": 0.9284,
      "step": 490
    },
    {
      "epoch": 0.225589609205879,
      "grad_norm": 0.11640054732561111,
      "learning_rate": 0.0009548769371011851,
      "loss": 0.9986,
      "step": 495
    },
    {
      "epoch": 0.2278682921271505,
      "grad_norm": 0.10924868285655975,
      "learning_rate": 0.0009544211485870556,
      "loss": 0.9648,
      "step": 500
    },
    {
      "epoch": 0.23014697504842202,
      "grad_norm": 0.12101496011018753,
      "learning_rate": 0.0009539653600729262,
      "loss": 0.9203,
      "step": 505
    },
    {
      "epoch": 0.23242565796969353,
      "grad_norm": 0.10965333878993988,
      "learning_rate": 0.0009535095715587967,
      "loss": 1.0195,
      "step": 510
    },
    {
      "epoch": 0.23470434089096504,
      "grad_norm": 0.12445380538702011,
      "learning_rate": 0.0009530537830446672,
      "loss": 0.9628,
      "step": 515
    },
    {
      "epoch": 0.23698302381223652,
      "grad_norm": 0.11953502893447876,
      "learning_rate": 0.0009525979945305379,
      "loss": 0.9014,
      "step": 520
    },
    {
      "epoch": 0.23926170673350802,
      "grad_norm": 0.1437353491783142,
      "learning_rate": 0.0009521422060164085,
      "loss": 1.0213,
      "step": 525
    },
    {
      "epoch": 0.24154038965477953,
      "grad_norm": 0.113726906478405,
      "learning_rate": 0.0009516864175022789,
      "loss": 0.9323,
      "step": 530
    },
    {
      "epoch": 0.24381907257605104,
      "grad_norm": 0.13160257041454315,
      "learning_rate": 0.0009512306289881495,
      "loss": 0.9547,
      "step": 535
    },
    {
      "epoch": 0.24609775549732255,
      "grad_norm": 0.10455253720283508,
      "learning_rate": 0.0009507748404740201,
      "loss": 0.9062,
      "step": 540
    },
    {
      "epoch": 0.24837643841859405,
      "grad_norm": 0.13184143602848053,
      "learning_rate": 0.0009503190519598906,
      "loss": 0.9816,
      "step": 545
    },
    {
      "epoch": 0.25065512133986556,
      "grad_norm": 0.12978142499923706,
      "learning_rate": 0.0009498632634457612,
      "loss": 0.9757,
      "step": 550
    },
    {
      "epoch": 0.25293380426113704,
      "grad_norm": 0.11331029236316681,
      "learning_rate": 0.0009494074749316317,
      "loss": 0.9482,
      "step": 555
    },
    {
      "epoch": 0.2552124871824086,
      "grad_norm": 0.12034566700458527,
      "learning_rate": 0.0009489516864175023,
      "loss": 0.9348,
      "step": 560
    },
    {
      "epoch": 0.25749117010368006,
      "grad_norm": 0.12097957730293274,
      "learning_rate": 0.0009484958979033729,
      "loss": 0.9876,
      "step": 565
    },
    {
      "epoch": 0.2597698530249516,
      "grad_norm": 0.10560165345668793,
      "learning_rate": 0.0009480401093892435,
      "loss": 0.8937,
      "step": 570
    },
    {
      "epoch": 0.26204853594622307,
      "grad_norm": 0.11995459347963333,
      "learning_rate": 0.0009475843208751139,
      "loss": 0.9336,
      "step": 575
    },
    {
      "epoch": 0.2643272188674946,
      "grad_norm": 0.1246040016412735,
      "learning_rate": 0.0009471285323609845,
      "loss": 0.959,
      "step": 580
    },
    {
      "epoch": 0.2666059017887661,
      "grad_norm": 0.12361729890108109,
      "learning_rate": 0.0009466727438468551,
      "loss": 1.0027,
      "step": 585
    },
    {
      "epoch": 0.2688845847100376,
      "grad_norm": 0.11128146946430206,
      "learning_rate": 0.0009462169553327256,
      "loss": 0.879,
      "step": 590
    },
    {
      "epoch": 0.2711632676313091,
      "grad_norm": 0.111976258456707,
      "learning_rate": 0.0009457611668185962,
      "loss": 0.9879,
      "step": 595
    },
    {
      "epoch": 0.2734419505525806,
      "grad_norm": 0.12304849922657013,
      "learning_rate": 0.0009453053783044668,
      "loss": 0.9456,
      "step": 600
    },
    {
      "epoch": 0.2757206334738521,
      "grad_norm": 0.12333634495735168,
      "learning_rate": 0.0009448495897903373,
      "loss": 0.9521,
      "step": 605
    },
    {
      "epoch": 0.2779993163951236,
      "grad_norm": 0.11333766579627991,
      "learning_rate": 0.0009443938012762079,
      "loss": 0.9341,
      "step": 610
    },
    {
      "epoch": 0.28027799931639513,
      "grad_norm": 0.13577967882156372,
      "learning_rate": 0.0009439380127620785,
      "loss": 0.9754,
      "step": 615
    },
    {
      "epoch": 0.2825566822376666,
      "grad_norm": 0.10655386000871658,
      "learning_rate": 0.0009434822242479489,
      "loss": 0.9789,
      "step": 620
    },
    {
      "epoch": 0.28483536515893815,
      "grad_norm": 0.15203267335891724,
      "learning_rate": 0.0009430264357338195,
      "loss": 0.9451,
      "step": 625
    },
    {
      "epoch": 0.2871140480802096,
      "grad_norm": 0.1078549474477768,
      "learning_rate": 0.0009425706472196901,
      "loss": 0.9075,
      "step": 630
    },
    {
      "epoch": 0.28939273100148116,
      "grad_norm": 0.1377035528421402,
      "learning_rate": 0.0009421148587055607,
      "loss": 0.9537,
      "step": 635
    },
    {
      "epoch": 0.29167141392275264,
      "grad_norm": 0.1152191162109375,
      "learning_rate": 0.0009416590701914312,
      "loss": 0.958,
      "step": 640
    },
    {
      "epoch": 0.2939500968440242,
      "grad_norm": 0.1309157907962799,
      "learning_rate": 0.0009412032816773018,
      "loss": 0.956,
      "step": 645
    },
    {
      "epoch": 0.29622877976529566,
      "grad_norm": 0.12400003522634506,
      "learning_rate": 0.0009407474931631723,
      "loss": 0.9313,
      "step": 650
    },
    {
      "epoch": 0.29850746268656714,
      "grad_norm": 0.14170987904071808,
      "learning_rate": 0.0009402917046490429,
      "loss": 0.941,
      "step": 655
    },
    {
      "epoch": 0.3007861456078387,
      "grad_norm": 0.11736692488193512,
      "learning_rate": 0.0009398359161349133,
      "loss": 0.9179,
      "step": 660
    },
    {
      "epoch": 0.30306482852911015,
      "grad_norm": 0.11447620391845703,
      "learning_rate": 0.0009393801276207839,
      "loss": 0.9419,
      "step": 665
    },
    {
      "epoch": 0.3053435114503817,
      "grad_norm": 0.1310737431049347,
      "learning_rate": 0.0009389243391066545,
      "loss": 0.9511,
      "step": 670
    },
    {
      "epoch": 0.30762219437165317,
      "grad_norm": 0.1319255232810974,
      "learning_rate": 0.0009384685505925252,
      "loss": 0.9129,
      "step": 675
    },
    {
      "epoch": 0.3099008772929247,
      "grad_norm": 0.15739628672599792,
      "learning_rate": 0.0009380127620783957,
      "loss": 1.0098,
      "step": 680
    },
    {
      "epoch": 0.3121795602141962,
      "grad_norm": 0.12432535737752914,
      "learning_rate": 0.0009375569735642662,
      "loss": 0.9478,
      "step": 685
    },
    {
      "epoch": 0.3144582431354677,
      "grad_norm": 0.12540653347969055,
      "learning_rate": 0.0009371011850501368,
      "loss": 0.9363,
      "step": 690
    },
    {
      "epoch": 0.3167369260567392,
      "grad_norm": 0.12726224958896637,
      "learning_rate": 0.0009366453965360073,
      "loss": 0.8978,
      "step": 695
    },
    {
      "epoch": 0.31901560897801073,
      "grad_norm": 0.15135599672794342,
      "learning_rate": 0.0009361896080218779,
      "loss": 0.9311,
      "step": 700
    },
    {
      "epoch": 0.3212942918992822,
      "grad_norm": 0.12312465906143188,
      "learning_rate": 0.0009357338195077483,
      "loss": 0.9075,
      "step": 705
    },
    {
      "epoch": 0.3235729748205537,
      "grad_norm": 0.13183347880840302,
      "learning_rate": 0.000935278030993619,
      "loss": 0.928,
      "step": 710
    },
    {
      "epoch": 0.32585165774182523,
      "grad_norm": 0.1150856465101242,
      "learning_rate": 0.0009348222424794896,
      "loss": 0.9375,
      "step": 715
    },
    {
      "epoch": 0.3281303406630967,
      "grad_norm": 0.13035635650157928,
      "learning_rate": 0.0009343664539653602,
      "loss": 0.94,
      "step": 720
    },
    {
      "epoch": 0.33040902358436824,
      "grad_norm": 0.13293299078941345,
      "learning_rate": 0.0009339106654512306,
      "loss": 0.9296,
      "step": 725
    },
    {
      "epoch": 0.3326877065056397,
      "grad_norm": 0.12464173138141632,
      "learning_rate": 0.0009334548769371012,
      "loss": 0.8977,
      "step": 730
    },
    {
      "epoch": 0.33496638942691126,
      "grad_norm": 0.11725596338510513,
      "learning_rate": 0.0009329990884229717,
      "loss": 0.951,
      "step": 735
    },
    {
      "epoch": 0.33724507234818274,
      "grad_norm": 0.14468272030353546,
      "learning_rate": 0.0009325432999088423,
      "loss": 1.037,
      "step": 740
    },
    {
      "epoch": 0.3395237552694543,
      "grad_norm": 0.12415248900651932,
      "learning_rate": 0.0009320875113947129,
      "loss": 0.9625,
      "step": 745
    },
    {
      "epoch": 0.34180243819072575,
      "grad_norm": 0.12203644216060638,
      "learning_rate": 0.0009316317228805834,
      "loss": 0.8901,
      "step": 750
    },
    {
      "epoch": 0.3440811211119973,
      "grad_norm": 0.14319323003292084,
      "learning_rate": 0.000931175934366454,
      "loss": 0.9406,
      "step": 755
    },
    {
      "epoch": 0.34635980403326877,
      "grad_norm": 0.1215897649526596,
      "learning_rate": 0.0009307201458523246,
      "loss": 0.9146,
      "step": 760
    },
    {
      "epoch": 0.34863848695454025,
      "grad_norm": 0.13291974365711212,
      "learning_rate": 0.0009302643573381951,
      "loss": 0.9175,
      "step": 765
    },
    {
      "epoch": 0.3509171698758118,
      "grad_norm": 0.12854835391044617,
      "learning_rate": 0.0009298085688240656,
      "loss": 0.8935,
      "step": 770
    },
    {
      "epoch": 0.35319585279708327,
      "grad_norm": 0.12934555113315582,
      "learning_rate": 0.0009293527803099362,
      "loss": 0.9706,
      "step": 775
    },
    {
      "epoch": 0.3554745357183548,
      "grad_norm": 0.1279033124446869,
      "learning_rate": 0.0009288969917958067,
      "loss": 0.9796,
      "step": 780
    },
    {
      "epoch": 0.3577532186396263,
      "grad_norm": 0.14462311565876007,
      "learning_rate": 0.0009284412032816773,
      "loss": 0.9986,
      "step": 785
    },
    {
      "epoch": 0.3600319015608978,
      "grad_norm": 0.1174454316496849,
      "learning_rate": 0.0009279854147675479,
      "loss": 0.9456,
      "step": 790
    },
    {
      "epoch": 0.3623105844821693,
      "grad_norm": 0.12243582308292389,
      "learning_rate": 0.0009275296262534184,
      "loss": 0.9281,
      "step": 795
    },
    {
      "epoch": 0.36458926740344083,
      "grad_norm": 0.1211414486169815,
      "learning_rate": 0.000927073837739289,
      "loss": 0.9352,
      "step": 800
    },
    {
      "epoch": 0.3668679503247123,
      "grad_norm": 0.13699643313884735,
      "learning_rate": 0.0009266180492251596,
      "loss": 0.8514,
      "step": 805
    },
    {
      "epoch": 0.36914663324598385,
      "grad_norm": 0.12660907208919525,
      "learning_rate": 0.00092616226071103,
      "loss": 0.9666,
      "step": 810
    },
    {
      "epoch": 0.3714253161672553,
      "grad_norm": 0.12480060756206512,
      "learning_rate": 0.0009257064721969006,
      "loss": 0.9472,
      "step": 815
    },
    {
      "epoch": 0.3737039990885268,
      "grad_norm": 0.12947490811347961,
      "learning_rate": 0.0009252506836827712,
      "loss": 1.0032,
      "step": 820
    },
    {
      "epoch": 0.37598268200979834,
      "grad_norm": 0.13831546902656555,
      "learning_rate": 0.0009247948951686418,
      "loss": 0.9065,
      "step": 825
    },
    {
      "epoch": 0.3782613649310698,
      "grad_norm": 0.15379518270492554,
      "learning_rate": 0.0009243391066545124,
      "loss": 0.9483,
      "step": 830
    },
    {
      "epoch": 0.38054004785234136,
      "grad_norm": 0.1163540855050087,
      "learning_rate": 0.0009238833181403829,
      "loss": 0.9568,
      "step": 835
    },
    {
      "epoch": 0.38281873077361284,
      "grad_norm": 0.13917338848114014,
      "learning_rate": 0.0009234275296262534,
      "loss": 0.9184,
      "step": 840
    },
    {
      "epoch": 0.38509741369488437,
      "grad_norm": 0.1298019289970398,
      "learning_rate": 0.000922971741112124,
      "loss": 0.9512,
      "step": 845
    },
    {
      "epoch": 0.38737609661615585,
      "grad_norm": 0.12695527076721191,
      "learning_rate": 0.0009225159525979946,
      "loss": 0.9894,
      "step": 850
    },
    {
      "epoch": 0.3896547795374274,
      "grad_norm": 0.1436893492937088,
      "learning_rate": 0.000922060164083865,
      "loss": 0.9219,
      "step": 855
    },
    {
      "epoch": 0.39193346245869887,
      "grad_norm": 0.13510450720787048,
      "learning_rate": 0.0009216043755697356,
      "loss": 0.9708,
      "step": 860
    },
    {
      "epoch": 0.3942121453799704,
      "grad_norm": 0.15884274244308472,
      "learning_rate": 0.0009211485870556063,
      "loss": 0.9011,
      "step": 865
    },
    {
      "epoch": 0.3964908283012419,
      "grad_norm": 0.12714359164237976,
      "learning_rate": 0.0009206927985414768,
      "loss": 0.969,
      "step": 870
    },
    {
      "epoch": 0.39876951122251336,
      "grad_norm": 0.12694977223873138,
      "learning_rate": 0.0009202370100273473,
      "loss": 0.8908,
      "step": 875
    },
    {
      "epoch": 0.4010481941437849,
      "grad_norm": 0.1311943680047989,
      "learning_rate": 0.0009197812215132179,
      "loss": 0.9632,
      "step": 880
    },
    {
      "epoch": 0.4033268770650564,
      "grad_norm": 0.12838102877140045,
      "learning_rate": 0.0009193254329990884,
      "loss": 0.9285,
      "step": 885
    },
    {
      "epoch": 0.4056055599863279,
      "grad_norm": 0.13826465606689453,
      "learning_rate": 0.000918869644484959,
      "loss": 1.0181,
      "step": 890
    },
    {
      "epoch": 0.4078842429075994,
      "grad_norm": 0.14367245137691498,
      "learning_rate": 0.0009184138559708296,
      "loss": 0.9106,
      "step": 895
    },
    {
      "epoch": 0.41016292582887093,
      "grad_norm": 0.1405639499425888,
      "learning_rate": 0.0009179580674567,
      "loss": 1.0031,
      "step": 900
    },
    {
      "epoch": 0.4124416087501424,
      "grad_norm": 0.1320701390504837,
      "learning_rate": 0.0009175022789425707,
      "loss": 0.9587,
      "step": 905
    },
    {
      "epoch": 0.41472029167141394,
      "grad_norm": 0.1418411135673523,
      "learning_rate": 0.0009170464904284413,
      "loss": 0.9347,
      "step": 910
    },
    {
      "epoch": 0.4169989745926854,
      "grad_norm": 0.12313879281282425,
      "learning_rate": 0.0009165907019143118,
      "loss": 0.8689,
      "step": 915
    },
    {
      "epoch": 0.41927765751395696,
      "grad_norm": 0.1282174438238144,
      "learning_rate": 0.0009161349134001823,
      "loss": 0.9814,
      "step": 920
    },
    {
      "epoch": 0.42155634043522844,
      "grad_norm": 0.12278856337070465,
      "learning_rate": 0.0009156791248860529,
      "loss": 0.9435,
      "step": 925
    },
    {
      "epoch": 0.4238350233564999,
      "grad_norm": 0.12727299332618713,
      "learning_rate": 0.0009152233363719234,
      "loss": 0.9065,
      "step": 930
    },
    {
      "epoch": 0.42611370627777145,
      "grad_norm": 0.13353604078292847,
      "learning_rate": 0.000914767547857794,
      "loss": 0.8682,
      "step": 935
    },
    {
      "epoch": 0.42839238919904293,
      "grad_norm": 0.1339009404182434,
      "learning_rate": 0.0009143117593436645,
      "loss": 0.958,
      "step": 940
    },
    {
      "epoch": 0.43067107212031447,
      "grad_norm": 0.1257205605506897,
      "learning_rate": 0.0009138559708295351,
      "loss": 0.9432,
      "step": 945
    },
    {
      "epoch": 0.43294975504158595,
      "grad_norm": 0.132902130484581,
      "learning_rate": 0.0009134001823154057,
      "loss": 0.9508,
      "step": 950
    },
    {
      "epoch": 0.4352284379628575,
      "grad_norm": 0.15629568696022034,
      "learning_rate": 0.0009129443938012763,
      "loss": 0.949,
      "step": 955
    },
    {
      "epoch": 0.43750712088412896,
      "grad_norm": 0.11243929713964462,
      "learning_rate": 0.0009124886052871468,
      "loss": 0.9242,
      "step": 960
    },
    {
      "epoch": 0.4397858038054005,
      "grad_norm": 0.12550589442253113,
      "learning_rate": 0.0009120328167730173,
      "loss": 0.9067,
      "step": 965
    },
    {
      "epoch": 0.442064486726672,
      "grad_norm": 0.12432727217674255,
      "learning_rate": 0.0009115770282588879,
      "loss": 0.9145,
      "step": 970
    },
    {
      "epoch": 0.4443431696479435,
      "grad_norm": 0.1366255283355713,
      "learning_rate": 0.0009111212397447584,
      "loss": 0.9394,
      "step": 975
    },
    {
      "epoch": 0.446621852569215,
      "grad_norm": 0.13807351887226105,
      "learning_rate": 0.0009106654512306291,
      "loss": 1.0138,
      "step": 980
    },
    {
      "epoch": 0.4489005354904865,
      "grad_norm": 0.11883367598056793,
      "learning_rate": 0.0009102096627164996,
      "loss": 0.8986,
      "step": 985
    },
    {
      "epoch": 0.451179218411758,
      "grad_norm": 0.13321243226528168,
      "learning_rate": 0.0009097538742023701,
      "loss": 0.8986,
      "step": 990
    },
    {
      "epoch": 0.4534579013330295,
      "grad_norm": 0.13463422656059265,
      "learning_rate": 0.0009092980856882407,
      "loss": 0.9512,
      "step": 995
    },
    {
      "epoch": 0.455736584254301,
      "grad_norm": 0.127491295337677,
      "learning_rate": 0.0009088422971741113,
      "loss": 0.9044,
      "step": 1000
    },
    {
      "epoch": 0.4580152671755725,
      "grad_norm": 0.13046576082706451,
      "learning_rate": 0.0009083865086599817,
      "loss": 0.9238,
      "step": 1005
    },
    {
      "epoch": 0.46029395009684404,
      "grad_norm": 0.11968263238668442,
      "learning_rate": 0.0009079307201458523,
      "loss": 0.926,
      "step": 1010
    },
    {
      "epoch": 0.4625726330181155,
      "grad_norm": 0.12671422958374023,
      "learning_rate": 0.0009074749316317228,
      "loss": 0.9165,
      "step": 1015
    },
    {
      "epoch": 0.46485131593938706,
      "grad_norm": 0.15490251779556274,
      "learning_rate": 0.0009070191431175935,
      "loss": 0.947,
      "step": 1020
    },
    {
      "epoch": 0.46712999886065854,
      "grad_norm": 0.1319398134946823,
      "learning_rate": 0.0009065633546034641,
      "loss": 0.8895,
      "step": 1025
    },
    {
      "epoch": 0.46940868178193007,
      "grad_norm": 0.13212162256240845,
      "learning_rate": 0.0009061075660893346,
      "loss": 0.9039,
      "step": 1030
    },
    {
      "epoch": 0.47168736470320155,
      "grad_norm": 0.13071350753307343,
      "learning_rate": 0.0009056517775752051,
      "loss": 0.9728,
      "step": 1035
    },
    {
      "epoch": 0.47396604762447303,
      "grad_norm": 0.14256185293197632,
      "learning_rate": 0.0009051959890610757,
      "loss": 0.9164,
      "step": 1040
    },
    {
      "epoch": 0.47624473054574457,
      "grad_norm": 0.13525396585464478,
      "learning_rate": 0.0009047402005469463,
      "loss": 0.9537,
      "step": 1045
    },
    {
      "epoch": 0.47852341346701605,
      "grad_norm": 0.13032911717891693,
      "learning_rate": 0.0009042844120328167,
      "loss": 0.9196,
      "step": 1050
    },
    {
      "epoch": 0.4808020963882876,
      "grad_norm": 0.14549599587917328,
      "learning_rate": 0.0009038286235186874,
      "loss": 0.9358,
      "step": 1055
    },
    {
      "epoch": 0.48308077930955906,
      "grad_norm": 0.11673810333013535,
      "learning_rate": 0.000903372835004558,
      "loss": 0.926,
      "step": 1060
    },
    {
      "epoch": 0.4853594622308306,
      "grad_norm": 0.1395089030265808,
      "learning_rate": 0.0009029170464904285,
      "loss": 0.9939,
      "step": 1065
    },
    {
      "epoch": 0.4876381451521021,
      "grad_norm": 0.1363755315542221,
      "learning_rate": 0.000902461257976299,
      "loss": 0.8382,
      "step": 1070
    },
    {
      "epoch": 0.4899168280733736,
      "grad_norm": 0.13836126029491425,
      "learning_rate": 0.0009020054694621695,
      "loss": 0.974,
      "step": 1075
    },
    {
      "epoch": 0.4921955109946451,
      "grad_norm": 0.14280329644680023,
      "learning_rate": 0.0009015496809480401,
      "loss": 0.924,
      "step": 1080
    },
    {
      "epoch": 0.4944741939159166,
      "grad_norm": 0.13463515043258667,
      "learning_rate": 0.0009010938924339107,
      "loss": 0.9403,
      "step": 1085
    },
    {
      "epoch": 0.4967528768371881,
      "grad_norm": 0.1219530925154686,
      "learning_rate": 0.0009006381039197812,
      "loss": 0.9177,
      "step": 1090
    },
    {
      "epoch": 0.4990315597584596,
      "grad_norm": 0.13787896931171417,
      "learning_rate": 0.0009001823154056518,
      "loss": 0.9323,
      "step": 1095
    },
    {
      "epoch": 0.5013102426797311,
      "grad_norm": 0.14427416026592255,
      "learning_rate": 0.0008997265268915224,
      "loss": 0.9479,
      "step": 1100
    },
    {
      "epoch": 0.5035889256010027,
      "grad_norm": 0.12902353703975677,
      "learning_rate": 0.000899270738377393,
      "loss": 0.883,
      "step": 1105
    },
    {
      "epoch": 0.5058676085222741,
      "grad_norm": 0.12941420078277588,
      "learning_rate": 0.0008988149498632635,
      "loss": 0.9347,
      "step": 1110
    },
    {
      "epoch": 0.5081462914435456,
      "grad_norm": 0.18012239038944244,
      "learning_rate": 0.000898359161349134,
      "loss": 0.9338,
      "step": 1115
    },
    {
      "epoch": 0.5104249743648172,
      "grad_norm": 0.13309596478939056,
      "learning_rate": 0.0008979033728350045,
      "loss": 0.9196,
      "step": 1120
    },
    {
      "epoch": 0.5127036572860887,
      "grad_norm": 0.13313528895378113,
      "learning_rate": 0.0008974475843208751,
      "loss": 1.0077,
      "step": 1125
    },
    {
      "epoch": 0.5149823402073601,
      "grad_norm": 0.12437841296195984,
      "learning_rate": 0.0008969917958067457,
      "loss": 0.9154,
      "step": 1130
    },
    {
      "epoch": 0.5172610231286316,
      "grad_norm": 0.16179345548152924,
      "learning_rate": 0.0008965360072926162,
      "loss": 0.958,
      "step": 1135
    },
    {
      "epoch": 0.5195397060499032,
      "grad_norm": 0.2213214486837387,
      "learning_rate": 0.0008960802187784868,
      "loss": 0.9002,
      "step": 1140
    },
    {
      "epoch": 0.5218183889711746,
      "grad_norm": 0.13946284353733063,
      "learning_rate": 0.0008956244302643574,
      "loss": 0.9716,
      "step": 1145
    },
    {
      "epoch": 0.5240970718924461,
      "grad_norm": 0.12701274454593658,
      "learning_rate": 0.0008951686417502279,
      "loss": 0.9258,
      "step": 1150
    },
    {
      "epoch": 0.5263757548137177,
      "grad_norm": 0.13497546315193176,
      "learning_rate": 0.0008947128532360985,
      "loss": 0.8921,
      "step": 1155
    },
    {
      "epoch": 0.5286544377349892,
      "grad_norm": 0.1311323195695877,
      "learning_rate": 0.000894257064721969,
      "loss": 0.9295,
      "step": 1160
    },
    {
      "epoch": 0.5309331206562606,
      "grad_norm": 0.14472800493240356,
      "learning_rate": 0.0008938012762078395,
      "loss": 0.917,
      "step": 1165
    },
    {
      "epoch": 0.5332118035775322,
      "grad_norm": 0.14514611661434174,
      "learning_rate": 0.0008933454876937102,
      "loss": 0.9661,
      "step": 1170
    },
    {
      "epoch": 0.5354904864988037,
      "grad_norm": 0.12884724140167236,
      "learning_rate": 0.0008928896991795808,
      "loss": 0.9582,
      "step": 1175
    },
    {
      "epoch": 0.5377691694200752,
      "grad_norm": 0.14155709743499756,
      "learning_rate": 0.0008924339106654512,
      "loss": 0.8899,
      "step": 1180
    },
    {
      "epoch": 0.5400478523413467,
      "grad_norm": 0.1542409062385559,
      "learning_rate": 0.0008919781221513218,
      "loss": 0.9366,
      "step": 1185
    },
    {
      "epoch": 0.5423265352626182,
      "grad_norm": 0.14219745993614197,
      "learning_rate": 0.0008915223336371924,
      "loss": 0.9187,
      "step": 1190
    },
    {
      "epoch": 0.5446052181838897,
      "grad_norm": 0.1309528797864914,
      "learning_rate": 0.0008910665451230629,
      "loss": 0.9371,
      "step": 1195
    },
    {
      "epoch": 0.5468839011051612,
      "grad_norm": 0.14337968826293945,
      "learning_rate": 0.0008906107566089334,
      "loss": 0.9052,
      "step": 1200
    },
    {
      "epoch": 0.5491625840264327,
      "grad_norm": 0.14293202757835388,
      "learning_rate": 0.000890154968094804,
      "loss": 0.937,
      "step": 1205
    },
    {
      "epoch": 0.5514412669477042,
      "grad_norm": 0.13278134167194366,
      "learning_rate": 0.0008896991795806746,
      "loss": 0.922,
      "step": 1210
    },
    {
      "epoch": 0.5537199498689758,
      "grad_norm": 0.14502961933612823,
      "learning_rate": 0.0008892433910665452,
      "loss": 0.9513,
      "step": 1215
    },
    {
      "epoch": 0.5559986327902472,
      "grad_norm": 0.15366731584072113,
      "learning_rate": 0.0008887876025524158,
      "loss": 0.8918,
      "step": 1220
    },
    {
      "epoch": 0.5582773157115187,
      "grad_norm": 0.1306658536195755,
      "learning_rate": 0.0008883318140382862,
      "loss": 0.9918,
      "step": 1225
    },
    {
      "epoch": 0.5605559986327903,
      "grad_norm": 0.1455005705356598,
      "learning_rate": 0.0008878760255241568,
      "loss": 0.902,
      "step": 1230
    },
    {
      "epoch": 0.5628346815540618,
      "grad_norm": 0.13323292136192322,
      "learning_rate": 0.0008874202370100274,
      "loss": 0.9143,
      "step": 1235
    },
    {
      "epoch": 0.5651133644753332,
      "grad_norm": 0.13893190026283264,
      "learning_rate": 0.0008869644484958979,
      "loss": 0.9451,
      "step": 1240
    },
    {
      "epoch": 0.5673920473966048,
      "grad_norm": 0.14276674389839172,
      "learning_rate": 0.0008865086599817684,
      "loss": 0.878,
      "step": 1245
    },
    {
      "epoch": 0.5696707303178763,
      "grad_norm": 0.13818784058094025,
      "learning_rate": 0.0008860528714676391,
      "loss": 0.9012,
      "step": 1250
    },
    {
      "epoch": 0.5719494132391477,
      "grad_norm": 0.13997294008731842,
      "learning_rate": 0.0008855970829535096,
      "loss": 0.9107,
      "step": 1255
    },
    {
      "epoch": 0.5742280961604193,
      "grad_norm": 0.12402183562517166,
      "learning_rate": 0.0008851412944393802,
      "loss": 0.9029,
      "step": 1260
    },
    {
      "epoch": 0.5765067790816908,
      "grad_norm": 0.13874591886997223,
      "learning_rate": 0.0008846855059252507,
      "loss": 0.9326,
      "step": 1265
    },
    {
      "epoch": 0.5787854620029623,
      "grad_norm": 0.1248471587896347,
      "learning_rate": 0.0008842297174111212,
      "loss": 0.9848,
      "step": 1270
    },
    {
      "epoch": 0.5810641449242337,
      "grad_norm": 0.1422695368528366,
      "learning_rate": 0.0008837739288969918,
      "loss": 0.9093,
      "step": 1275
    },
    {
      "epoch": 0.5833428278455053,
      "grad_norm": 0.18609541654586792,
      "learning_rate": 0.0008833181403828624,
      "loss": 0.8671,
      "step": 1280
    },
    {
      "epoch": 0.5856215107667768,
      "grad_norm": 0.15380732715129852,
      "learning_rate": 0.0008828623518687328,
      "loss": 0.9465,
      "step": 1285
    },
    {
      "epoch": 0.5879001936880484,
      "grad_norm": 0.13570678234100342,
      "learning_rate": 0.0008824065633546035,
      "loss": 0.9147,
      "step": 1290
    },
    {
      "epoch": 0.5901788766093198,
      "grad_norm": 0.12859474122524261,
      "learning_rate": 0.0008819507748404741,
      "loss": 0.9085,
      "step": 1295
    },
    {
      "epoch": 0.5924575595305913,
      "grad_norm": 0.13247829675674438,
      "learning_rate": 0.0008814949863263446,
      "loss": 0.9282,
      "step": 1300
    },
    {
      "epoch": 0.5947362424518629,
      "grad_norm": 0.12720292806625366,
      "learning_rate": 0.0008810391978122152,
      "loss": 0.9192,
      "step": 1305
    },
    {
      "epoch": 0.5970149253731343,
      "grad_norm": 0.12680092453956604,
      "learning_rate": 0.0008805834092980857,
      "loss": 0.8649,
      "step": 1310
    },
    {
      "epoch": 0.5992936082944058,
      "grad_norm": 0.12105657905340195,
      "learning_rate": 0.0008801276207839562,
      "loss": 0.9232,
      "step": 1315
    },
    {
      "epoch": 0.6015722912156773,
      "grad_norm": 0.1268223375082016,
      "learning_rate": 0.0008796718322698268,
      "loss": 0.9174,
      "step": 1320
    },
    {
      "epoch": 0.6038509741369489,
      "grad_norm": 0.16509036719799042,
      "learning_rate": 0.0008792160437556975,
      "loss": 0.9608,
      "step": 1325
    },
    {
      "epoch": 0.6061296570582203,
      "grad_norm": 0.13809671998023987,
      "learning_rate": 0.0008787602552415679,
      "loss": 0.8723,
      "step": 1330
    },
    {
      "epoch": 0.6084083399794918,
      "grad_norm": 0.1288832724094391,
      "learning_rate": 0.0008783044667274385,
      "loss": 0.8901,
      "step": 1335
    },
    {
      "epoch": 0.6106870229007634,
      "grad_norm": 0.13317418098449707,
      "learning_rate": 0.0008778486782133091,
      "loss": 0.8695,
      "step": 1340
    },
    {
      "epoch": 0.6129657058220349,
      "grad_norm": 0.14917926490306854,
      "learning_rate": 0.0008773928896991796,
      "loss": 0.931,
      "step": 1345
    },
    {
      "epoch": 0.6152443887433063,
      "grad_norm": 0.1465471386909485,
      "learning_rate": 0.0008769371011850501,
      "loss": 0.9492,
      "step": 1350
    },
    {
      "epoch": 0.6175230716645779,
      "grad_norm": 0.1285935938358307,
      "learning_rate": 0.0008764813126709206,
      "loss": 0.9031,
      "step": 1355
    },
    {
      "epoch": 0.6198017545858494,
      "grad_norm": 0.1309671849012375,
      "learning_rate": 0.0008760255241567912,
      "loss": 0.9413,
      "step": 1360
    },
    {
      "epoch": 0.6220804375071208,
      "grad_norm": 0.1616896390914917,
      "learning_rate": 0.0008755697356426619,
      "loss": 0.8891,
      "step": 1365
    },
    {
      "epoch": 0.6243591204283924,
      "grad_norm": 0.15325944125652313,
      "learning_rate": 0.0008751139471285325,
      "loss": 0.9326,
      "step": 1370
    },
    {
      "epoch": 0.6266378033496639,
      "grad_norm": 0.135898157954216,
      "learning_rate": 0.0008746581586144029,
      "loss": 0.9201,
      "step": 1375
    },
    {
      "epoch": 0.6289164862709354,
      "grad_norm": 0.15929991006851196,
      "learning_rate": 0.0008742023701002735,
      "loss": 0.9605,
      "step": 1380
    },
    {
      "epoch": 0.6311951691922069,
      "grad_norm": 0.1406155377626419,
      "learning_rate": 0.000873746581586144,
      "loss": 0.8967,
      "step": 1385
    },
    {
      "epoch": 0.6334738521134784,
      "grad_norm": 0.14968961477279663,
      "learning_rate": 0.0008732907930720146,
      "loss": 0.8918,
      "step": 1390
    },
    {
      "epoch": 0.6357525350347499,
      "grad_norm": 0.14663679897785187,
      "learning_rate": 0.0008728350045578851,
      "loss": 0.9399,
      "step": 1395
    },
    {
      "epoch": 0.6380312179560215,
      "grad_norm": 0.14594709873199463,
      "learning_rate": 0.0008723792160437556,
      "loss": 0.946,
      "step": 1400
    },
    {
      "epoch": 0.6403099008772929,
      "grad_norm": 0.13924655318260193,
      "learning_rate": 0.0008719234275296263,
      "loss": 0.9494,
      "step": 1405
    },
    {
      "epoch": 0.6425885837985644,
      "grad_norm": 0.15016880631446838,
      "learning_rate": 0.0008714676390154969,
      "loss": 0.9119,
      "step": 1410
    },
    {
      "epoch": 0.644867266719836,
      "grad_norm": 0.14622029662132263,
      "learning_rate": 0.0008710118505013673,
      "loss": 0.9341,
      "step": 1415
    },
    {
      "epoch": 0.6471459496411074,
      "grad_norm": 0.13982966542243958,
      "learning_rate": 0.0008705560619872379,
      "loss": 0.9108,
      "step": 1420
    },
    {
      "epoch": 0.6494246325623789,
      "grad_norm": 0.14329767227172852,
      "learning_rate": 0.0008701002734731085,
      "loss": 0.9285,
      "step": 1425
    },
    {
      "epoch": 0.6517033154836505,
      "grad_norm": 0.1621554046869278,
      "learning_rate": 0.000869644484958979,
      "loss": 0.9307,
      "step": 1430
    },
    {
      "epoch": 0.653981998404922,
      "grad_norm": 0.14895497262477875,
      "learning_rate": 0.0008691886964448496,
      "loss": 0.9184,
      "step": 1435
    },
    {
      "epoch": 0.6562606813261934,
      "grad_norm": 0.1178068071603775,
      "learning_rate": 0.0008687329079307202,
      "loss": 0.8987,
      "step": 1440
    },
    {
      "epoch": 0.658539364247465,
      "grad_norm": 0.13230739533901215,
      "learning_rate": 0.0008682771194165907,
      "loss": 0.8996,
      "step": 1445
    },
    {
      "epoch": 0.6608180471687365,
      "grad_norm": 0.13261131942272186,
      "learning_rate": 0.0008678213309024613,
      "loss": 0.9731,
      "step": 1450
    },
    {
      "epoch": 0.663096730090008,
      "grad_norm": 0.1370345652103424,
      "learning_rate": 0.0008673655423883319,
      "loss": 0.9156,
      "step": 1455
    },
    {
      "epoch": 0.6653754130112794,
      "grad_norm": 0.12463196367025375,
      "learning_rate": 0.0008669097538742023,
      "loss": 0.8615,
      "step": 1460
    },
    {
      "epoch": 0.667654095932551,
      "grad_norm": 0.15024346113204956,
      "learning_rate": 0.0008664539653600729,
      "loss": 0.9549,
      "step": 1465
    },
    {
      "epoch": 0.6699327788538225,
      "grad_norm": 0.14229421317577362,
      "learning_rate": 0.0008659981768459435,
      "loss": 0.9105,
      "step": 1470
    },
    {
      "epoch": 0.6722114617750939,
      "grad_norm": 0.12410091608762741,
      "learning_rate": 0.000865542388331814,
      "loss": 0.987,
      "step": 1475
    },
    {
      "epoch": 0.6744901446963655,
      "grad_norm": 0.15418703854084015,
      "learning_rate": 0.0008650865998176846,
      "loss": 0.8625,
      "step": 1480
    },
    {
      "epoch": 0.676768827617637,
      "grad_norm": 0.19048985838890076,
      "learning_rate": 0.0008646308113035552,
      "loss": 0.8892,
      "step": 1485
    },
    {
      "epoch": 0.6790475105389085,
      "grad_norm": 0.16257400810718536,
      "learning_rate": 0.0008641750227894257,
      "loss": 0.9325,
      "step": 1490
    },
    {
      "epoch": 0.68132619346018,
      "grad_norm": 0.17544066905975342,
      "learning_rate": 0.0008637192342752963,
      "loss": 0.8445,
      "step": 1495
    },
    {
      "epoch": 0.6836048763814515,
      "grad_norm": 0.12358692288398743,
      "learning_rate": 0.0008632634457611669,
      "loss": 0.9307,
      "step": 1500
    },
    {
      "epoch": 0.685883559302723,
      "grad_norm": 0.166404590010643,
      "learning_rate": 0.0008628076572470373,
      "loss": 0.9794,
      "step": 1505
    },
    {
      "epoch": 0.6881622422239946,
      "grad_norm": 0.13778018951416016,
      "learning_rate": 0.0008623518687329079,
      "loss": 0.927,
      "step": 1510
    },
    {
      "epoch": 0.690440925145266,
      "grad_norm": 0.13926957547664642,
      "learning_rate": 0.0008618960802187786,
      "loss": 0.9445,
      "step": 1515
    },
    {
      "epoch": 0.6927196080665375,
      "grad_norm": 0.1191454827785492,
      "learning_rate": 0.0008614402917046491,
      "loss": 0.8585,
      "step": 1520
    },
    {
      "epoch": 0.6949982909878091,
      "grad_norm": 0.12737198173999786,
      "learning_rate": 0.0008609845031905196,
      "loss": 0.9427,
      "step": 1525
    },
    {
      "epoch": 0.6972769739090805,
      "grad_norm": 0.12932349741458893,
      "learning_rate": 0.0008605287146763902,
      "loss": 0.9167,
      "step": 1530
    },
    {
      "epoch": 0.699555656830352,
      "grad_norm": 0.13123835623264313,
      "learning_rate": 0.0008600729261622607,
      "loss": 0.8945,
      "step": 1535
    },
    {
      "epoch": 0.7018343397516236,
      "grad_norm": 0.1524791717529297,
      "learning_rate": 0.0008596171376481313,
      "loss": 0.9622,
      "step": 1540
    },
    {
      "epoch": 0.7041130226728951,
      "grad_norm": 0.13369344174861908,
      "learning_rate": 0.0008591613491340018,
      "loss": 0.8811,
      "step": 1545
    },
    {
      "epoch": 0.7063917055941665,
      "grad_norm": 0.1311752200126648,
      "learning_rate": 0.0008587055606198723,
      "loss": 0.9211,
      "step": 1550
    },
    {
      "epoch": 0.7086703885154381,
      "grad_norm": 0.12036149948835373,
      "learning_rate": 0.000858249772105743,
      "loss": 0.9142,
      "step": 1555
    },
    {
      "epoch": 0.7109490714367096,
      "grad_norm": 0.1449228972196579,
      "learning_rate": 0.0008577939835916136,
      "loss": 0.9257,
      "step": 1560
    },
    {
      "epoch": 0.7132277543579811,
      "grad_norm": 0.14967447519302368,
      "learning_rate": 0.0008573381950774841,
      "loss": 0.9249,
      "step": 1565
    },
    {
      "epoch": 0.7155064372792526,
      "grad_norm": 0.1446738988161087,
      "learning_rate": 0.0008568824065633546,
      "loss": 0.9241,
      "step": 1570
    },
    {
      "epoch": 0.7177851202005241,
      "grad_norm": 0.13602711260318756,
      "learning_rate": 0.0008564266180492252,
      "loss": 0.9577,
      "step": 1575
    },
    {
      "epoch": 0.7200638031217956,
      "grad_norm": 0.1191105842590332,
      "learning_rate": 0.0008559708295350957,
      "loss": 0.9147,
      "step": 1580
    },
    {
      "epoch": 0.7223424860430671,
      "grad_norm": 0.1357746571302414,
      "learning_rate": 0.0008555150410209663,
      "loss": 0.937,
      "step": 1585
    },
    {
      "epoch": 0.7246211689643386,
      "grad_norm": 0.15246951580047607,
      "learning_rate": 0.0008550592525068368,
      "loss": 0.9562,
      "step": 1590
    },
    {
      "epoch": 0.7268998518856101,
      "grad_norm": 0.1267576515674591,
      "learning_rate": 0.0008546034639927074,
      "loss": 0.8918,
      "step": 1595
    },
    {
      "epoch": 0.7291785348068817,
      "grad_norm": 0.14655396342277527,
      "learning_rate": 0.000854147675478578,
      "loss": 0.9598,
      "step": 1600
    },
    {
      "epoch": 0.7314572177281531,
      "grad_norm": 0.13260279595851898,
      "learning_rate": 0.0008536918869644486,
      "loss": 0.8548,
      "step": 1605
    },
    {
      "epoch": 0.7337359006494246,
      "grad_norm": 0.13540731370449066,
      "learning_rate": 0.000853236098450319,
      "loss": 0.9462,
      "step": 1610
    },
    {
      "epoch": 0.7360145835706962,
      "grad_norm": 0.13684846460819244,
      "learning_rate": 0.0008527803099361896,
      "loss": 0.8882,
      "step": 1615
    },
    {
      "epoch": 0.7382932664919677,
      "grad_norm": 0.14281310141086578,
      "learning_rate": 0.0008523245214220602,
      "loss": 0.8981,
      "step": 1620
    },
    {
      "epoch": 0.7405719494132391,
      "grad_norm": 0.1365939825773239,
      "learning_rate": 0.0008518687329079307,
      "loss": 0.8922,
      "step": 1625
    },
    {
      "epoch": 0.7428506323345107,
      "grad_norm": 0.13656771183013916,
      "learning_rate": 0.0008514129443938014,
      "loss": 0.9143,
      "step": 1630
    },
    {
      "epoch": 0.7451293152557822,
      "grad_norm": 0.13790377974510193,
      "learning_rate": 0.0008509571558796719,
      "loss": 0.8343,
      "step": 1635
    },
    {
      "epoch": 0.7474079981770536,
      "grad_norm": 0.14255569875240326,
      "learning_rate": 0.0008505013673655424,
      "loss": 0.8972,
      "step": 1640
    },
    {
      "epoch": 0.7496866810983251,
      "grad_norm": 0.14300860464572906,
      "learning_rate": 0.000850045578851413,
      "loss": 0.9111,
      "step": 1645
    },
    {
      "epoch": 0.7519653640195967,
      "grad_norm": 0.15205678343772888,
      "learning_rate": 0.0008495897903372836,
      "loss": 0.9293,
      "step": 1650
    },
    {
      "epoch": 0.7542440469408682,
      "grad_norm": 0.14500831067562103,
      "learning_rate": 0.000849134001823154,
      "loss": 0.8558,
      "step": 1655
    },
    {
      "epoch": 0.7565227298621396,
      "grad_norm": 0.1427745819091797,
      "learning_rate": 0.0008486782133090246,
      "loss": 0.9494,
      "step": 1660
    },
    {
      "epoch": 0.7588014127834112,
      "grad_norm": 0.13449975848197937,
      "learning_rate": 0.0008482224247948952,
      "loss": 0.8681,
      "step": 1665
    },
    {
      "epoch": 0.7610800957046827,
      "grad_norm": 0.1576683074235916,
      "learning_rate": 0.0008477666362807658,
      "loss": 0.9381,
      "step": 1670
    },
    {
      "epoch": 0.7633587786259542,
      "grad_norm": 0.17333069443702698,
      "learning_rate": 0.0008473108477666363,
      "loss": 0.9048,
      "step": 1675
    },
    {
      "epoch": 0.7656374615472257,
      "grad_norm": 0.1646099090576172,
      "learning_rate": 0.0008468550592525069,
      "loss": 0.9345,
      "step": 1680
    },
    {
      "epoch": 0.7679161444684972,
      "grad_norm": 0.15910065174102783,
      "learning_rate": 0.0008463992707383774,
      "loss": 0.8786,
      "step": 1685
    },
    {
      "epoch": 0.7701948273897687,
      "grad_norm": 0.1477038413286209,
      "learning_rate": 0.000845943482224248,
      "loss": 0.9311,
      "step": 1690
    },
    {
      "epoch": 0.7724735103110402,
      "grad_norm": 0.13789674639701843,
      "learning_rate": 0.0008454876937101186,
      "loss": 0.9016,
      "step": 1695
    },
    {
      "epoch": 0.7747521932323117,
      "grad_norm": 0.15997081995010376,
      "learning_rate": 0.000845031905195989,
      "loss": 0.873,
      "step": 1700
    },
    {
      "epoch": 0.7770308761535832,
      "grad_norm": 0.15340040624141693,
      "learning_rate": 0.0008445761166818596,
      "loss": 0.8693,
      "step": 1705
    },
    {
      "epoch": 0.7793095590748548,
      "grad_norm": 0.14968164265155792,
      "learning_rate": 0.0008441203281677303,
      "loss": 0.9554,
      "step": 1710
    },
    {
      "epoch": 0.7815882419961262,
      "grad_norm": 0.16490007936954498,
      "learning_rate": 0.0008436645396536008,
      "loss": 0.9696,
      "step": 1715
    },
    {
      "epoch": 0.7838669249173977,
      "grad_norm": 0.14489418268203735,
      "learning_rate": 0.0008432087511394713,
      "loss": 0.9845,
      "step": 1720
    },
    {
      "epoch": 0.7861456078386693,
      "grad_norm": 0.14899364113807678,
      "learning_rate": 0.0008427529626253418,
      "loss": 0.9589,
      "step": 1725
    },
    {
      "epoch": 0.7884242907599408,
      "grad_norm": 0.14395561814308167,
      "learning_rate": 0.0008422971741112124,
      "loss": 0.9208,
      "step": 1730
    },
    {
      "epoch": 0.7907029736812122,
      "grad_norm": 0.13319964706897736,
      "learning_rate": 0.000841841385597083,
      "loss": 0.9302,
      "step": 1735
    },
    {
      "epoch": 0.7929816566024838,
      "grad_norm": 0.13878197968006134,
      "learning_rate": 0.0008413855970829534,
      "loss": 0.9135,
      "step": 1740
    },
    {
      "epoch": 0.7952603395237553,
      "grad_norm": 0.1744093894958496,
      "learning_rate": 0.000840929808568824,
      "loss": 0.8953,
      "step": 1745
    },
    {
      "epoch": 0.7975390224450267,
      "grad_norm": 0.1359575241804123,
      "learning_rate": 0.0008404740200546947,
      "loss": 0.8985,
      "step": 1750
    },
    {
      "epoch": 0.7998177053662983,
      "grad_norm": 0.15856248140335083,
      "learning_rate": 0.0008400182315405653,
      "loss": 0.9338,
      "step": 1755
    },
    {
      "epoch": 0.8020963882875698,
      "grad_norm": 0.13436023890972137,
      "learning_rate": 0.0008395624430264357,
      "loss": 0.8757,
      "step": 1760
    },
    {
      "epoch": 0.8043750712088413,
      "grad_norm": 0.15711069107055664,
      "learning_rate": 0.0008391066545123063,
      "loss": 0.8649,
      "step": 1765
    },
    {
      "epoch": 0.8066537541301128,
      "grad_norm": 0.18393848836421967,
      "learning_rate": 0.0008386508659981768,
      "loss": 0.9557,
      "step": 1770
    },
    {
      "epoch": 0.8089324370513843,
      "grad_norm": 0.16089868545532227,
      "learning_rate": 0.0008381950774840474,
      "loss": 0.9478,
      "step": 1775
    },
    {
      "epoch": 0.8112111199726558,
      "grad_norm": 0.14547842741012573,
      "learning_rate": 0.000837739288969918,
      "loss": 0.9637,
      "step": 1780
    },
    {
      "epoch": 0.8134898028939274,
      "grad_norm": 0.13979864120483398,
      "learning_rate": 0.0008372835004557885,
      "loss": 0.9775,
      "step": 1785
    },
    {
      "epoch": 0.8157684858151988,
      "grad_norm": 0.159184068441391,
      "learning_rate": 0.0008368277119416591,
      "loss": 0.8969,
      "step": 1790
    },
    {
      "epoch": 0.8180471687364703,
      "grad_norm": 0.14917801320552826,
      "learning_rate": 0.0008363719234275297,
      "loss": 0.9058,
      "step": 1795
    },
    {
      "epoch": 0.8203258516577419,
      "grad_norm": 0.16008202731609344,
      "learning_rate": 0.0008359161349134002,
      "loss": 0.8878,
      "step": 1800
    },
    {
      "epoch": 0.8226045345790133,
      "grad_norm": 0.14713391661643982,
      "learning_rate": 0.0008354603463992707,
      "loss": 0.9096,
      "step": 1805
    },
    {
      "epoch": 0.8248832175002848,
      "grad_norm": 0.1349904090166092,
      "learning_rate": 0.0008350045578851413,
      "loss": 0.8728,
      "step": 1810
    },
    {
      "epoch": 0.8271619004215564,
      "grad_norm": 0.12233688682317734,
      "learning_rate": 0.0008345487693710118,
      "loss": 0.9212,
      "step": 1815
    },
    {
      "epoch": 0.8294405833428279,
      "grad_norm": 0.12475632131099701,
      "learning_rate": 0.0008340929808568824,
      "loss": 0.8953,
      "step": 1820
    },
    {
      "epoch": 0.8317192662640993,
      "grad_norm": 0.16355104744434357,
      "learning_rate": 0.000833637192342753,
      "loss": 1.0108,
      "step": 1825
    },
    {
      "epoch": 0.8339979491853708,
      "grad_norm": 0.16167505085468292,
      "learning_rate": 0.0008331814038286235,
      "loss": 0.937,
      "step": 1830
    },
    {
      "epoch": 0.8362766321066424,
      "grad_norm": 0.15243341028690338,
      "learning_rate": 0.0008327256153144941,
      "loss": 0.9009,
      "step": 1835
    },
    {
      "epoch": 0.8385553150279139,
      "grad_norm": 0.1594979614019394,
      "learning_rate": 0.0008322698268003647,
      "loss": 0.9324,
      "step": 1840
    },
    {
      "epoch": 0.8408339979491853,
      "grad_norm": 0.13959600031375885,
      "learning_rate": 0.0008318140382862352,
      "loss": 0.8666,
      "step": 1845
    },
    {
      "epoch": 0.8431126808704569,
      "grad_norm": 0.16554296016693115,
      "learning_rate": 0.0008313582497721057,
      "loss": 0.9279,
      "step": 1850
    },
    {
      "epoch": 0.8453913637917284,
      "grad_norm": 0.1296825408935547,
      "learning_rate": 0.0008309024612579763,
      "loss": 0.8666,
      "step": 1855
    },
    {
      "epoch": 0.8476700467129998,
      "grad_norm": 0.139198437333107,
      "learning_rate": 0.0008304466727438468,
      "loss": 0.8998,
      "step": 1860
    },
    {
      "epoch": 0.8499487296342714,
      "grad_norm": 0.15941818058490753,
      "learning_rate": 0.0008299908842297175,
      "loss": 0.8924,
      "step": 1865
    },
    {
      "epoch": 0.8522274125555429,
      "grad_norm": 0.1334047168493271,
      "learning_rate": 0.000829535095715588,
      "loss": 0.9426,
      "step": 1870
    },
    {
      "epoch": 0.8545060954768144,
      "grad_norm": 0.14410072565078735,
      "learning_rate": 0.0008290793072014585,
      "loss": 0.9045,
      "step": 1875
    },
    {
      "epoch": 0.8567847783980859,
      "grad_norm": 0.16626960039138794,
      "learning_rate": 0.0008286235186873291,
      "loss": 0.9747,
      "step": 1880
    },
    {
      "epoch": 0.8590634613193574,
      "grad_norm": 0.17085514962673187,
      "learning_rate": 0.0008281677301731997,
      "loss": 0.9264,
      "step": 1885
    },
    {
      "epoch": 0.8613421442406289,
      "grad_norm": 0.13531719148159027,
      "learning_rate": 0.0008277119416590701,
      "loss": 0.9292,
      "step": 1890
    },
    {
      "epoch": 0.8636208271619005,
      "grad_norm": 0.15169748663902283,
      "learning_rate": 0.0008272561531449407,
      "loss": 0.9995,
      "step": 1895
    },
    {
      "epoch": 0.8658995100831719,
      "grad_norm": 0.16661810874938965,
      "learning_rate": 0.0008268003646308114,
      "loss": 0.9651,
      "step": 1900
    },
    {
      "epoch": 0.8681781930044434,
      "grad_norm": 0.12726424634456635,
      "learning_rate": 0.0008263445761166819,
      "loss": 0.8952,
      "step": 1905
    },
    {
      "epoch": 0.870456875925715,
      "grad_norm": 0.1456633061170578,
      "learning_rate": 0.0008258887876025525,
      "loss": 0.9613,
      "step": 1910
    },
    {
      "epoch": 0.8727355588469864,
      "grad_norm": 0.13850876688957214,
      "learning_rate": 0.000825432999088423,
      "loss": 0.9021,
      "step": 1915
    },
    {
      "epoch": 0.8750142417682579,
      "grad_norm": 0.14792943000793457,
      "learning_rate": 0.0008249772105742935,
      "loss": 0.9305,
      "step": 1920
    },
    {
      "epoch": 0.8772929246895295,
      "grad_norm": 0.14341749250888824,
      "learning_rate": 0.0008245214220601641,
      "loss": 0.8876,
      "step": 1925
    },
    {
      "epoch": 0.879571607610801,
      "grad_norm": 0.13790662586688995,
      "learning_rate": 0.0008240656335460347,
      "loss": 0.8915,
      "step": 1930
    },
    {
      "epoch": 0.8818502905320724,
      "grad_norm": 0.12383104115724564,
      "learning_rate": 0.0008236098450319051,
      "loss": 0.8355,
      "step": 1935
    },
    {
      "epoch": 0.884128973453344,
      "grad_norm": 0.16112405061721802,
      "learning_rate": 0.0008231540565177758,
      "loss": 0.9386,
      "step": 1940
    },
    {
      "epoch": 0.8864076563746155,
      "grad_norm": 0.13996581733226776,
      "learning_rate": 0.0008226982680036464,
      "loss": 0.9462,
      "step": 1945
    },
    {
      "epoch": 0.888686339295887,
      "grad_norm": 0.15115101635456085,
      "learning_rate": 0.0008222424794895169,
      "loss": 0.9856,
      "step": 1950
    },
    {
      "epoch": 0.8909650222171585,
      "grad_norm": 0.1696198731660843,
      "learning_rate": 0.0008217866909753874,
      "loss": 0.9393,
      "step": 1955
    },
    {
      "epoch": 0.89324370513843,
      "grad_norm": 0.14121383428573608,
      "learning_rate": 0.000821330902461258,
      "loss": 0.8964,
      "step": 1960
    },
    {
      "epoch": 0.8955223880597015,
      "grad_norm": 0.15473255515098572,
      "learning_rate": 0.0008208751139471285,
      "loss": 0.9195,
      "step": 1965
    },
    {
      "epoch": 0.897801070980973,
      "grad_norm": 0.14964784681797028,
      "learning_rate": 0.0008204193254329991,
      "loss": 0.9705,
      "step": 1970
    },
    {
      "epoch": 0.9000797539022445,
      "grad_norm": 0.12608389556407928,
      "learning_rate": 0.0008199635369188698,
      "loss": 0.8264,
      "step": 1975
    },
    {
      "epoch": 0.902358436823516,
      "grad_norm": 0.1328454166650772,
      "learning_rate": 0.0008195077484047402,
      "loss": 0.8757,
      "step": 1980
    },
    {
      "epoch": 0.9046371197447876,
      "grad_norm": 0.13779884576797485,
      "learning_rate": 0.0008190519598906108,
      "loss": 0.8763,
      "step": 1985
    },
    {
      "epoch": 0.906915802666059,
      "grad_norm": 0.1510530561208725,
      "learning_rate": 0.0008185961713764814,
      "loss": 0.9735,
      "step": 1990
    },
    {
      "epoch": 0.9091944855873305,
      "grad_norm": 0.14289920032024384,
      "learning_rate": 0.0008181403828623519,
      "loss": 0.9473,
      "step": 1995
    },
    {
      "epoch": 0.911473168508602,
      "grad_norm": 0.14802832901477814,
      "learning_rate": 0.0008176845943482224,
      "loss": 0.8988,
      "step": 2000
    },
    {
      "epoch": 0.9137518514298736,
      "grad_norm": 0.15319262444972992,
      "learning_rate": 0.000817228805834093,
      "loss": 0.985,
      "step": 2005
    },
    {
      "epoch": 0.916030534351145,
      "grad_norm": 0.15308907628059387,
      "learning_rate": 0.0008167730173199635,
      "loss": 0.8806,
      "step": 2010
    },
    {
      "epoch": 0.9183092172724165,
      "grad_norm": 0.1483459621667862,
      "learning_rate": 0.0008163172288058342,
      "loss": 0.9078,
      "step": 2015
    },
    {
      "epoch": 0.9205879001936881,
      "grad_norm": 0.16430041193962097,
      "learning_rate": 0.0008158614402917047,
      "loss": 0.9653,
      "step": 2020
    },
    {
      "epoch": 0.9228665831149595,
      "grad_norm": 0.14540952444076538,
      "learning_rate": 0.0008154056517775752,
      "loss": 0.9327,
      "step": 2025
    },
    {
      "epoch": 0.925145266036231,
      "grad_norm": 0.1570722758769989,
      "learning_rate": 0.0008149498632634458,
      "loss": 0.8835,
      "step": 2030
    },
    {
      "epoch": 0.9274239489575026,
      "grad_norm": 0.15849198400974274,
      "learning_rate": 0.0008144940747493164,
      "loss": 0.9945,
      "step": 2035
    },
    {
      "epoch": 0.9297026318787741,
      "grad_norm": 0.14927251636981964,
      "learning_rate": 0.0008140382862351869,
      "loss": 0.8583,
      "step": 2040
    },
    {
      "epoch": 0.9319813148000455,
      "grad_norm": 0.1434575766324997,
      "learning_rate": 0.0008135824977210574,
      "loss": 0.8762,
      "step": 2045
    },
    {
      "epoch": 0.9342599977213171,
      "grad_norm": 0.14729444682598114,
      "learning_rate": 0.000813126709206928,
      "loss": 0.9367,
      "step": 2050
    },
    {
      "epoch": 0.9365386806425886,
      "grad_norm": 0.14349853992462158,
      "learning_rate": 0.0008126709206927986,
      "loss": 0.9137,
      "step": 2055
    },
    {
      "epoch": 0.9388173635638601,
      "grad_norm": 0.15456293523311615,
      "learning_rate": 0.0008122151321786692,
      "loss": 0.9223,
      "step": 2060
    },
    {
      "epoch": 0.9410960464851316,
      "grad_norm": 0.1556033045053482,
      "learning_rate": 0.0008117593436645396,
      "loss": 0.9508,
      "step": 2065
    },
    {
      "epoch": 0.9433747294064031,
      "grad_norm": 0.13937783241271973,
      "learning_rate": 0.0008113035551504102,
      "loss": 0.9212,
      "step": 2070
    },
    {
      "epoch": 0.9456534123276746,
      "grad_norm": 0.13846909999847412,
      "learning_rate": 0.0008108477666362808,
      "loss": 0.9113,
      "step": 2075
    },
    {
      "epoch": 0.9479320952489461,
      "grad_norm": 0.15579812228679657,
      "learning_rate": 0.0008103919781221513,
      "loss": 0.9229,
      "step": 2080
    },
    {
      "epoch": 0.9502107781702176,
      "grad_norm": 0.1532416045665741,
      "learning_rate": 0.0008099361896080218,
      "loss": 0.8328,
      "step": 2085
    },
    {
      "epoch": 0.9524894610914891,
      "grad_norm": 0.13696099817752838,
      "learning_rate": 0.0008094804010938924,
      "loss": 0.9143,
      "step": 2090
    },
    {
      "epoch": 0.9547681440127607,
      "grad_norm": 0.14125265181064606,
      "learning_rate": 0.000809024612579763,
      "loss": 0.9277,
      "step": 2095
    },
    {
      "epoch": 0.9570468269340321,
      "grad_norm": 0.16074562072753906,
      "learning_rate": 0.0008085688240656336,
      "loss": 0.9131,
      "step": 2100
    },
    {
      "epoch": 0.9593255098553036,
      "grad_norm": 0.13254640996456146,
      "learning_rate": 0.0008081130355515042,
      "loss": 0.8898,
      "step": 2105
    },
    {
      "epoch": 0.9616041927765752,
      "grad_norm": 0.14763005077838898,
      "learning_rate": 0.0008076572470373746,
      "loss": 0.9221,
      "step": 2110
    },
    {
      "epoch": 0.9638828756978467,
      "grad_norm": 0.16294661164283752,
      "learning_rate": 0.0008072014585232452,
      "loss": 0.8297,
      "step": 2115
    },
    {
      "epoch": 0.9661615586191181,
      "grad_norm": 0.132388636469841,
      "learning_rate": 0.0008067456700091158,
      "loss": 0.9123,
      "step": 2120
    },
    {
      "epoch": 0.9684402415403897,
      "grad_norm": 0.1336141675710678,
      "learning_rate": 0.0008062898814949863,
      "loss": 0.9098,
      "step": 2125
    },
    {
      "epoch": 0.9707189244616612,
      "grad_norm": 0.1307249814271927,
      "learning_rate": 0.0008058340929808569,
      "loss": 0.8132,
      "step": 2130
    },
    {
      "epoch": 0.9729976073829326,
      "grad_norm": 0.15206436812877655,
      "learning_rate": 0.0008053783044667275,
      "loss": 0.9074,
      "step": 2135
    },
    {
      "epoch": 0.9752762903042042,
      "grad_norm": 0.1488904505968094,
      "learning_rate": 0.000804922515952598,
      "loss": 0.8374,
      "step": 2140
    },
    {
      "epoch": 0.9775549732254757,
      "grad_norm": 0.14320574700832367,
      "learning_rate": 0.0008044667274384686,
      "loss": 0.8926,
      "step": 2145
    },
    {
      "epoch": 0.9798336561467472,
      "grad_norm": 0.17744623124599457,
      "learning_rate": 0.0008040109389243391,
      "loss": 0.9521,
      "step": 2150
    },
    {
      "epoch": 0.9821123390680186,
      "grad_norm": 0.14672499895095825,
      "learning_rate": 0.0008035551504102096,
      "loss": 0.91,
      "step": 2155
    },
    {
      "epoch": 0.9843910219892902,
      "grad_norm": 0.16193920373916626,
      "learning_rate": 0.0008030993618960802,
      "loss": 0.8889,
      "step": 2160
    },
    {
      "epoch": 0.9866697049105617,
      "grad_norm": 0.14069753885269165,
      "learning_rate": 0.0008026435733819508,
      "loss": 0.8865,
      "step": 2165
    },
    {
      "epoch": 0.9889483878318333,
      "grad_norm": 0.14819614589214325,
      "learning_rate": 0.0008021877848678214,
      "loss": 0.9536,
      "step": 2170
    },
    {
      "epoch": 0.9912270707531047,
      "grad_norm": 0.1371755748987198,
      "learning_rate": 0.0008017319963536919,
      "loss": 0.9551,
      "step": 2175
    },
    {
      "epoch": 0.9935057536743762,
      "grad_norm": 0.16888082027435303,
      "learning_rate": 0.0008012762078395625,
      "loss": 1.004,
      "step": 2180
    },
    {
      "epoch": 0.9957844365956477,
      "grad_norm": 0.128937229514122,
      "learning_rate": 0.000800820419325433,
      "loss": 0.9103,
      "step": 2185
    },
    {
      "epoch": 0.9980631195169192,
      "grad_norm": 0.17961503565311432,
      "learning_rate": 0.0008003646308113036,
      "loss": 0.8782,
      "step": 2190
    },
    {
      "epoch": 0.9998860658539365,
      "eval_loss": 0.8023901581764221,
      "eval_runtime": 793.4112,
      "eval_samples_per_second": 25.283,
      "eval_steps_per_second": 3.161,
      "step": 2194
    },
    {
      "epoch": 1.0003418024381907,
      "grad_norm": 0.2012586146593094,
      "learning_rate": 0.0007999088422971741,
      "loss": 0.9219,
      "step": 2195
    },
    {
      "epoch": 1.0026204853594622,
      "grad_norm": 0.18397094309329987,
      "learning_rate": 0.0007994530537830446,
      "loss": 0.9599,
      "step": 2200
    },
    {
      "epoch": 1.0048991682807338,
      "grad_norm": 0.1385863870382309,
      "learning_rate": 0.0007989972652689152,
      "loss": 0.9437,
      "step": 2205
    },
    {
      "epoch": 1.0071778512020053,
      "grad_norm": 0.1403636336326599,
      "learning_rate": 0.0007985414767547859,
      "loss": 0.9083,
      "step": 2210
    },
    {
      "epoch": 1.0094565341232768,
      "grad_norm": 0.16133703291416168,
      "learning_rate": 0.0007980856882406563,
      "loss": 0.9331,
      "step": 2215
    },
    {
      "epoch": 1.0117352170445482,
      "grad_norm": 0.14801166951656342,
      "learning_rate": 0.0007976298997265269,
      "loss": 0.878,
      "step": 2220
    },
    {
      "epoch": 1.0140138999658197,
      "grad_norm": 0.1446535587310791,
      "learning_rate": 0.0007971741112123975,
      "loss": 0.8463,
      "step": 2225
    },
    {
      "epoch": 1.0162925828870912,
      "grad_norm": 0.13603852689266205,
      "learning_rate": 0.000796718322698268,
      "loss": 0.8925,
      "step": 2230
    },
    {
      "epoch": 1.0185712658083628,
      "grad_norm": 0.15264029800891876,
      "learning_rate": 0.0007962625341841386,
      "loss": 0.9233,
      "step": 2235
    },
    {
      "epoch": 1.0208499487296343,
      "grad_norm": 0.15334424376487732,
      "learning_rate": 0.0007958067456700091,
      "loss": 0.924,
      "step": 2240
    },
    {
      "epoch": 1.0231286316509058,
      "grad_norm": 0.1449173390865326,
      "learning_rate": 0.0007953509571558797,
      "loss": 0.8939,
      "step": 2245
    },
    {
      "epoch": 1.0254073145721774,
      "grad_norm": 0.14219972491264343,
      "learning_rate": 0.0007948951686417503,
      "loss": 0.8434,
      "step": 2250
    },
    {
      "epoch": 1.0276859974934487,
      "grad_norm": 0.14481359720230103,
      "learning_rate": 0.0007944393801276209,
      "loss": 0.847,
      "step": 2255
    },
    {
      "epoch": 1.0299646804147202,
      "grad_norm": 0.1704932451248169,
      "learning_rate": 0.0007939835916134913,
      "loss": 0.9526,
      "step": 2260
    },
    {
      "epoch": 1.0322433633359918,
      "grad_norm": 0.1365237683057785,
      "learning_rate": 0.0007935278030993619,
      "loss": 0.8751,
      "step": 2265
    },
    {
      "epoch": 1.0345220462572633,
      "grad_norm": 0.13875068724155426,
      "learning_rate": 0.0007930720145852325,
      "loss": 0.8438,
      "step": 2270
    },
    {
      "epoch": 1.0368007291785348,
      "grad_norm": 0.1457943171262741,
      "learning_rate": 0.000792616226071103,
      "loss": 0.8638,
      "step": 2275
    },
    {
      "epoch": 1.0390794120998064,
      "grad_norm": 0.1472889482975006,
      "learning_rate": 0.0007921604375569735,
      "loss": 0.8768,
      "step": 2280
    },
    {
      "epoch": 1.041358095021078,
      "grad_norm": 0.16338159143924713,
      "learning_rate": 0.0007917046490428442,
      "loss": 0.8815,
      "step": 2285
    },
    {
      "epoch": 1.0436367779423494,
      "grad_norm": 0.1584775298833847,
      "learning_rate": 0.0007912488605287147,
      "loss": 0.967,
      "step": 2290
    },
    {
      "epoch": 1.0459154608636207,
      "grad_norm": 0.15277639031410217,
      "learning_rate": 0.0007907930720145853,
      "loss": 0.87,
      "step": 2295
    },
    {
      "epoch": 1.0481941437848923,
      "grad_norm": 0.1390099674463272,
      "learning_rate": 0.0007903372835004558,
      "loss": 0.8793,
      "step": 2300
    },
    {
      "epoch": 1.0504728267061638,
      "grad_norm": 0.14645035564899445,
      "learning_rate": 0.0007898814949863263,
      "loss": 0.8459,
      "step": 2305
    },
    {
      "epoch": 1.0527515096274354,
      "grad_norm": 0.16654400527477264,
      "learning_rate": 0.0007894257064721969,
      "loss": 0.9184,
      "step": 2310
    },
    {
      "epoch": 1.055030192548707,
      "grad_norm": 0.14529049396514893,
      "learning_rate": 0.0007889699179580675,
      "loss": 0.8599,
      "step": 2315
    },
    {
      "epoch": 1.0573088754699784,
      "grad_norm": 0.15471410751342773,
      "learning_rate": 0.000788514129443938,
      "loss": 0.8705,
      "step": 2320
    },
    {
      "epoch": 1.05958755839125,
      "grad_norm": 0.15342718362808228,
      "learning_rate": 0.0007880583409298086,
      "loss": 0.9464,
      "step": 2325
    },
    {
      "epoch": 1.0618662413125213,
      "grad_norm": 0.1719491183757782,
      "learning_rate": 0.0007876025524156792,
      "loss": 0.894,
      "step": 2330
    },
    {
      "epoch": 1.0641449242337928,
      "grad_norm": 0.15610603988170624,
      "learning_rate": 0.0007871467639015497,
      "loss": 0.8949,
      "step": 2335
    },
    {
      "epoch": 1.0664236071550643,
      "grad_norm": 0.145268514752388,
      "learning_rate": 0.0007866909753874203,
      "loss": 0.9512,
      "step": 2340
    },
    {
      "epoch": 1.0687022900763359,
      "grad_norm": 0.13046854734420776,
      "learning_rate": 0.0007862351868732908,
      "loss": 0.8902,
      "step": 2345
    },
    {
      "epoch": 1.0709809729976074,
      "grad_norm": 0.1520213633775711,
      "learning_rate": 0.0007857793983591613,
      "loss": 0.956,
      "step": 2350
    },
    {
      "epoch": 1.073259655918879,
      "grad_norm": 0.1622418314218521,
      "learning_rate": 0.0007853236098450319,
      "loss": 0.9099,
      "step": 2355
    },
    {
      "epoch": 1.0755383388401505,
      "grad_norm": 0.15852409601211548,
      "learning_rate": 0.0007848678213309026,
      "loss": 0.8988,
      "step": 2360
    },
    {
      "epoch": 1.0778170217614218,
      "grad_norm": 0.15294630825519562,
      "learning_rate": 0.000784412032816773,
      "loss": 0.9563,
      "step": 2365
    },
    {
      "epoch": 1.0800957046826933,
      "grad_norm": 0.1580750048160553,
      "learning_rate": 0.0007839562443026436,
      "loss": 0.8704,
      "step": 2370
    },
    {
      "epoch": 1.0823743876039649,
      "grad_norm": 0.15936365723609924,
      "learning_rate": 0.0007835004557885142,
      "loss": 0.8932,
      "step": 2375
    },
    {
      "epoch": 1.0846530705252364,
      "grad_norm": 0.15247249603271484,
      "learning_rate": 0.0007830446672743847,
      "loss": 0.8759,
      "step": 2380
    },
    {
      "epoch": 1.086931753446508,
      "grad_norm": 0.170144721865654,
      "learning_rate": 0.0007825888787602553,
      "loss": 0.8779,
      "step": 2385
    },
    {
      "epoch": 1.0892104363677795,
      "grad_norm": 0.16903235018253326,
      "learning_rate": 0.0007821330902461257,
      "loss": 0.9092,
      "step": 2390
    },
    {
      "epoch": 1.091489119289051,
      "grad_norm": 0.1421811729669571,
      "learning_rate": 0.0007816773017319963,
      "loss": 0.8128,
      "step": 2395
    },
    {
      "epoch": 1.0937678022103223,
      "grad_norm": 0.17226701974868774,
      "learning_rate": 0.000781221513217867,
      "loss": 0.9017,
      "step": 2400
    },
    {
      "epoch": 1.0960464851315939,
      "grad_norm": 0.14640824496746063,
      "learning_rate": 0.0007807657247037376,
      "loss": 0.8865,
      "step": 2405
    },
    {
      "epoch": 1.0983251680528654,
      "grad_norm": 0.14705024659633636,
      "learning_rate": 0.000780309936189608,
      "loss": 0.8695,
      "step": 2410
    },
    {
      "epoch": 1.100603850974137,
      "grad_norm": 0.15635466575622559,
      "learning_rate": 0.0007798541476754786,
      "loss": 0.9076,
      "step": 2415
    },
    {
      "epoch": 1.1028825338954085,
      "grad_norm": 0.1412791609764099,
      "learning_rate": 0.0007793983591613491,
      "loss": 0.9115,
      "step": 2420
    },
    {
      "epoch": 1.10516121681668,
      "grad_norm": 0.15122833847999573,
      "learning_rate": 0.0007789425706472197,
      "loss": 0.9102,
      "step": 2425
    },
    {
      "epoch": 1.1074398997379515,
      "grad_norm": 0.14427447319030762,
      "learning_rate": 0.0007784867821330902,
      "loss": 0.8772,
      "step": 2430
    },
    {
      "epoch": 1.109718582659223,
      "grad_norm": 0.14267009496688843,
      "learning_rate": 0.0007780309936189607,
      "loss": 0.9173,
      "step": 2435
    },
    {
      "epoch": 1.1119972655804944,
      "grad_norm": 0.14030992984771729,
      "learning_rate": 0.0007775752051048314,
      "loss": 0.8981,
      "step": 2440
    },
    {
      "epoch": 1.114275948501766,
      "grad_norm": 0.16125749051570892,
      "learning_rate": 0.000777119416590702,
      "loss": 0.8603,
      "step": 2445
    },
    {
      "epoch": 1.1165546314230375,
      "grad_norm": 0.16046735644340515,
      "learning_rate": 0.0007766636280765725,
      "loss": 0.8735,
      "step": 2450
    },
    {
      "epoch": 1.118833314344309,
      "grad_norm": 0.14999786019325256,
      "learning_rate": 0.000776207839562443,
      "loss": 0.9055,
      "step": 2455
    },
    {
      "epoch": 1.1211119972655805,
      "grad_norm": 0.15016792714595795,
      "learning_rate": 0.0007757520510483136,
      "loss": 0.8707,
      "step": 2460
    },
    {
      "epoch": 1.123390680186852,
      "grad_norm": 0.14855417609214783,
      "learning_rate": 0.0007752962625341841,
      "loss": 0.8538,
      "step": 2465
    },
    {
      "epoch": 1.1256693631081236,
      "grad_norm": 0.15547384321689606,
      "learning_rate": 0.0007748404740200547,
      "loss": 0.8509,
      "step": 2470
    },
    {
      "epoch": 1.127948046029395,
      "grad_norm": 0.15769225358963013,
      "learning_rate": 0.0007743846855059253,
      "loss": 0.8976,
      "step": 2475
    },
    {
      "epoch": 1.1302267289506664,
      "grad_norm": 0.16491453349590302,
      "learning_rate": 0.0007739288969917958,
      "loss": 0.9053,
      "step": 2480
    },
    {
      "epoch": 1.132505411871938,
      "grad_norm": 0.15246862173080444,
      "learning_rate": 0.0007734731084776664,
      "loss": 0.8932,
      "step": 2485
    },
    {
      "epoch": 1.1347840947932095,
      "grad_norm": 0.14930479228496552,
      "learning_rate": 0.000773017319963537,
      "loss": 0.8641,
      "step": 2490
    },
    {
      "epoch": 1.137062777714481,
      "grad_norm": 0.14368097484111786,
      "learning_rate": 0.0007725615314494074,
      "loss": 0.856,
      "step": 2495
    },
    {
      "epoch": 1.1393414606357526,
      "grad_norm": 0.1564527004957199,
      "learning_rate": 0.000772105742935278,
      "loss": 0.9239,
      "step": 2500
    },
    {
      "epoch": 1.1416201435570241,
      "grad_norm": 0.1704804003238678,
      "learning_rate": 0.0007716499544211486,
      "loss": 0.855,
      "step": 2505
    },
    {
      "epoch": 1.1438988264782957,
      "grad_norm": 0.14540724456310272,
      "learning_rate": 0.0007711941659070191,
      "loss": 0.9646,
      "step": 2510
    },
    {
      "epoch": 1.146177509399567,
      "grad_norm": 0.1463654488325119,
      "learning_rate": 0.0007707383773928898,
      "loss": 0.9035,
      "step": 2515
    },
    {
      "epoch": 1.1484561923208385,
      "grad_norm": 0.14632010459899902,
      "learning_rate": 0.0007702825888787603,
      "loss": 0.7857,
      "step": 2520
    },
    {
      "epoch": 1.15073487524211,
      "grad_norm": 0.15911206603050232,
      "learning_rate": 0.0007698268003646308,
      "loss": 0.9176,
      "step": 2525
    },
    {
      "epoch": 1.1530135581633816,
      "grad_norm": 0.16381680965423584,
      "learning_rate": 0.0007693710118505014,
      "loss": 0.9229,
      "step": 2530
    },
    {
      "epoch": 1.1552922410846531,
      "grad_norm": 0.15640105307102203,
      "learning_rate": 0.000768915223336372,
      "loss": 0.8561,
      "step": 2535
    },
    {
      "epoch": 1.1575709240059247,
      "grad_norm": 0.16131125390529633,
      "learning_rate": 0.0007684594348222424,
      "loss": 0.8917,
      "step": 2540
    },
    {
      "epoch": 1.159849606927196,
      "grad_norm": 0.148236483335495,
      "learning_rate": 0.000768003646308113,
      "loss": 0.9154,
      "step": 2545
    },
    {
      "epoch": 1.1621282898484675,
      "grad_norm": 0.16147315502166748,
      "learning_rate": 0.0007675478577939836,
      "loss": 0.8343,
      "step": 2550
    },
    {
      "epoch": 1.164406972769739,
      "grad_norm": 0.162669375538826,
      "learning_rate": 0.0007670920692798542,
      "loss": 0.8521,
      "step": 2555
    },
    {
      "epoch": 1.1666856556910106,
      "grad_norm": 0.15305738151073456,
      "learning_rate": 0.0007666362807657247,
      "loss": 0.9117,
      "step": 2560
    },
    {
      "epoch": 1.168964338612282,
      "grad_norm": 0.14889976382255554,
      "learning_rate": 0.0007661804922515953,
      "loss": 0.8889,
      "step": 2565
    },
    {
      "epoch": 1.1712430215335536,
      "grad_norm": 0.1586340367794037,
      "learning_rate": 0.0007657247037374658,
      "loss": 0.9019,
      "step": 2570
    },
    {
      "epoch": 1.1735217044548252,
      "grad_norm": 0.15020321309566498,
      "learning_rate": 0.0007652689152233364,
      "loss": 0.8784,
      "step": 2575
    },
    {
      "epoch": 1.1758003873760967,
      "grad_norm": 0.1709943562746048,
      "learning_rate": 0.000764813126709207,
      "loss": 0.909,
      "step": 2580
    },
    {
      "epoch": 1.178079070297368,
      "grad_norm": 0.15467624366283417,
      "learning_rate": 0.0007643573381950774,
      "loss": 0.9011,
      "step": 2585
    },
    {
      "epoch": 1.1803577532186396,
      "grad_norm": 0.13799293339252472,
      "learning_rate": 0.0007639015496809481,
      "loss": 0.9037,
      "step": 2590
    },
    {
      "epoch": 1.182636436139911,
      "grad_norm": 0.1605415642261505,
      "learning_rate": 0.0007634457611668187,
      "loss": 0.8947,
      "step": 2595
    },
    {
      "epoch": 1.1849151190611826,
      "grad_norm": 0.14372847974300385,
      "learning_rate": 0.0007629899726526892,
      "loss": 0.8212,
      "step": 2600
    },
    {
      "epoch": 1.1871938019824542,
      "grad_norm": 0.14690780639648438,
      "learning_rate": 0.0007625341841385597,
      "loss": 0.8969,
      "step": 2605
    },
    {
      "epoch": 1.1894724849037257,
      "grad_norm": 0.1578640192747116,
      "learning_rate": 0.0007620783956244303,
      "loss": 0.9284,
      "step": 2610
    },
    {
      "epoch": 1.1917511678249972,
      "grad_norm": 0.17085562646389008,
      "learning_rate": 0.0007616226071103008,
      "loss": 0.8427,
      "step": 2615
    },
    {
      "epoch": 1.1940298507462686,
      "grad_norm": 0.1420556753873825,
      "learning_rate": 0.0007611668185961714,
      "loss": 0.7966,
      "step": 2620
    },
    {
      "epoch": 1.19630853366754,
      "grad_norm": 0.151236891746521,
      "learning_rate": 0.0007607110300820419,
      "loss": 0.8882,
      "step": 2625
    },
    {
      "epoch": 1.1985872165888116,
      "grad_norm": 0.1647651195526123,
      "learning_rate": 0.0007602552415679125,
      "loss": 0.8744,
      "step": 2630
    },
    {
      "epoch": 1.2008658995100832,
      "grad_norm": 0.14515246450901031,
      "learning_rate": 0.0007597994530537831,
      "loss": 0.8707,
      "step": 2635
    },
    {
      "epoch": 1.2031445824313547,
      "grad_norm": 0.17344944179058075,
      "learning_rate": 0.0007593436645396537,
      "loss": 0.9666,
      "step": 2640
    },
    {
      "epoch": 1.2054232653526262,
      "grad_norm": 0.14966435730457306,
      "learning_rate": 0.0007588878760255242,
      "loss": 0.8562,
      "step": 2645
    },
    {
      "epoch": 1.2077019482738978,
      "grad_norm": 0.1589399129152298,
      "learning_rate": 0.0007584320875113947,
      "loss": 0.8675,
      "step": 2650
    },
    {
      "epoch": 1.2099806311951693,
      "grad_norm": 0.1543280929327011,
      "learning_rate": 0.0007579762989972653,
      "loss": 0.8936,
      "step": 2655
    },
    {
      "epoch": 1.2122593141164406,
      "grad_norm": 0.14928534626960754,
      "learning_rate": 0.0007575205104831358,
      "loss": 0.8201,
      "step": 2660
    },
    {
      "epoch": 1.2145379970377121,
      "grad_norm": 0.14493440091609955,
      "learning_rate": 0.0007570647219690064,
      "loss": 0.8777,
      "step": 2665
    },
    {
      "epoch": 1.2168166799589837,
      "grad_norm": 0.1452850103378296,
      "learning_rate": 0.000756608933454877,
      "loss": 0.9084,
      "step": 2670
    },
    {
      "epoch": 1.2190953628802552,
      "grad_norm": 0.1722506731748581,
      "learning_rate": 0.0007561531449407475,
      "loss": 0.9186,
      "step": 2675
    },
    {
      "epoch": 1.2213740458015268,
      "grad_norm": 0.16253624856472015,
      "learning_rate": 0.0007556973564266181,
      "loss": 0.9431,
      "step": 2680
    },
    {
      "epoch": 1.2236527287227983,
      "grad_norm": 0.1515730321407318,
      "learning_rate": 0.0007552415679124887,
      "loss": 0.918,
      "step": 2685
    },
    {
      "epoch": 1.2259314116440698,
      "grad_norm": 0.1554817408323288,
      "learning_rate": 0.0007547857793983591,
      "loss": 0.811,
      "step": 2690
    },
    {
      "epoch": 1.2282100945653411,
      "grad_norm": 0.16742397844791412,
      "learning_rate": 0.0007543299908842297,
      "loss": 0.8909,
      "step": 2695
    },
    {
      "epoch": 1.2304887774866127,
      "grad_norm": 0.15629920363426208,
      "learning_rate": 0.0007538742023701003,
      "loss": 0.9149,
      "step": 2700
    },
    {
      "epoch": 1.2327674604078842,
      "grad_norm": 0.14331752061843872,
      "learning_rate": 0.0007534184138559709,
      "loss": 0.8875,
      "step": 2705
    },
    {
      "epoch": 1.2350461433291557,
      "grad_norm": 0.1907733529806137,
      "learning_rate": 0.0007529626253418415,
      "loss": 0.9125,
      "step": 2710
    },
    {
      "epoch": 1.2373248262504273,
      "grad_norm": 0.15667714178562164,
      "learning_rate": 0.000752506836827712,
      "loss": 0.9001,
      "step": 2715
    },
    {
      "epoch": 1.2396035091716988,
      "grad_norm": 0.16450858116149902,
      "learning_rate": 0.0007520510483135825,
      "loss": 0.8963,
      "step": 2720
    },
    {
      "epoch": 1.2418821920929703,
      "grad_norm": 0.15573866665363312,
      "learning_rate": 0.0007515952597994531,
      "loss": 0.9158,
      "step": 2725
    },
    {
      "epoch": 1.2441608750142419,
      "grad_norm": 0.16310150921344757,
      "learning_rate": 0.0007511394712853237,
      "loss": 0.9429,
      "step": 2730
    },
    {
      "epoch": 1.2464395579355132,
      "grad_norm": 0.15407384932041168,
      "learning_rate": 0.0007506836827711941,
      "loss": 0.8594,
      "step": 2735
    },
    {
      "epoch": 1.2487182408567847,
      "grad_norm": 0.158311128616333,
      "learning_rate": 0.0007502278942570647,
      "loss": 0.9016,
      "step": 2740
    },
    {
      "epoch": 1.2509969237780563,
      "grad_norm": 0.15949746966362,
      "learning_rate": 0.0007497721057429354,
      "loss": 0.9636,
      "step": 2745
    },
    {
      "epoch": 1.2532756066993278,
      "grad_norm": 0.15202577412128448,
      "learning_rate": 0.0007493163172288059,
      "loss": 0.9017,
      "step": 2750
    },
    {
      "epoch": 1.2555542896205993,
      "grad_norm": 0.16494591534137726,
      "learning_rate": 0.0007488605287146764,
      "loss": 0.9085,
      "step": 2755
    },
    {
      "epoch": 1.2578329725418709,
      "grad_norm": 0.15923449397087097,
      "learning_rate": 0.000748404740200547,
      "loss": 0.8767,
      "step": 2760
    },
    {
      "epoch": 1.2601116554631422,
      "grad_norm": 0.12732303142547607,
      "learning_rate": 0.0007479489516864175,
      "loss": 0.9191,
      "step": 2765
    },
    {
      "epoch": 1.2623903383844137,
      "grad_norm": 0.1534540206193924,
      "learning_rate": 0.0007474931631722881,
      "loss": 0.9878,
      "step": 2770
    },
    {
      "epoch": 1.2646690213056853,
      "grad_norm": 0.16413754224777222,
      "learning_rate": 0.0007470373746581585,
      "loss": 0.8985,
      "step": 2775
    },
    {
      "epoch": 1.2669477042269568,
      "grad_norm": 0.15870600938796997,
      "learning_rate": 0.0007465815861440291,
      "loss": 0.8573,
      "step": 2780
    },
    {
      "epoch": 1.2692263871482283,
      "grad_norm": 0.17724104225635529,
      "learning_rate": 0.0007461257976298998,
      "loss": 0.9449,
      "step": 2785
    },
    {
      "epoch": 1.2715050700694999,
      "grad_norm": 0.180013507604599,
      "learning_rate": 0.0007456700091157703,
      "loss": 0.9478,
      "step": 2790
    },
    {
      "epoch": 1.2737837529907714,
      "grad_norm": 0.16271501779556274,
      "learning_rate": 0.0007452142206016409,
      "loss": 0.9161,
      "step": 2795
    },
    {
      "epoch": 1.276062435912043,
      "grad_norm": 0.14157140254974365,
      "learning_rate": 0.0007447584320875114,
      "loss": 0.889,
      "step": 2800
    },
    {
      "epoch": 1.2783411188333145,
      "grad_norm": 0.16606804728507996,
      "learning_rate": 0.0007443026435733819,
      "loss": 0.9645,
      "step": 2805
    },
    {
      "epoch": 1.2806198017545858,
      "grad_norm": 0.16501638293266296,
      "learning_rate": 0.0007438468550592525,
      "loss": 0.8783,
      "step": 2810
    },
    {
      "epoch": 1.2828984846758573,
      "grad_norm": 0.1462571769952774,
      "learning_rate": 0.0007433910665451231,
      "loss": 0.8355,
      "step": 2815
    },
    {
      "epoch": 1.2851771675971289,
      "grad_norm": 0.16504701972007751,
      "learning_rate": 0.0007429352780309936,
      "loss": 0.9889,
      "step": 2820
    },
    {
      "epoch": 1.2874558505184004,
      "grad_norm": 0.14849863946437836,
      "learning_rate": 0.0007424794895168642,
      "loss": 0.8445,
      "step": 2825
    },
    {
      "epoch": 1.289734533439672,
      "grad_norm": 0.15607726573944092,
      "learning_rate": 0.0007420237010027348,
      "loss": 0.9159,
      "step": 2830
    },
    {
      "epoch": 1.2920132163609432,
      "grad_norm": 0.1654951423406601,
      "learning_rate": 0.0007415679124886053,
      "loss": 0.8877,
      "step": 2835
    },
    {
      "epoch": 1.2942918992822148,
      "grad_norm": 0.16658195853233337,
      "learning_rate": 0.0007411121239744758,
      "loss": 0.9494,
      "step": 2840
    },
    {
      "epoch": 1.2965705822034863,
      "grad_norm": 0.15710458159446716,
      "learning_rate": 0.0007406563354603464,
      "loss": 0.8379,
      "step": 2845
    },
    {
      "epoch": 1.2988492651247578,
      "grad_norm": 0.14082081615924835,
      "learning_rate": 0.0007402005469462169,
      "loss": 0.8686,
      "step": 2850
    },
    {
      "epoch": 1.3011279480460294,
      "grad_norm": 0.16467402875423431,
      "learning_rate": 0.0007397447584320875,
      "loss": 0.8762,
      "step": 2855
    },
    {
      "epoch": 1.303406630967301,
      "grad_norm": 0.15663738548755646,
      "learning_rate": 0.0007392889699179582,
      "loss": 0.9036,
      "step": 2860
    },
    {
      "epoch": 1.3056853138885725,
      "grad_norm": 0.16929540038108826,
      "learning_rate": 0.0007388331814038286,
      "loss": 0.9316,
      "step": 2865
    },
    {
      "epoch": 1.307963996809844,
      "grad_norm": 0.1464441418647766,
      "learning_rate": 0.0007383773928896992,
      "loss": 0.8885,
      "step": 2870
    },
    {
      "epoch": 1.3102426797311155,
      "grad_norm": 0.15386299788951874,
      "learning_rate": 0.0007379216043755698,
      "loss": 0.8701,
      "step": 2875
    },
    {
      "epoch": 1.312521362652387,
      "grad_norm": 0.13686628639698029,
      "learning_rate": 0.0007374658158614403,
      "loss": 0.8768,
      "step": 2880
    },
    {
      "epoch": 1.3148000455736584,
      "grad_norm": 0.1717004030942917,
      "learning_rate": 0.0007370100273473108,
      "loss": 0.8886,
      "step": 2885
    },
    {
      "epoch": 1.31707872849493,
      "grad_norm": 0.14570537209510803,
      "learning_rate": 0.0007365542388331814,
      "loss": 0.8649,
      "step": 2890
    },
    {
      "epoch": 1.3193574114162014,
      "grad_norm": 0.1416633427143097,
      "learning_rate": 0.0007360984503190519,
      "loss": 0.8201,
      "step": 2895
    },
    {
      "epoch": 1.321636094337473,
      "grad_norm": 0.14834856986999512,
      "learning_rate": 0.0007356426618049226,
      "loss": 0.8476,
      "step": 2900
    },
    {
      "epoch": 1.3239147772587445,
      "grad_norm": 0.15419186651706696,
      "learning_rate": 0.0007351868732907931,
      "loss": 0.9087,
      "step": 2905
    },
    {
      "epoch": 1.3261934601800158,
      "grad_norm": 0.15423046052455902,
      "learning_rate": 0.0007347310847766636,
      "loss": 0.8936,
      "step": 2910
    },
    {
      "epoch": 1.3284721431012874,
      "grad_norm": 0.15277472138404846,
      "learning_rate": 0.0007342752962625342,
      "loss": 0.959,
      "step": 2915
    },
    {
      "epoch": 1.330750826022559,
      "grad_norm": 0.15677288174629211,
      "learning_rate": 0.0007338195077484048,
      "loss": 0.9124,
      "step": 2920
    },
    {
      "epoch": 1.3330295089438304,
      "grad_norm": 0.1470227986574173,
      "learning_rate": 0.0007333637192342753,
      "loss": 0.8517,
      "step": 2925
    },
    {
      "epoch": 1.335308191865102,
      "grad_norm": 0.16235233843326569,
      "learning_rate": 0.0007329079307201458,
      "loss": 0.9735,
      "step": 2930
    },
    {
      "epoch": 1.3375868747863735,
      "grad_norm": 0.15309543907642365,
      "learning_rate": 0.0007324521422060165,
      "loss": 0.9165,
      "step": 2935
    },
    {
      "epoch": 1.339865557707645,
      "grad_norm": 0.16133959591388702,
      "learning_rate": 0.000731996353691887,
      "loss": 0.9782,
      "step": 2940
    },
    {
      "epoch": 1.3421442406289166,
      "grad_norm": 0.1461542248725891,
      "learning_rate": 0.0007315405651777576,
      "loss": 0.7974,
      "step": 2945
    },
    {
      "epoch": 1.344422923550188,
      "grad_norm": 0.17490971088409424,
      "learning_rate": 0.0007310847766636281,
      "loss": 0.8737,
      "step": 2950
    },
    {
      "epoch": 1.3467016064714594,
      "grad_norm": 0.14365652203559875,
      "learning_rate": 0.0007306289881494986,
      "loss": 0.8329,
      "step": 2955
    },
    {
      "epoch": 1.348980289392731,
      "grad_norm": 0.16046328842639923,
      "learning_rate": 0.0007301731996353692,
      "loss": 0.9078,
      "step": 2960
    },
    {
      "epoch": 1.3512589723140025,
      "grad_norm": 0.153128519654274,
      "learning_rate": 0.0007297174111212398,
      "loss": 0.8718,
      "step": 2965
    },
    {
      "epoch": 1.353537655235274,
      "grad_norm": 0.14935988187789917,
      "learning_rate": 0.0007292616226071102,
      "loss": 0.8772,
      "step": 2970
    },
    {
      "epoch": 1.3558163381565456,
      "grad_norm": 0.14699803292751312,
      "learning_rate": 0.0007288058340929809,
      "loss": 0.8251,
      "step": 2975
    },
    {
      "epoch": 1.358095021077817,
      "grad_norm": 0.1802758127450943,
      "learning_rate": 0.0007283500455788515,
      "loss": 0.8913,
      "step": 2980
    },
    {
      "epoch": 1.3603737039990884,
      "grad_norm": 0.16649523377418518,
      "learning_rate": 0.000727894257064722,
      "loss": 0.8897,
      "step": 2985
    },
    {
      "epoch": 1.36265238692036,
      "grad_norm": 0.16660849750041962,
      "learning_rate": 0.0007274384685505926,
      "loss": 0.9128,
      "step": 2990
    },
    {
      "epoch": 1.3649310698416315,
      "grad_norm": 0.16364113986492157,
      "learning_rate": 0.000726982680036463,
      "loss": 0.9096,
      "step": 2995
    },
    {
      "epoch": 1.367209752762903,
      "grad_norm": 0.15880104899406433,
      "learning_rate": 0.0007265268915223336,
      "loss": 0.9049,
      "step": 3000
    },
    {
      "epoch": 1.3694884356841746,
      "grad_norm": 0.15883786976337433,
      "learning_rate": 0.0007260711030082042,
      "loss": 0.8644,
      "step": 3005
    },
    {
      "epoch": 1.371767118605446,
      "grad_norm": 0.1446443349123001,
      "learning_rate": 0.0007256153144940748,
      "loss": 0.874,
      "step": 3010
    },
    {
      "epoch": 1.3740458015267176,
      "grad_norm": 0.15256153047084808,
      "learning_rate": 0.0007251595259799453,
      "loss": 0.9209,
      "step": 3015
    },
    {
      "epoch": 1.3763244844479892,
      "grad_norm": 0.15181966125965118,
      "learning_rate": 0.0007247037374658159,
      "loss": 0.8049,
      "step": 3020
    },
    {
      "epoch": 1.3786031673692607,
      "grad_norm": 0.14965562522411346,
      "learning_rate": 0.0007242479489516865,
      "loss": 0.8966,
      "step": 3025
    },
    {
      "epoch": 1.380881850290532,
      "grad_norm": 0.14712700247764587,
      "learning_rate": 0.000723792160437557,
      "loss": 0.8699,
      "step": 3030
    },
    {
      "epoch": 1.3831605332118035,
      "grad_norm": 0.16339482367038727,
      "learning_rate": 0.0007233363719234275,
      "loss": 0.8981,
      "step": 3035
    },
    {
      "epoch": 1.385439216133075,
      "grad_norm": 0.13662028312683105,
      "learning_rate": 0.000722880583409298,
      "loss": 0.8237,
      "step": 3040
    },
    {
      "epoch": 1.3877178990543466,
      "grad_norm": 0.14982284605503082,
      "learning_rate": 0.0007224247948951686,
      "loss": 0.8834,
      "step": 3045
    },
    {
      "epoch": 1.3899965819756182,
      "grad_norm": 0.17208191752433777,
      "learning_rate": 0.0007219690063810393,
      "loss": 0.842,
      "step": 3050
    },
    {
      "epoch": 1.3922752648968895,
      "grad_norm": 0.1635766625404358,
      "learning_rate": 0.0007215132178669099,
      "loss": 0.8444,
      "step": 3055
    },
    {
      "epoch": 1.394553947818161,
      "grad_norm": 0.16071482002735138,
      "learning_rate": 0.0007210574293527803,
      "loss": 0.9612,
      "step": 3060
    },
    {
      "epoch": 1.3968326307394325,
      "grad_norm": 0.1603916883468628,
      "learning_rate": 0.0007206016408386509,
      "loss": 0.866,
      "step": 3065
    },
    {
      "epoch": 1.399111313660704,
      "grad_norm": 0.15300558507442474,
      "learning_rate": 0.0007201458523245215,
      "loss": 0.9228,
      "step": 3070
    },
    {
      "epoch": 1.4013899965819756,
      "grad_norm": 0.1771816909313202,
      "learning_rate": 0.000719690063810392,
      "loss": 0.948,
      "step": 3075
    },
    {
      "epoch": 1.4036686795032471,
      "grad_norm": 0.1672586351633072,
      "learning_rate": 0.0007192342752962625,
      "loss": 0.9452,
      "step": 3080
    },
    {
      "epoch": 1.4059473624245187,
      "grad_norm": 0.166086345911026,
      "learning_rate": 0.000718778486782133,
      "loss": 0.8695,
      "step": 3085
    },
    {
      "epoch": 1.4082260453457902,
      "grad_norm": 0.1622636765241623,
      "learning_rate": 0.0007183226982680037,
      "loss": 0.9039,
      "step": 3090
    },
    {
      "epoch": 1.4105047282670617,
      "grad_norm": 0.19210673868656158,
      "learning_rate": 0.0007178669097538743,
      "loss": 0.8087,
      "step": 3095
    },
    {
      "epoch": 1.412783411188333,
      "grad_norm": 0.1701834797859192,
      "learning_rate": 0.0007174111212397447,
      "loss": 0.9778,
      "step": 3100
    },
    {
      "epoch": 1.4150620941096046,
      "grad_norm": 0.14662061631679535,
      "learning_rate": 0.0007169553327256153,
      "loss": 0.8633,
      "step": 3105
    },
    {
      "epoch": 1.4173407770308761,
      "grad_norm": 0.1615397334098816,
      "learning_rate": 0.0007164995442114859,
      "loss": 0.8888,
      "step": 3110
    },
    {
      "epoch": 1.4196194599521477,
      "grad_norm": 0.16150493919849396,
      "learning_rate": 0.0007160437556973564,
      "loss": 0.903,
      "step": 3115
    },
    {
      "epoch": 1.4218981428734192,
      "grad_norm": 0.1609194129705429,
      "learning_rate": 0.000715587967183227,
      "loss": 0.8752,
      "step": 3120
    },
    {
      "epoch": 1.4241768257946907,
      "grad_norm": 0.16180773079395294,
      "learning_rate": 0.0007151321786690975,
      "loss": 0.8992,
      "step": 3125
    },
    {
      "epoch": 1.426455508715962,
      "grad_norm": 0.16091716289520264,
      "learning_rate": 0.0007146763901549681,
      "loss": 0.889,
      "step": 3130
    },
    {
      "epoch": 1.4287341916372336,
      "grad_norm": 0.16030150651931763,
      "learning_rate": 0.0007142206016408387,
      "loss": 0.8921,
      "step": 3135
    },
    {
      "epoch": 1.4310128745585051,
      "grad_norm": 0.170382559299469,
      "learning_rate": 0.0007137648131267093,
      "loss": 0.8567,
      "step": 3140
    },
    {
      "epoch": 1.4332915574797767,
      "grad_norm": 0.19072683155536652,
      "learning_rate": 0.0007133090246125797,
      "loss": 0.8831,
      "step": 3145
    },
    {
      "epoch": 1.4355702404010482,
      "grad_norm": 0.1332167387008667,
      "learning_rate": 0.0007128532360984503,
      "loss": 0.8323,
      "step": 3150
    },
    {
      "epoch": 1.4378489233223197,
      "grad_norm": 0.15187720954418182,
      "learning_rate": 0.0007123974475843209,
      "loss": 0.8559,
      "step": 3155
    },
    {
      "epoch": 1.4401276062435913,
      "grad_norm": 0.16988372802734375,
      "learning_rate": 0.0007119416590701914,
      "loss": 0.9251,
      "step": 3160
    },
    {
      "epoch": 1.4424062891648628,
      "grad_norm": 0.1548493206501007,
      "learning_rate": 0.000711485870556062,
      "loss": 0.9195,
      "step": 3165
    },
    {
      "epoch": 1.4446849720861343,
      "grad_norm": 0.1506761908531189,
      "learning_rate": 0.0007110300820419326,
      "loss": 0.911,
      "step": 3170
    },
    {
      "epoch": 1.4469636550074056,
      "grad_norm": 0.15938323736190796,
      "learning_rate": 0.0007105742935278031,
      "loss": 0.9058,
      "step": 3175
    },
    {
      "epoch": 1.4492423379286772,
      "grad_norm": 0.15033189952373505,
      "learning_rate": 0.0007101185050136737,
      "loss": 0.8866,
      "step": 3180
    },
    {
      "epoch": 1.4515210208499487,
      "grad_norm": 0.16011536121368408,
      "learning_rate": 0.0007096627164995443,
      "loss": 0.8857,
      "step": 3185
    },
    {
      "epoch": 1.4537997037712203,
      "grad_norm": 0.1454964280128479,
      "learning_rate": 0.0007092069279854147,
      "loss": 0.8668,
      "step": 3190
    },
    {
      "epoch": 1.4560783866924918,
      "grad_norm": 0.18023408949375153,
      "learning_rate": 0.0007087511394712853,
      "loss": 0.9336,
      "step": 3195
    },
    {
      "epoch": 1.4583570696137633,
      "grad_norm": 0.18555784225463867,
      "learning_rate": 0.0007082953509571559,
      "loss": 0.8976,
      "step": 3200
    },
    {
      "epoch": 1.4606357525350346,
      "grad_norm": 0.1656550168991089,
      "learning_rate": 0.0007078395624430265,
      "loss": 0.899,
      "step": 3205
    },
    {
      "epoch": 1.4629144354563062,
      "grad_norm": 0.15083932876586914,
      "learning_rate": 0.000707383773928897,
      "loss": 0.8305,
      "step": 3210
    },
    {
      "epoch": 1.4651931183775777,
      "grad_norm": 0.15208248794078827,
      "learning_rate": 0.0007069279854147676,
      "loss": 0.8574,
      "step": 3215
    },
    {
      "epoch": 1.4674718012988492,
      "grad_norm": 0.1674986183643341,
      "learning_rate": 0.0007064721969006381,
      "loss": 0.8896,
      "step": 3220
    },
    {
      "epoch": 1.4697504842201208,
      "grad_norm": 0.16380077600479126,
      "learning_rate": 0.0007060164083865087,
      "loss": 0.9252,
      "step": 3225
    },
    {
      "epoch": 1.4720291671413923,
      "grad_norm": 0.15686525404453278,
      "learning_rate": 0.0007055606198723792,
      "loss": 0.8914,
      "step": 3230
    },
    {
      "epoch": 1.4743078500626638,
      "grad_norm": 0.15142282843589783,
      "learning_rate": 0.0007051048313582497,
      "loss": 0.8986,
      "step": 3235
    },
    {
      "epoch": 1.4765865329839354,
      "grad_norm": 0.1993783563375473,
      "learning_rate": 0.0007046490428441203,
      "loss": 0.8757,
      "step": 3240
    },
    {
      "epoch": 1.478865215905207,
      "grad_norm": 0.16392700374126434,
      "learning_rate": 0.000704193254329991,
      "loss": 0.8724,
      "step": 3245
    },
    {
      "epoch": 1.4811438988264782,
      "grad_norm": 0.15566149353981018,
      "learning_rate": 0.0007037374658158614,
      "loss": 0.8622,
      "step": 3250
    },
    {
      "epoch": 1.4834225817477498,
      "grad_norm": 0.18785300850868225,
      "learning_rate": 0.000703281677301732,
      "loss": 0.8194,
      "step": 3255
    },
    {
      "epoch": 1.4857012646690213,
      "grad_norm": 0.17423754930496216,
      "learning_rate": 0.0007028258887876026,
      "loss": 0.8867,
      "step": 3260
    },
    {
      "epoch": 1.4879799475902928,
      "grad_norm": 0.16508951783180237,
      "learning_rate": 0.0007023701002734731,
      "loss": 0.9481,
      "step": 3265
    },
    {
      "epoch": 1.4902586305115644,
      "grad_norm": 0.16570043563842773,
      "learning_rate": 0.0007019143117593437,
      "loss": 0.9279,
      "step": 3270
    },
    {
      "epoch": 1.4925373134328357,
      "grad_norm": 0.17302320897579193,
      "learning_rate": 0.0007014585232452142,
      "loss": 0.885,
      "step": 3275
    },
    {
      "epoch": 1.4948159963541072,
      "grad_norm": 0.16829422116279602,
      "learning_rate": 0.0007010027347310848,
      "loss": 0.8944,
      "step": 3280
    },
    {
      "epoch": 1.4970946792753788,
      "grad_norm": 0.18360403180122375,
      "learning_rate": 0.0007005469462169554,
      "loss": 0.9392,
      "step": 3285
    },
    {
      "epoch": 1.4993733621966503,
      "grad_norm": 0.17969724535942078,
      "learning_rate": 0.000700091157702826,
      "loss": 0.9036,
      "step": 3290
    },
    {
      "epoch": 1.5016520451179218,
      "grad_norm": 0.14764168858528137,
      "learning_rate": 0.0006996353691886964,
      "loss": 0.8536,
      "step": 3295
    },
    {
      "epoch": 1.5039307280391934,
      "grad_norm": 0.1553378403186798,
      "learning_rate": 0.000699179580674567,
      "loss": 0.9658,
      "step": 3300
    },
    {
      "epoch": 1.506209410960465,
      "grad_norm": 0.1449325531721115,
      "learning_rate": 0.0006987237921604376,
      "loss": 0.872,
      "step": 3305
    },
    {
      "epoch": 1.5084880938817364,
      "grad_norm": 0.17436596751213074,
      "learning_rate": 0.0006982680036463081,
      "loss": 0.8809,
      "step": 3310
    },
    {
      "epoch": 1.510766776803008,
      "grad_norm": 0.16775202751159668,
      "learning_rate": 0.0006978122151321786,
      "loss": 0.8949,
      "step": 3315
    },
    {
      "epoch": 1.5130454597242795,
      "grad_norm": 0.1432698369026184,
      "learning_rate": 0.0006973564266180493,
      "loss": 0.8675,
      "step": 3320
    },
    {
      "epoch": 1.5153241426455508,
      "grad_norm": 0.17781423032283783,
      "learning_rate": 0.0006969006381039198,
      "loss": 0.8693,
      "step": 3325
    },
    {
      "epoch": 1.5176028255668224,
      "grad_norm": 0.16270071268081665,
      "learning_rate": 0.0006964448495897904,
      "loss": 0.9475,
      "step": 3330
    },
    {
      "epoch": 1.519881508488094,
      "grad_norm": 0.16914670169353485,
      "learning_rate": 0.000695989061075661,
      "loss": 0.9204,
      "step": 3335
    },
    {
      "epoch": 1.5221601914093654,
      "grad_norm": 0.16067108511924744,
      "learning_rate": 0.0006955332725615314,
      "loss": 0.8437,
      "step": 3340
    },
    {
      "epoch": 1.5244388743306367,
      "grad_norm": 0.16310587525367737,
      "learning_rate": 0.000695077484047402,
      "loss": 1.013,
      "step": 3345
    },
    {
      "epoch": 1.5267175572519083,
      "grad_norm": 0.16041025519371033,
      "learning_rate": 0.0006946216955332726,
      "loss": 0.8592,
      "step": 3350
    },
    {
      "epoch": 1.5289962401731798,
      "grad_norm": 0.14819402992725372,
      "learning_rate": 0.0006941659070191431,
      "loss": 0.8876,
      "step": 3355
    },
    {
      "epoch": 1.5312749230944513,
      "grad_norm": 0.14234186708927155,
      "learning_rate": 0.0006937101185050137,
      "loss": 0.8377,
      "step": 3360
    },
    {
      "epoch": 1.5335536060157229,
      "grad_norm": 0.17347431182861328,
      "learning_rate": 0.0006932543299908843,
      "loss": 0.8671,
      "step": 3365
    },
    {
      "epoch": 1.5358322889369944,
      "grad_norm": 0.16258127987384796,
      "learning_rate": 0.0006927985414767548,
      "loss": 0.8379,
      "step": 3370
    },
    {
      "epoch": 1.538110971858266,
      "grad_norm": 0.1547560840845108,
      "learning_rate": 0.0006923427529626254,
      "loss": 0.8929,
      "step": 3375
    },
    {
      "epoch": 1.5403896547795375,
      "grad_norm": 0.17858368158340454,
      "learning_rate": 0.0006918869644484958,
      "loss": 0.9336,
      "step": 3380
    },
    {
      "epoch": 1.542668337700809,
      "grad_norm": 0.14136464893817902,
      "learning_rate": 0.0006914311759343664,
      "loss": 0.8873,
      "step": 3385
    },
    {
      "epoch": 1.5449470206220806,
      "grad_norm": 0.16749879717826843,
      "learning_rate": 0.000690975387420237,
      "loss": 0.8833,
      "step": 3390
    },
    {
      "epoch": 1.547225703543352,
      "grad_norm": 0.16989539563655853,
      "learning_rate": 0.0006905195989061077,
      "loss": 0.8284,
      "step": 3395
    },
    {
      "epoch": 1.5495043864646234,
      "grad_norm": 0.15460242331027985,
      "learning_rate": 0.0006900638103919782,
      "loss": 0.8754,
      "step": 3400
    },
    {
      "epoch": 1.551783069385895,
      "grad_norm": 0.1643906682729721,
      "learning_rate": 0.0006896080218778487,
      "loss": 0.8644,
      "step": 3405
    },
    {
      "epoch": 1.5540617523071665,
      "grad_norm": 0.16841894388198853,
      "learning_rate": 0.0006891522333637192,
      "loss": 0.9053,
      "step": 3410
    },
    {
      "epoch": 1.556340435228438,
      "grad_norm": 0.14691464602947235,
      "learning_rate": 0.0006886964448495898,
      "loss": 0.8904,
      "step": 3415
    },
    {
      "epoch": 1.5586191181497093,
      "grad_norm": 0.17315715551376343,
      "learning_rate": 0.0006882406563354604,
      "loss": 0.8535,
      "step": 3420
    },
    {
      "epoch": 1.5608978010709809,
      "grad_norm": 0.16629093885421753,
      "learning_rate": 0.0006877848678213308,
      "loss": 0.8979,
      "step": 3425
    },
    {
      "epoch": 1.5631764839922524,
      "grad_norm": 0.18886776268482208,
      "learning_rate": 0.0006873290793072014,
      "loss": 0.9176,
      "step": 3430
    },
    {
      "epoch": 1.565455166913524,
      "grad_norm": 0.15921783447265625,
      "learning_rate": 0.0006868732907930721,
      "loss": 0.9134,
      "step": 3435
    },
    {
      "epoch": 1.5677338498347955,
      "grad_norm": 0.16944146156311035,
      "learning_rate": 0.0006864175022789427,
      "loss": 0.8978,
      "step": 3440
    },
    {
      "epoch": 1.570012532756067,
      "grad_norm": 0.1683155745267868,
      "learning_rate": 0.0006859617137648131,
      "loss": 0.9366,
      "step": 3445
    },
    {
      "epoch": 1.5722912156773385,
      "grad_norm": 0.16390568017959595,
      "learning_rate": 0.0006855059252506837,
      "loss": 0.8514,
      "step": 3450
    },
    {
      "epoch": 1.57456989859861,
      "grad_norm": 0.17432940006256104,
      "learning_rate": 0.0006850501367365542,
      "loss": 0.8951,
      "step": 3455
    },
    {
      "epoch": 1.5768485815198816,
      "grad_norm": 0.1787983477115631,
      "learning_rate": 0.0006845943482224248,
      "loss": 0.9133,
      "step": 3460
    },
    {
      "epoch": 1.5791272644411531,
      "grad_norm": 0.15218310058116913,
      "learning_rate": 0.0006841385597082954,
      "loss": 0.926,
      "step": 3465
    },
    {
      "epoch": 1.5814059473624247,
      "grad_norm": 0.16668929159641266,
      "learning_rate": 0.0006836827711941658,
      "loss": 0.8761,
      "step": 3470
    },
    {
      "epoch": 1.583684630283696,
      "grad_norm": 0.15210013091564178,
      "learning_rate": 0.0006832269826800365,
      "loss": 0.8784,
      "step": 3475
    },
    {
      "epoch": 1.5859633132049675,
      "grad_norm": 0.18946874141693115,
      "learning_rate": 0.0006827711941659071,
      "loss": 0.9609,
      "step": 3480
    },
    {
      "epoch": 1.588241996126239,
      "grad_norm": 0.1629585176706314,
      "learning_rate": 0.0006823154056517776,
      "loss": 0.8732,
      "step": 3485
    },
    {
      "epoch": 1.5905206790475104,
      "grad_norm": 0.15917328000068665,
      "learning_rate": 0.0006818596171376481,
      "loss": 0.9477,
      "step": 3490
    },
    {
      "epoch": 1.592799361968782,
      "grad_norm": 0.1740906536579132,
      "learning_rate": 0.0006814038286235187,
      "loss": 0.9004,
      "step": 3495
    },
    {
      "epoch": 1.5950780448900534,
      "grad_norm": 0.17103242874145508,
      "learning_rate": 0.0006809480401093892,
      "loss": 0.8911,
      "step": 3500
    },
    {
      "epoch": 1.597356727811325,
      "grad_norm": 0.16465790569782257,
      "learning_rate": 0.0006804922515952598,
      "loss": 0.9327,
      "step": 3505
    },
    {
      "epoch": 1.5996354107325965,
      "grad_norm": 0.15760625898838043,
      "learning_rate": 0.0006800364630811303,
      "loss": 0.8902,
      "step": 3510
    },
    {
      "epoch": 1.601914093653868,
      "grad_norm": 0.14840568602085114,
      "learning_rate": 0.0006795806745670009,
      "loss": 0.853,
      "step": 3515
    },
    {
      "epoch": 1.6041927765751396,
      "grad_norm": 0.1666347086429596,
      "learning_rate": 0.0006791248860528715,
      "loss": 0.8946,
      "step": 3520
    },
    {
      "epoch": 1.6064714594964111,
      "grad_norm": 0.15916144847869873,
      "learning_rate": 0.0006786690975387421,
      "loss": 0.9039,
      "step": 3525
    },
    {
      "epoch": 1.6087501424176827,
      "grad_norm": 0.15589900314807892,
      "learning_rate": 0.0006782133090246126,
      "loss": 0.8796,
      "step": 3530
    },
    {
      "epoch": 1.6110288253389542,
      "grad_norm": 0.14631642401218414,
      "learning_rate": 0.0006777575205104831,
      "loss": 0.8967,
      "step": 3535
    },
    {
      "epoch": 1.6133075082602257,
      "grad_norm": 0.14905403554439545,
      "learning_rate": 0.0006773017319963537,
      "loss": 0.8199,
      "step": 3540
    },
    {
      "epoch": 1.615586191181497,
      "grad_norm": 0.16466668248176575,
      "learning_rate": 0.0006768459434822242,
      "loss": 0.9012,
      "step": 3545
    },
    {
      "epoch": 1.6178648741027686,
      "grad_norm": 0.16315622627735138,
      "learning_rate": 0.0006763901549680949,
      "loss": 0.9393,
      "step": 3550
    },
    {
      "epoch": 1.6201435570240401,
      "grad_norm": 0.17641381919384003,
      "learning_rate": 0.0006759343664539654,
      "loss": 0.8691,
      "step": 3555
    },
    {
      "epoch": 1.6224222399453117,
      "grad_norm": 0.14904770255088806,
      "learning_rate": 0.0006754785779398359,
      "loss": 0.8896,
      "step": 3560
    },
    {
      "epoch": 1.624700922866583,
      "grad_norm": 0.1434256136417389,
      "learning_rate": 0.0006750227894257065,
      "loss": 0.9428,
      "step": 3565
    },
    {
      "epoch": 1.6269796057878545,
      "grad_norm": 0.1490059494972229,
      "learning_rate": 0.0006745670009115771,
      "loss": 0.8878,
      "step": 3570
    },
    {
      "epoch": 1.629258288709126,
      "grad_norm": 0.14780128002166748,
      "learning_rate": 0.0006741112123974475,
      "loss": 0.8769,
      "step": 3575
    },
    {
      "epoch": 1.6315369716303976,
      "grad_norm": 0.15179327130317688,
      "learning_rate": 0.0006736554238833181,
      "loss": 0.8892,
      "step": 3580
    },
    {
      "epoch": 1.633815654551669,
      "grad_norm": 0.16848579049110413,
      "learning_rate": 0.0006731996353691887,
      "loss": 0.9857,
      "step": 3585
    },
    {
      "epoch": 1.6360943374729406,
      "grad_norm": 0.158471941947937,
      "learning_rate": 0.0006727438468550593,
      "loss": 0.8638,
      "step": 3590
    },
    {
      "epoch": 1.6383730203942122,
      "grad_norm": 0.15913373231887817,
      "learning_rate": 0.0006722880583409299,
      "loss": 0.9156,
      "step": 3595
    },
    {
      "epoch": 1.6406517033154837,
      "grad_norm": 0.16863004863262177,
      "learning_rate": 0.0006718322698268004,
      "loss": 0.7948,
      "step": 3600
    },
    {
      "epoch": 1.6429303862367552,
      "grad_norm": 0.1558290421962738,
      "learning_rate": 0.0006713764813126709,
      "loss": 0.8986,
      "step": 3605
    },
    {
      "epoch": 1.6452090691580268,
      "grad_norm": 0.16701823472976685,
      "learning_rate": 0.0006709206927985415,
      "loss": 0.8422,
      "step": 3610
    },
    {
      "epoch": 1.6474877520792983,
      "grad_norm": 0.15407730638980865,
      "learning_rate": 0.0006704649042844121,
      "loss": 0.8956,
      "step": 3615
    },
    {
      "epoch": 1.6497664350005696,
      "grad_norm": 0.15278984606266022,
      "learning_rate": 0.0006700091157702825,
      "loss": 0.9137,
      "step": 3620
    },
    {
      "epoch": 1.6520451179218412,
      "grad_norm": 0.15514960885047913,
      "learning_rate": 0.0006695533272561532,
      "loss": 0.8291,
      "step": 3625
    },
    {
      "epoch": 1.6543238008431127,
      "grad_norm": 0.17071014642715454,
      "learning_rate": 0.0006690975387420238,
      "loss": 0.8741,
      "step": 3630
    },
    {
      "epoch": 1.6566024837643842,
      "grad_norm": 0.16326315701007843,
      "learning_rate": 0.0006686417502278943,
      "loss": 0.8936,
      "step": 3635
    },
    {
      "epoch": 1.6588811666856556,
      "grad_norm": 0.17669951915740967,
      "learning_rate": 0.0006681859617137648,
      "loss": 0.8205,
      "step": 3640
    },
    {
      "epoch": 1.661159849606927,
      "grad_norm": 0.17554707825183868,
      "learning_rate": 0.0006677301731996354,
      "loss": 0.8975,
      "step": 3645
    },
    {
      "epoch": 1.6634385325281986,
      "grad_norm": 0.18521659076213837,
      "learning_rate": 0.0006672743846855059,
      "loss": 0.936,
      "step": 3650
    },
    {
      "epoch": 1.6657172154494702,
      "grad_norm": 0.16266019642353058,
      "learning_rate": 0.0006668185961713765,
      "loss": 0.7884,
      "step": 3655
    },
    {
      "epoch": 1.6679958983707417,
      "grad_norm": 0.1637265831232071,
      "learning_rate": 0.0006663628076572471,
      "loss": 0.8442,
      "step": 3660
    },
    {
      "epoch": 1.6702745812920132,
      "grad_norm": 0.18716469407081604,
      "learning_rate": 0.0006659070191431176,
      "loss": 0.8947,
      "step": 3665
    },
    {
      "epoch": 1.6725532642132848,
      "grad_norm": 0.16912296414375305,
      "learning_rate": 0.0006654512306289882,
      "loss": 0.8135,
      "step": 3670
    },
    {
      "epoch": 1.6748319471345563,
      "grad_norm": 0.1301882416009903,
      "learning_rate": 0.0006649954421148588,
      "loss": 0.8447,
      "step": 3675
    },
    {
      "epoch": 1.6771106300558278,
      "grad_norm": 0.16467545926570892,
      "learning_rate": 0.0006645396536007293,
      "loss": 0.8725,
      "step": 3680
    },
    {
      "epoch": 1.6793893129770994,
      "grad_norm": 0.16623204946517944,
      "learning_rate": 0.0006640838650865998,
      "loss": 0.8441,
      "step": 3685
    },
    {
      "epoch": 1.681667995898371,
      "grad_norm": 0.17915473878383636,
      "learning_rate": 0.0006636280765724704,
      "loss": 0.8926,
      "step": 3690
    },
    {
      "epoch": 1.6839466788196422,
      "grad_norm": 0.15235620737075806,
      "learning_rate": 0.0006631722880583409,
      "loss": 0.8754,
      "step": 3695
    },
    {
      "epoch": 1.6862253617409138,
      "grad_norm": 0.15784761309623718,
      "learning_rate": 0.0006627164995442115,
      "loss": 0.8599,
      "step": 3700
    },
    {
      "epoch": 1.6885040446621853,
      "grad_norm": 0.14265218377113342,
      "learning_rate": 0.000662260711030082,
      "loss": 0.8324,
      "step": 3705
    },
    {
      "epoch": 1.6907827275834566,
      "grad_norm": 0.15137161314487457,
      "learning_rate": 0.0006618049225159526,
      "loss": 0.8686,
      "step": 3710
    },
    {
      "epoch": 1.6930614105047281,
      "grad_norm": 0.14901262521743774,
      "learning_rate": 0.0006613491340018232,
      "loss": 0.8989,
      "step": 3715
    },
    {
      "epoch": 1.6953400934259997,
      "grad_norm": 0.17436537146568298,
      "learning_rate": 0.0006608933454876938,
      "loss": 0.899,
      "step": 3720
    },
    {
      "epoch": 1.6976187763472712,
      "grad_norm": 0.16465184092521667,
      "learning_rate": 0.0006604375569735643,
      "loss": 0.8928,
      "step": 3725
    },
    {
      "epoch": 1.6998974592685427,
      "grad_norm": 0.1770787537097931,
      "learning_rate": 0.0006599817684594348,
      "loss": 0.859,
      "step": 3730
    },
    {
      "epoch": 1.7021761421898143,
      "grad_norm": 0.18650957942008972,
      "learning_rate": 0.0006595259799453053,
      "loss": 0.8927,
      "step": 3735
    },
    {
      "epoch": 1.7044548251110858,
      "grad_norm": 0.15753620862960815,
      "learning_rate": 0.000659070191431176,
      "loss": 0.9111,
      "step": 3740
    },
    {
      "epoch": 1.7067335080323573,
      "grad_norm": 0.15031324326992035,
      "learning_rate": 0.0006586144029170466,
      "loss": 0.8709,
      "step": 3745
    },
    {
      "epoch": 1.7090121909536289,
      "grad_norm": 0.15032295882701874,
      "learning_rate": 0.000658158614402917,
      "loss": 0.8696,
      "step": 3750
    },
    {
      "epoch": 1.7112908738749004,
      "grad_norm": 0.15558771789073944,
      "learning_rate": 0.0006577028258887876,
      "loss": 0.8861,
      "step": 3755
    },
    {
      "epoch": 1.713569556796172,
      "grad_norm": 0.15421198308467865,
      "learning_rate": 0.0006572470373746582,
      "loss": 0.8788,
      "step": 3760
    },
    {
      "epoch": 1.7158482397174433,
      "grad_norm": 0.16312028467655182,
      "learning_rate": 0.0006567912488605287,
      "loss": 0.8759,
      "step": 3765
    },
    {
      "epoch": 1.7181269226387148,
      "grad_norm": 0.160825714468956,
      "learning_rate": 0.0006563354603463992,
      "loss": 0.9155,
      "step": 3770
    },
    {
      "epoch": 1.7204056055599863,
      "grad_norm": 0.17248572409152985,
      "learning_rate": 0.0006558796718322698,
      "loss": 0.8658,
      "step": 3775
    },
    {
      "epoch": 1.7226842884812579,
      "grad_norm": 0.21778759360313416,
      "learning_rate": 0.0006554238833181405,
      "loss": 0.8414,
      "step": 3780
    },
    {
      "epoch": 1.7249629714025292,
      "grad_norm": 0.18774458765983582,
      "learning_rate": 0.000654968094804011,
      "loss": 0.9039,
      "step": 3785
    },
    {
      "epoch": 1.7272416543238007,
      "grad_norm": 0.1789574772119522,
      "learning_rate": 0.0006545123062898815,
      "loss": 0.851,
      "step": 3790
    },
    {
      "epoch": 1.7295203372450723,
      "grad_norm": 0.17148630321025848,
      "learning_rate": 0.000654056517775752,
      "loss": 0.8786,
      "step": 3795
    },
    {
      "epoch": 1.7317990201663438,
      "grad_norm": 0.16520605981349945,
      "learning_rate": 0.0006536007292616226,
      "loss": 0.9033,
      "step": 3800
    },
    {
      "epoch": 1.7340777030876153,
      "grad_norm": 0.1509886384010315,
      "learning_rate": 0.0006531449407474932,
      "loss": 0.9102,
      "step": 3805
    },
    {
      "epoch": 1.7363563860088869,
      "grad_norm": 0.14856384694576263,
      "learning_rate": 0.0006526891522333637,
      "loss": 0.8848,
      "step": 3810
    },
    {
      "epoch": 1.7386350689301584,
      "grad_norm": 0.15910281240940094,
      "learning_rate": 0.0006522333637192342,
      "loss": 0.8864,
      "step": 3815
    },
    {
      "epoch": 1.74091375185143,
      "grad_norm": 0.1568024903535843,
      "learning_rate": 0.0006517775752051049,
      "loss": 0.8631,
      "step": 3820
    },
    {
      "epoch": 1.7431924347727015,
      "grad_norm": 0.157038152217865,
      "learning_rate": 0.0006513217866909754,
      "loss": 0.8854,
      "step": 3825
    },
    {
      "epoch": 1.745471117693973,
      "grad_norm": 0.1632712185382843,
      "learning_rate": 0.000650865998176846,
      "loss": 0.789,
      "step": 3830
    },
    {
      "epoch": 1.7477498006152445,
      "grad_norm": 0.15845654904842377,
      "learning_rate": 0.0006504102096627165,
      "loss": 0.9268,
      "step": 3835
    },
    {
      "epoch": 1.7500284835365159,
      "grad_norm": 0.19203679263591766,
      "learning_rate": 0.000649954421148587,
      "loss": 0.8931,
      "step": 3840
    },
    {
      "epoch": 1.7523071664577874,
      "grad_norm": 0.15941105782985687,
      "learning_rate": 0.0006494986326344576,
      "loss": 0.8909,
      "step": 3845
    },
    {
      "epoch": 1.754585849379059,
      "grad_norm": 0.16288477182388306,
      "learning_rate": 0.0006490428441203282,
      "loss": 0.871,
      "step": 3850
    },
    {
      "epoch": 1.7568645323003305,
      "grad_norm": 0.15899764001369476,
      "learning_rate": 0.0006485870556061986,
      "loss": 0.9041,
      "step": 3855
    },
    {
      "epoch": 1.7591432152216018,
      "grad_norm": 0.17480763792991638,
      "learning_rate": 0.0006481312670920693,
      "loss": 0.8836,
      "step": 3860
    },
    {
      "epoch": 1.7614218981428733,
      "grad_norm": 0.16292083263397217,
      "learning_rate": 0.0006476754785779399,
      "loss": 0.8585,
      "step": 3865
    },
    {
      "epoch": 1.7637005810641448,
      "grad_norm": 0.19208920001983643,
      "learning_rate": 0.0006472196900638104,
      "loss": 0.938,
      "step": 3870
    },
    {
      "epoch": 1.7659792639854164,
      "grad_norm": 0.17350582778453827,
      "learning_rate": 0.000646763901549681,
      "loss": 0.9313,
      "step": 3875
    },
    {
      "epoch": 1.768257946906688,
      "grad_norm": 0.1847466677427292,
      "learning_rate": 0.0006463081130355515,
      "loss": 0.9162,
      "step": 3880
    },
    {
      "epoch": 1.7705366298279595,
      "grad_norm": 0.1508982926607132,
      "learning_rate": 0.000645852324521422,
      "loss": 0.8829,
      "step": 3885
    },
    {
      "epoch": 1.772815312749231,
      "grad_norm": 0.16109327971935272,
      "learning_rate": 0.0006453965360072926,
      "loss": 0.8771,
      "step": 3890
    },
    {
      "epoch": 1.7750939956705025,
      "grad_norm": 0.19091853499412537,
      "learning_rate": 0.0006449407474931633,
      "loss": 0.9218,
      "step": 3895
    },
    {
      "epoch": 1.777372678591774,
      "grad_norm": 0.15589173138141632,
      "learning_rate": 0.0006444849589790337,
      "loss": 0.8713,
      "step": 3900
    },
    {
      "epoch": 1.7796513615130456,
      "grad_norm": 0.16589222848415375,
      "learning_rate": 0.0006440291704649043,
      "loss": 0.9326,
      "step": 3905
    },
    {
      "epoch": 1.7819300444343171,
      "grad_norm": 0.16091816127300262,
      "learning_rate": 0.0006435733819507749,
      "loss": 0.8354,
      "step": 3910
    },
    {
      "epoch": 1.7842087273555884,
      "grad_norm": 0.1553407609462738,
      "learning_rate": 0.0006431175934366454,
      "loss": 0.8575,
      "step": 3915
    },
    {
      "epoch": 1.78648741027686,
      "grad_norm": 0.18891474604606628,
      "learning_rate": 0.0006426618049225159,
      "loss": 0.9019,
      "step": 3920
    },
    {
      "epoch": 1.7887660931981315,
      "grad_norm": 0.17240558564662933,
      "learning_rate": 0.0006422060164083865,
      "loss": 0.8667,
      "step": 3925
    },
    {
      "epoch": 1.7910447761194028,
      "grad_norm": 0.18485990166664124,
      "learning_rate": 0.000641750227894257,
      "loss": 0.8595,
      "step": 3930
    },
    {
      "epoch": 1.7933234590406744,
      "grad_norm": 0.16615743935108185,
      "learning_rate": 0.0006412944393801277,
      "loss": 0.9374,
      "step": 3935
    },
    {
      "epoch": 1.795602141961946,
      "grad_norm": 0.1990218460559845,
      "learning_rate": 0.0006408386508659983,
      "loss": 0.9296,
      "step": 3940
    },
    {
      "epoch": 1.7978808248832174,
      "grad_norm": 0.1496436595916748,
      "learning_rate": 0.0006403828623518687,
      "loss": 0.7861,
      "step": 3945
    },
    {
      "epoch": 1.800159507804489,
      "grad_norm": 0.1822705715894699,
      "learning_rate": 0.0006399270738377393,
      "loss": 0.8709,
      "step": 3950
    },
    {
      "epoch": 1.8024381907257605,
      "grad_norm": 0.17043331265449524,
      "learning_rate": 0.0006394712853236099,
      "loss": 0.8444,
      "step": 3955
    },
    {
      "epoch": 1.804716873647032,
      "grad_norm": 0.171026349067688,
      "learning_rate": 0.0006390154968094804,
      "loss": 0.9324,
      "step": 3960
    },
    {
      "epoch": 1.8069955565683036,
      "grad_norm": 0.1597854048013687,
      "learning_rate": 0.0006385597082953509,
      "loss": 0.8517,
      "step": 3965
    },
    {
      "epoch": 1.809274239489575,
      "grad_norm": 0.15125171840190887,
      "learning_rate": 0.0006381039197812215,
      "loss": 0.8426,
      "step": 3970
    },
    {
      "epoch": 1.8115529224108466,
      "grad_norm": 0.18407511711120605,
      "learning_rate": 0.0006376481312670921,
      "loss": 0.9172,
      "step": 3975
    },
    {
      "epoch": 1.8138316053321182,
      "grad_norm": 0.17426910996437073,
      "learning_rate": 0.0006371923427529627,
      "loss": 0.8719,
      "step": 3980
    },
    {
      "epoch": 1.8161102882533895,
      "grad_norm": 0.16318969428539276,
      "learning_rate": 0.0006367365542388332,
      "loss": 0.9279,
      "step": 3985
    },
    {
      "epoch": 1.818388971174661,
      "grad_norm": 0.17125460505485535,
      "learning_rate": 0.0006362807657247037,
      "loss": 0.8783,
      "step": 3990
    },
    {
      "epoch": 1.8206676540959326,
      "grad_norm": 0.15742672979831696,
      "learning_rate": 0.0006358249772105743,
      "loss": 0.8802,
      "step": 3995
    },
    {
      "epoch": 1.822946337017204,
      "grad_norm": 0.15650297701358795,
      "learning_rate": 0.0006353691886964449,
      "loss": 0.9488,
      "step": 4000
    },
    {
      "epoch": 1.8252250199384754,
      "grad_norm": 0.14951655268669128,
      "learning_rate": 0.0006349134001823154,
      "loss": 0.8679,
      "step": 4005
    },
    {
      "epoch": 1.827503702859747,
      "grad_norm": 0.1454230099916458,
      "learning_rate": 0.000634457611668186,
      "loss": 0.8803,
      "step": 4010
    },
    {
      "epoch": 1.8297823857810185,
      "grad_norm": 0.15413609147071838,
      "learning_rate": 0.0006340018231540566,
      "loss": 0.8141,
      "step": 4015
    },
    {
      "epoch": 1.83206106870229,
      "grad_norm": 0.1627730280160904,
      "learning_rate": 0.0006335460346399271,
      "loss": 0.8771,
      "step": 4020
    },
    {
      "epoch": 1.8343397516235616,
      "grad_norm": 0.152581125497818,
      "learning_rate": 0.0006330902461257977,
      "loss": 0.8926,
      "step": 4025
    },
    {
      "epoch": 1.836618434544833,
      "grad_norm": 0.19089622795581818,
      "learning_rate": 0.0006326344576116682,
      "loss": 0.8805,
      "step": 4030
    },
    {
      "epoch": 1.8388971174661046,
      "grad_norm": 0.16863283514976501,
      "learning_rate": 0.0006321786690975387,
      "loss": 0.8628,
      "step": 4035
    },
    {
      "epoch": 1.8411758003873762,
      "grad_norm": 0.1554223746061325,
      "learning_rate": 0.0006317228805834093,
      "loss": 0.8019,
      "step": 4040
    },
    {
      "epoch": 1.8434544833086477,
      "grad_norm": 0.18285095691680908,
      "learning_rate": 0.0006312670920692799,
      "loss": 0.8931,
      "step": 4045
    },
    {
      "epoch": 1.8457331662299192,
      "grad_norm": 0.17418868839740753,
      "learning_rate": 0.0006308113035551504,
      "loss": 0.8498,
      "step": 4050
    },
    {
      "epoch": 1.8480118491511908,
      "grad_norm": 0.1711757332086563,
      "learning_rate": 0.000630355515041021,
      "loss": 0.8874,
      "step": 4055
    },
    {
      "epoch": 1.850290532072462,
      "grad_norm": 0.15627935528755188,
      "learning_rate": 0.0006298997265268916,
      "loss": 0.8591,
      "step": 4060
    },
    {
      "epoch": 1.8525692149937336,
      "grad_norm": 0.16980907320976257,
      "learning_rate": 0.0006294439380127621,
      "loss": 0.9013,
      "step": 4065
    },
    {
      "epoch": 1.8548478979150052,
      "grad_norm": 0.18031106889247894,
      "learning_rate": 0.0006289881494986327,
      "loss": 0.8506,
      "step": 4070
    },
    {
      "epoch": 1.8571265808362767,
      "grad_norm": 0.15851819515228271,
      "learning_rate": 0.0006285323609845031,
      "loss": 0.868,
      "step": 4075
    },
    {
      "epoch": 1.859405263757548,
      "grad_norm": 0.1530737578868866,
      "learning_rate": 0.0006280765724703737,
      "loss": 0.8694,
      "step": 4080
    },
    {
      "epoch": 1.8616839466788195,
      "grad_norm": 0.14582288265228271,
      "learning_rate": 0.0006276207839562444,
      "loss": 0.8219,
      "step": 4085
    },
    {
      "epoch": 1.863962629600091,
      "grad_norm": 0.18774089217185974,
      "learning_rate": 0.000627164995442115,
      "loss": 0.8742,
      "step": 4090
    },
    {
      "epoch": 1.8662413125213626,
      "grad_norm": 0.15002906322479248,
      "learning_rate": 0.0006267092069279854,
      "loss": 0.9382,
      "step": 4095
    },
    {
      "epoch": 1.8685199954426341,
      "grad_norm": 0.1654605269432068,
      "learning_rate": 0.000626253418413856,
      "loss": 0.8998,
      "step": 4100
    },
    {
      "epoch": 1.8707986783639057,
      "grad_norm": 0.16400764882564545,
      "learning_rate": 0.0006257976298997265,
      "loss": 0.8673,
      "step": 4105
    },
    {
      "epoch": 1.8730773612851772,
      "grad_norm": 0.17620471119880676,
      "learning_rate": 0.0006253418413855971,
      "loss": 0.8778,
      "step": 4110
    },
    {
      "epoch": 1.8753560442064487,
      "grad_norm": 0.19134825468063354,
      "learning_rate": 0.0006248860528714676,
      "loss": 0.8457,
      "step": 4115
    },
    {
      "epoch": 1.8776347271277203,
      "grad_norm": 0.17811906337738037,
      "learning_rate": 0.0006244302643573381,
      "loss": 0.8885,
      "step": 4120
    },
    {
      "epoch": 1.8799134100489918,
      "grad_norm": 0.15422867238521576,
      "learning_rate": 0.0006239744758432088,
      "loss": 0.8378,
      "step": 4125
    },
    {
      "epoch": 1.8821920929702634,
      "grad_norm": 0.15551601350307465,
      "learning_rate": 0.0006235186873290794,
      "loss": 0.8809,
      "step": 4130
    },
    {
      "epoch": 1.8844707758915347,
      "grad_norm": 0.15928076207637787,
      "learning_rate": 0.00062306289881495,
      "loss": 0.8044,
      "step": 4135
    },
    {
      "epoch": 1.8867494588128062,
      "grad_norm": 0.1689259111881256,
      "learning_rate": 0.0006226071103008204,
      "loss": 0.8406,
      "step": 4140
    },
    {
      "epoch": 1.8890281417340777,
      "grad_norm": 0.1785009801387787,
      "learning_rate": 0.000622151321786691,
      "loss": 0.8875,
      "step": 4145
    },
    {
      "epoch": 1.891306824655349,
      "grad_norm": 0.16628743708133698,
      "learning_rate": 0.0006216955332725615,
      "loss": 0.8881,
      "step": 4150
    },
    {
      "epoch": 1.8935855075766206,
      "grad_norm": 0.14506720006465912,
      "learning_rate": 0.0006212397447584321,
      "loss": 0.8534,
      "step": 4155
    },
    {
      "epoch": 1.8958641904978921,
      "grad_norm": 0.1643017679452896,
      "learning_rate": 0.0006207839562443026,
      "loss": 0.8361,
      "step": 4160
    },
    {
      "epoch": 1.8981428734191637,
      "grad_norm": 0.19130554795265198,
      "learning_rate": 0.0006203281677301732,
      "loss": 0.8298,
      "step": 4165
    },
    {
      "epoch": 1.9004215563404352,
      "grad_norm": 0.15748248994350433,
      "learning_rate": 0.0006198723792160438,
      "loss": 0.8525,
      "step": 4170
    },
    {
      "epoch": 1.9027002392617067,
      "grad_norm": 0.16031228005886078,
      "learning_rate": 0.0006194165907019144,
      "loss": 0.909,
      "step": 4175
    },
    {
      "epoch": 1.9049789221829783,
      "grad_norm": 0.16477163136005402,
      "learning_rate": 0.0006189608021877848,
      "loss": 0.8884,
      "step": 4180
    },
    {
      "epoch": 1.9072576051042498,
      "grad_norm": 0.15229244530200958,
      "learning_rate": 0.0006185050136736554,
      "loss": 0.8607,
      "step": 4185
    },
    {
      "epoch": 1.9095362880255213,
      "grad_norm": 0.15150196850299835,
      "learning_rate": 0.000618049225159526,
      "loss": 0.8791,
      "step": 4190
    },
    {
      "epoch": 1.9118149709467929,
      "grad_norm": 0.16399160027503967,
      "learning_rate": 0.0006175934366453965,
      "loss": 0.8735,
      "step": 4195
    },
    {
      "epoch": 1.9140936538680644,
      "grad_norm": 0.16528384387493134,
      "learning_rate": 0.0006171376481312672,
      "loss": 0.9058,
      "step": 4200
    },
    {
      "epoch": 1.9163723367893357,
      "grad_norm": 0.15969732403755188,
      "learning_rate": 0.0006166818596171377,
      "loss": 0.871,
      "step": 4205
    },
    {
      "epoch": 1.9186510197106073,
      "grad_norm": 0.16041995584964752,
      "learning_rate": 0.0006162260711030082,
      "loss": 0.8635,
      "step": 4210
    },
    {
      "epoch": 1.9209297026318788,
      "grad_norm": 0.16234171390533447,
      "learning_rate": 0.0006157702825888788,
      "loss": 0.8462,
      "step": 4215
    },
    {
      "epoch": 1.9232083855531503,
      "grad_norm": 0.14623446762561798,
      "learning_rate": 0.0006153144940747494,
      "loss": 0.8817,
      "step": 4220
    },
    {
      "epoch": 1.9254870684744216,
      "grad_norm": 0.18903551995754242,
      "learning_rate": 0.0006148587055606198,
      "loss": 0.7836,
      "step": 4225
    },
    {
      "epoch": 1.9277657513956932,
      "grad_norm": 0.16316522657871246,
      "learning_rate": 0.0006144029170464904,
      "loss": 0.9306,
      "step": 4230
    },
    {
      "epoch": 1.9300444343169647,
      "grad_norm": 0.15727995336055756,
      "learning_rate": 0.000613947128532361,
      "loss": 0.8754,
      "step": 4235
    },
    {
      "epoch": 1.9323231172382362,
      "grad_norm": 0.17110374569892883,
      "learning_rate": 0.0006134913400182316,
      "loss": 0.88,
      "step": 4240
    },
    {
      "epoch": 1.9346018001595078,
      "grad_norm": 0.1580423265695572,
      "learning_rate": 0.0006130355515041021,
      "loss": 0.8986,
      "step": 4245
    },
    {
      "epoch": 1.9368804830807793,
      "grad_norm": 0.1625204086303711,
      "learning_rate": 0.0006125797629899727,
      "loss": 0.9017,
      "step": 4250
    },
    {
      "epoch": 1.9391591660020508,
      "grad_norm": 0.1607917845249176,
      "learning_rate": 0.0006121239744758432,
      "loss": 0.8611,
      "step": 4255
    },
    {
      "epoch": 1.9414378489233224,
      "grad_norm": 0.19620633125305176,
      "learning_rate": 0.0006116681859617138,
      "loss": 0.9015,
      "step": 4260
    },
    {
      "epoch": 1.943716531844594,
      "grad_norm": 0.1831812858581543,
      "learning_rate": 0.0006112123974475843,
      "loss": 0.9285,
      "step": 4265
    },
    {
      "epoch": 1.9459952147658655,
      "grad_norm": 0.1958901584148407,
      "learning_rate": 0.0006107566089334548,
      "loss": 0.8922,
      "step": 4270
    },
    {
      "epoch": 1.948273897687137,
      "grad_norm": 0.16082413494586945,
      "learning_rate": 0.0006103008204193254,
      "loss": 0.9349,
      "step": 4275
    },
    {
      "epoch": 1.9505525806084083,
      "grad_norm": 0.17029252648353577,
      "learning_rate": 0.0006098450319051961,
      "loss": 0.9388,
      "step": 4280
    },
    {
      "epoch": 1.9528312635296798,
      "grad_norm": 0.1668144017457962,
      "learning_rate": 0.0006093892433910666,
      "loss": 0.9048,
      "step": 4285
    },
    {
      "epoch": 1.9551099464509514,
      "grad_norm": 0.16046059131622314,
      "learning_rate": 0.0006089334548769371,
      "loss": 0.8726,
      "step": 4290
    },
    {
      "epoch": 1.957388629372223,
      "grad_norm": 0.1715429127216339,
      "learning_rate": 0.0006084776663628077,
      "loss": 0.7983,
      "step": 4295
    },
    {
      "epoch": 1.9596673122934942,
      "grad_norm": 0.1771450936794281,
      "learning_rate": 0.0006080218778486782,
      "loss": 0.9036,
      "step": 4300
    },
    {
      "epoch": 1.9619459952147658,
      "grad_norm": 0.1681606024503708,
      "learning_rate": 0.0006075660893345488,
      "loss": 0.8521,
      "step": 4305
    },
    {
      "epoch": 1.9642246781360373,
      "grad_norm": 0.16675446927547455,
      "learning_rate": 0.0006071103008204193,
      "loss": 0.8727,
      "step": 4310
    },
    {
      "epoch": 1.9665033610573088,
      "grad_norm": 0.16467812657356262,
      "learning_rate": 0.0006066545123062898,
      "loss": 0.8145,
      "step": 4315
    },
    {
      "epoch": 1.9687820439785804,
      "grad_norm": 0.1470973789691925,
      "learning_rate": 0.0006061987237921605,
      "loss": 0.866,
      "step": 4320
    },
    {
      "epoch": 1.971060726899852,
      "grad_norm": 0.19304044544696808,
      "learning_rate": 0.0006057429352780311,
      "loss": 0.8937,
      "step": 4325
    },
    {
      "epoch": 1.9733394098211234,
      "grad_norm": 0.172626331448555,
      "learning_rate": 0.0006052871467639015,
      "loss": 0.9068,
      "step": 4330
    },
    {
      "epoch": 1.975618092742395,
      "grad_norm": 0.17286615073680878,
      "learning_rate": 0.0006048313582497721,
      "loss": 0.9916,
      "step": 4335
    },
    {
      "epoch": 1.9778967756636665,
      "grad_norm": 0.15804767608642578,
      "learning_rate": 0.0006043755697356427,
      "loss": 0.8845,
      "step": 4340
    },
    {
      "epoch": 1.980175458584938,
      "grad_norm": 0.17305949330329895,
      "learning_rate": 0.0006039197812215132,
      "loss": 0.832,
      "step": 4345
    },
    {
      "epoch": 1.9824541415062096,
      "grad_norm": 0.15246599912643433,
      "learning_rate": 0.0006034639927073838,
      "loss": 0.8889,
      "step": 4350
    },
    {
      "epoch": 1.984732824427481,
      "grad_norm": 0.15830343961715698,
      "learning_rate": 0.0006030082041932544,
      "loss": 0.8645,
      "step": 4355
    },
    {
      "epoch": 1.9870115073487524,
      "grad_norm": 0.17063474655151367,
      "learning_rate": 0.0006025524156791249,
      "loss": 0.8635,
      "step": 4360
    },
    {
      "epoch": 1.989290190270024,
      "grad_norm": 0.17632798850536346,
      "learning_rate": 0.0006020966271649955,
      "loss": 0.8768,
      "step": 4365
    },
    {
      "epoch": 1.9915688731912953,
      "grad_norm": 0.15465405583381653,
      "learning_rate": 0.0006016408386508661,
      "loss": 0.88,
      "step": 4370
    },
    {
      "epoch": 1.9938475561125668,
      "grad_norm": 0.15984690189361572,
      "learning_rate": 0.0006011850501367365,
      "loss": 0.8527,
      "step": 4375
    },
    {
      "epoch": 1.9961262390338383,
      "grad_norm": 0.18223103880882263,
      "learning_rate": 0.0006007292616226071,
      "loss": 0.8786,
      "step": 4380
    },
    {
      "epoch": 1.9984049219551099,
      "grad_norm": 0.19384711980819702,
      "learning_rate": 0.0006002734731084777,
      "loss": 0.8858,
      "step": 4385
    },
    {
      "epoch": 1.9997721317078727,
      "eval_loss": 0.7715873718261719,
      "eval_runtime": 793.4715,
      "eval_samples_per_second": 25.281,
      "eval_steps_per_second": 3.161,
      "step": 4388
    },
    {
      "epoch": 2.0006836048763814,
      "grad_norm": 0.15652315318584442,
      "learning_rate": 0.0005998176845943482,
      "loss": 0.8508,
      "step": 4390
    },
    {
      "epoch": 2.002962287797653,
      "grad_norm": 0.16440053284168243,
      "learning_rate": 0.0005993618960802188,
      "loss": 0.834,
      "step": 4395
    },
    {
      "epoch": 2.0052409707189245,
      "grad_norm": 0.1687597632408142,
      "learning_rate": 0.0005989061075660894,
      "loss": 0.8558,
      "step": 4400
    },
    {
      "epoch": 2.007519653640196,
      "grad_norm": 0.17012061178684235,
      "learning_rate": 0.0005984503190519599,
      "loss": 0.9171,
      "step": 4405
    },
    {
      "epoch": 2.0097983365614676,
      "grad_norm": 0.16624945402145386,
      "learning_rate": 0.0005979945305378305,
      "loss": 0.8752,
      "step": 4410
    },
    {
      "epoch": 2.012077019482739,
      "grad_norm": 0.18640756607055664,
      "learning_rate": 0.000597538742023701,
      "loss": 0.8512,
      "step": 4415
    },
    {
      "epoch": 2.0143557024040106,
      "grad_norm": 0.1499350219964981,
      "learning_rate": 0.0005970829535095715,
      "loss": 0.8874,
      "step": 4420
    },
    {
      "epoch": 2.016634385325282,
      "grad_norm": 0.17012932896614075,
      "learning_rate": 0.0005966271649954421,
      "loss": 0.8365,
      "step": 4425
    },
    {
      "epoch": 2.0189130682465537,
      "grad_norm": 0.16577626764774323,
      "learning_rate": 0.0005961713764813126,
      "loss": 0.8591,
      "step": 4430
    },
    {
      "epoch": 2.021191751167825,
      "grad_norm": 0.18787001073360443,
      "learning_rate": 0.0005957155879671833,
      "loss": 0.8687,
      "step": 4435
    },
    {
      "epoch": 2.0234704340890963,
      "grad_norm": 0.1706690788269043,
      "learning_rate": 0.0005952597994530538,
      "loss": 0.9054,
      "step": 4440
    },
    {
      "epoch": 2.025749117010368,
      "grad_norm": 0.16311012208461761,
      "learning_rate": 0.0005948040109389243,
      "loss": 0.8717,
      "step": 4445
    },
    {
      "epoch": 2.0280277999316394,
      "grad_norm": 0.15088506042957306,
      "learning_rate": 0.0005943482224247949,
      "loss": 0.8395,
      "step": 4450
    },
    {
      "epoch": 2.030306482852911,
      "grad_norm": 0.1735272854566574,
      "learning_rate": 0.0005938924339106655,
      "loss": 0.8624,
      "step": 4455
    },
    {
      "epoch": 2.0325851657741825,
      "grad_norm": 0.17969456315040588,
      "learning_rate": 0.0005934366453965359,
      "loss": 0.8376,
      "step": 4460
    },
    {
      "epoch": 2.034863848695454,
      "grad_norm": 0.1737034171819687,
      "learning_rate": 0.0005929808568824065,
      "loss": 0.8956,
      "step": 4465
    },
    {
      "epoch": 2.0371425316167255,
      "grad_norm": 0.18710659444332123,
      "learning_rate": 0.0005925250683682772,
      "loss": 0.8296,
      "step": 4470
    },
    {
      "epoch": 2.039421214537997,
      "grad_norm": 0.20648546516895294,
      "learning_rate": 0.0005920692798541477,
      "loss": 0.83,
      "step": 4475
    },
    {
      "epoch": 2.0416998974592686,
      "grad_norm": 0.1711016595363617,
      "learning_rate": 0.0005916134913400183,
      "loss": 0.9436,
      "step": 4480
    },
    {
      "epoch": 2.04397858038054,
      "grad_norm": 0.16445587575435638,
      "learning_rate": 0.0005911577028258888,
      "loss": 0.8461,
      "step": 4485
    },
    {
      "epoch": 2.0462572633018117,
      "grad_norm": 0.14430585503578186,
      "learning_rate": 0.0005907019143117593,
      "loss": 0.809,
      "step": 4490
    },
    {
      "epoch": 2.048535946223083,
      "grad_norm": 0.15260957181453705,
      "learning_rate": 0.0005902461257976299,
      "loss": 0.8708,
      "step": 4495
    },
    {
      "epoch": 2.0508146291443548,
      "grad_norm": 0.14294932782649994,
      "learning_rate": 0.0005897903372835005,
      "loss": 0.8472,
      "step": 4500
    },
    {
      "epoch": 2.0530933120656263,
      "grad_norm": 0.15101203322410583,
      "learning_rate": 0.0005893345487693709,
      "loss": 0.8648,
      "step": 4505
    },
    {
      "epoch": 2.0553719949868974,
      "grad_norm": 0.15476013720035553,
      "learning_rate": 0.0005888787602552416,
      "loss": 0.8761,
      "step": 4510
    },
    {
      "epoch": 2.057650677908169,
      "grad_norm": 0.18926239013671875,
      "learning_rate": 0.0005884229717411122,
      "loss": 0.9116,
      "step": 4515
    },
    {
      "epoch": 2.0599293608294404,
      "grad_norm": 0.18188858032226562,
      "learning_rate": 0.0005879671832269827,
      "loss": 0.8811,
      "step": 4520
    },
    {
      "epoch": 2.062208043750712,
      "grad_norm": 0.18115803599357605,
      "learning_rate": 0.0005875113947128532,
      "loss": 0.87,
      "step": 4525
    },
    {
      "epoch": 2.0644867266719835,
      "grad_norm": 0.17555996775627136,
      "learning_rate": 0.0005870556061987238,
      "loss": 0.8874,
      "step": 4530
    },
    {
      "epoch": 2.066765409593255,
      "grad_norm": 0.1652979701757431,
      "learning_rate": 0.0005865998176845943,
      "loss": 0.8663,
      "step": 4535
    },
    {
      "epoch": 2.0690440925145266,
      "grad_norm": 0.18945854902267456,
      "learning_rate": 0.0005861440291704649,
      "loss": 0.9057,
      "step": 4540
    },
    {
      "epoch": 2.071322775435798,
      "grad_norm": 0.17263370752334595,
      "learning_rate": 0.0005856882406563356,
      "loss": 0.8634,
      "step": 4545
    },
    {
      "epoch": 2.0736014583570697,
      "grad_norm": 0.17221808433532715,
      "learning_rate": 0.000585232452142206,
      "loss": 0.8995,
      "step": 4550
    },
    {
      "epoch": 2.075880141278341,
      "grad_norm": 0.16701728105545044,
      "learning_rate": 0.0005847766636280766,
      "loss": 0.8264,
      "step": 4555
    },
    {
      "epoch": 2.0781588241996127,
      "grad_norm": 0.19209441542625427,
      "learning_rate": 0.0005843208751139472,
      "loss": 0.9117,
      "step": 4560
    },
    {
      "epoch": 2.0804375071208843,
      "grad_norm": 0.16025176644325256,
      "learning_rate": 0.0005838650865998177,
      "loss": 0.8713,
      "step": 4565
    },
    {
      "epoch": 2.082716190042156,
      "grad_norm": 0.173211470246315,
      "learning_rate": 0.0005834092980856882,
      "loss": 0.8539,
      "step": 4570
    },
    {
      "epoch": 2.0849948729634273,
      "grad_norm": 0.166391059756279,
      "learning_rate": 0.0005829535095715588,
      "loss": 0.8382,
      "step": 4575
    },
    {
      "epoch": 2.087273555884699,
      "grad_norm": 0.1731705665588379,
      "learning_rate": 0.0005824977210574293,
      "loss": 0.8466,
      "step": 4580
    },
    {
      "epoch": 2.08955223880597,
      "grad_norm": 0.1658148318529129,
      "learning_rate": 0.0005820419325433,
      "loss": 0.934,
      "step": 4585
    },
    {
      "epoch": 2.0918309217272415,
      "grad_norm": 0.14501190185546875,
      "learning_rate": 0.0005815861440291705,
      "loss": 0.8957,
      "step": 4590
    },
    {
      "epoch": 2.094109604648513,
      "grad_norm": 0.16343189775943756,
      "learning_rate": 0.000581130355515041,
      "loss": 0.8536,
      "step": 4595
    },
    {
      "epoch": 2.0963882875697846,
      "grad_norm": 0.19694533944129944,
      "learning_rate": 0.0005806745670009116,
      "loss": 0.8603,
      "step": 4600
    },
    {
      "epoch": 2.098666970491056,
      "grad_norm": 0.16850019991397858,
      "learning_rate": 0.0005802187784867822,
      "loss": 0.9055,
      "step": 4605
    },
    {
      "epoch": 2.1009456534123276,
      "grad_norm": 0.1598106026649475,
      "learning_rate": 0.0005797629899726527,
      "loss": 0.8109,
      "step": 4610
    },
    {
      "epoch": 2.103224336333599,
      "grad_norm": 0.1675305962562561,
      "learning_rate": 0.0005793072014585232,
      "loss": 0.9331,
      "step": 4615
    },
    {
      "epoch": 2.1055030192548707,
      "grad_norm": 0.15340295433998108,
      "learning_rate": 0.0005788514129443938,
      "loss": 0.8264,
      "step": 4620
    },
    {
      "epoch": 2.1077817021761422,
      "grad_norm": 0.170551598072052,
      "learning_rate": 0.0005783956244302644,
      "loss": 0.9063,
      "step": 4625
    },
    {
      "epoch": 2.110060385097414,
      "grad_norm": 0.190398171544075,
      "learning_rate": 0.000577939835916135,
      "loss": 0.8824,
      "step": 4630
    },
    {
      "epoch": 2.1123390680186853,
      "grad_norm": 0.16866131126880646,
      "learning_rate": 0.0005774840474020055,
      "loss": 0.8423,
      "step": 4635
    },
    {
      "epoch": 2.114617750939957,
      "grad_norm": 0.16871261596679688,
      "learning_rate": 0.000577028258887876,
      "loss": 0.8706,
      "step": 4640
    },
    {
      "epoch": 2.1168964338612284,
      "grad_norm": 0.17751674354076385,
      "learning_rate": 0.0005765724703737466,
      "loss": 0.796,
      "step": 4645
    },
    {
      "epoch": 2.1191751167825,
      "grad_norm": 0.15920855104923248,
      "learning_rate": 0.0005761166818596172,
      "loss": 0.8633,
      "step": 4650
    },
    {
      "epoch": 2.121453799703771,
      "grad_norm": 0.1762808859348297,
      "learning_rate": 0.0005756608933454876,
      "loss": 0.8899,
      "step": 4655
    },
    {
      "epoch": 2.1237324826250426,
      "grad_norm": 0.17221027612686157,
      "learning_rate": 0.0005752051048313582,
      "loss": 0.9069,
      "step": 4660
    },
    {
      "epoch": 2.126011165546314,
      "grad_norm": 0.17434334754943848,
      "learning_rate": 0.0005747493163172289,
      "loss": 0.9107,
      "step": 4665
    },
    {
      "epoch": 2.1282898484675856,
      "grad_norm": 0.18517260253429413,
      "learning_rate": 0.0005742935278030994,
      "loss": 0.9127,
      "step": 4670
    },
    {
      "epoch": 2.130568531388857,
      "grad_norm": 0.18095043301582336,
      "learning_rate": 0.00057383773928897,
      "loss": 0.8268,
      "step": 4675
    },
    {
      "epoch": 2.1328472143101287,
      "grad_norm": 0.17109712958335876,
      "learning_rate": 0.0005733819507748405,
      "loss": 0.8568,
      "step": 4680
    },
    {
      "epoch": 2.1351258972314002,
      "grad_norm": 0.1671684831380844,
      "learning_rate": 0.000572926162260711,
      "loss": 0.8394,
      "step": 4685
    },
    {
      "epoch": 2.1374045801526718,
      "grad_norm": 0.16517393290996552,
      "learning_rate": 0.0005724703737465816,
      "loss": 0.8153,
      "step": 4690
    },
    {
      "epoch": 2.1396832630739433,
      "grad_norm": 0.1610446274280548,
      "learning_rate": 0.0005720145852324522,
      "loss": 0.8298,
      "step": 4695
    },
    {
      "epoch": 2.141961945995215,
      "grad_norm": 0.1547217071056366,
      "learning_rate": 0.0005715587967183227,
      "loss": 0.8681,
      "step": 4700
    },
    {
      "epoch": 2.1442406289164864,
      "grad_norm": 0.16927176713943481,
      "learning_rate": 0.0005711030082041933,
      "loss": 0.9064,
      "step": 4705
    },
    {
      "epoch": 2.146519311837758,
      "grad_norm": 0.16545045375823975,
      "learning_rate": 0.0005706472196900639,
      "loss": 0.8855,
      "step": 4710
    },
    {
      "epoch": 2.1487979947590294,
      "grad_norm": 0.171253964304924,
      "learning_rate": 0.0005701914311759344,
      "loss": 0.8472,
      "step": 4715
    },
    {
      "epoch": 2.151076677680301,
      "grad_norm": 0.162411168217659,
      "learning_rate": 0.0005697356426618049,
      "loss": 0.8765,
      "step": 4720
    },
    {
      "epoch": 2.153355360601572,
      "grad_norm": 0.17024894058704376,
      "learning_rate": 0.0005692798541476754,
      "loss": 0.9057,
      "step": 4725
    },
    {
      "epoch": 2.1556340435228436,
      "grad_norm": 0.16570010781288147,
      "learning_rate": 0.000568824065633546,
      "loss": 0.9112,
      "step": 4730
    },
    {
      "epoch": 2.157912726444115,
      "grad_norm": 0.1716299057006836,
      "learning_rate": 0.0005683682771194166,
      "loss": 0.8366,
      "step": 4735
    },
    {
      "epoch": 2.1601914093653867,
      "grad_norm": 0.16714802384376526,
      "learning_rate": 0.0005679124886052872,
      "loss": 0.8865,
      "step": 4740
    },
    {
      "epoch": 2.162470092286658,
      "grad_norm": 0.16573114693164825,
      "learning_rate": 0.0005674567000911577,
      "loss": 0.8729,
      "step": 4745
    },
    {
      "epoch": 2.1647487752079297,
      "grad_norm": 0.15606433153152466,
      "learning_rate": 0.0005670009115770283,
      "loss": 0.8941,
      "step": 4750
    },
    {
      "epoch": 2.1670274581292013,
      "grad_norm": 0.175241619348526,
      "learning_rate": 0.0005665451230628989,
      "loss": 0.8431,
      "step": 4755
    },
    {
      "epoch": 2.169306141050473,
      "grad_norm": 0.1590818166732788,
      "learning_rate": 0.0005660893345487694,
      "loss": 0.8357,
      "step": 4760
    },
    {
      "epoch": 2.1715848239717443,
      "grad_norm": 0.1591997891664505,
      "learning_rate": 0.0005656335460346399,
      "loss": 0.8849,
      "step": 4765
    },
    {
      "epoch": 2.173863506893016,
      "grad_norm": 0.16830898821353912,
      "learning_rate": 0.0005651777575205104,
      "loss": 0.8921,
      "step": 4770
    },
    {
      "epoch": 2.1761421898142874,
      "grad_norm": 0.1552257537841797,
      "learning_rate": 0.000564721969006381,
      "loss": 0.8928,
      "step": 4775
    },
    {
      "epoch": 2.178420872735559,
      "grad_norm": 0.16247016191482544,
      "learning_rate": 0.0005642661804922517,
      "loss": 0.806,
      "step": 4780
    },
    {
      "epoch": 2.1806995556568305,
      "grad_norm": 0.1870071142911911,
      "learning_rate": 0.0005638103919781221,
      "loss": 0.8284,
      "step": 4785
    },
    {
      "epoch": 2.182978238578102,
      "grad_norm": 0.17959120869636536,
      "learning_rate": 0.0005633546034639927,
      "loss": 0.9253,
      "step": 4790
    },
    {
      "epoch": 2.1852569214993736,
      "grad_norm": 0.17644505202770233,
      "learning_rate": 0.0005628988149498633,
      "loss": 0.8263,
      "step": 4795
    },
    {
      "epoch": 2.1875356044206447,
      "grad_norm": 0.1623745709657669,
      "learning_rate": 0.0005624430264357338,
      "loss": 0.8544,
      "step": 4800
    },
    {
      "epoch": 2.189814287341916,
      "grad_norm": 0.16872917115688324,
      "learning_rate": 0.0005619872379216043,
      "loss": 0.8642,
      "step": 4805
    },
    {
      "epoch": 2.1920929702631877,
      "grad_norm": 0.15705831348896027,
      "learning_rate": 0.0005615314494074749,
      "loss": 0.8427,
      "step": 4810
    },
    {
      "epoch": 2.1943716531844593,
      "grad_norm": 0.18059001863002777,
      "learning_rate": 0.0005610756608933455,
      "loss": 0.8434,
      "step": 4815
    },
    {
      "epoch": 2.196650336105731,
      "grad_norm": 0.1675601452589035,
      "learning_rate": 0.0005606198723792161,
      "loss": 0.7725,
      "step": 4820
    },
    {
      "epoch": 2.1989290190270023,
      "grad_norm": 0.2043471485376358,
      "learning_rate": 0.0005601640838650867,
      "loss": 0.8619,
      "step": 4825
    },
    {
      "epoch": 2.201207701948274,
      "grad_norm": 0.1881742775440216,
      "learning_rate": 0.0005597082953509571,
      "loss": 0.8475,
      "step": 4830
    },
    {
      "epoch": 2.2034863848695454,
      "grad_norm": 0.17679686844348907,
      "learning_rate": 0.0005592525068368277,
      "loss": 0.9437,
      "step": 4835
    },
    {
      "epoch": 2.205765067790817,
      "grad_norm": 0.15859150886535645,
      "learning_rate": 0.0005587967183226983,
      "loss": 0.957,
      "step": 4840
    },
    {
      "epoch": 2.2080437507120885,
      "grad_norm": 0.15073895454406738,
      "learning_rate": 0.0005583409298085688,
      "loss": 0.8077,
      "step": 4845
    },
    {
      "epoch": 2.21032243363336,
      "grad_norm": 0.18153105676174164,
      "learning_rate": 0.0005578851412944393,
      "loss": 0.8017,
      "step": 4850
    },
    {
      "epoch": 2.2126011165546315,
      "grad_norm": 0.1619998663663864,
      "learning_rate": 0.00055742935278031,
      "loss": 0.8923,
      "step": 4855
    },
    {
      "epoch": 2.214879799475903,
      "grad_norm": 0.17068417370319366,
      "learning_rate": 0.0005569735642661805,
      "loss": 0.7879,
      "step": 4860
    },
    {
      "epoch": 2.2171584823971746,
      "grad_norm": 0.15686005353927612,
      "learning_rate": 0.0005565177757520511,
      "loss": 0.7908,
      "step": 4865
    },
    {
      "epoch": 2.219437165318446,
      "grad_norm": 0.14825762808322906,
      "learning_rate": 0.0005560619872379216,
      "loss": 0.8835,
      "step": 4870
    },
    {
      "epoch": 2.2217158482397172,
      "grad_norm": 0.14939990639686584,
      "learning_rate": 0.0005556061987237921,
      "loss": 0.8379,
      "step": 4875
    },
    {
      "epoch": 2.2239945311609888,
      "grad_norm": 0.16377300024032593,
      "learning_rate": 0.0005551504102096627,
      "loss": 0.8885,
      "step": 4880
    },
    {
      "epoch": 2.2262732140822603,
      "grad_norm": 0.17196238040924072,
      "learning_rate": 0.0005546946216955333,
      "loss": 0.8177,
      "step": 4885
    },
    {
      "epoch": 2.228551897003532,
      "grad_norm": 0.1533796340227127,
      "learning_rate": 0.0005542388331814038,
      "loss": 0.8482,
      "step": 4890
    },
    {
      "epoch": 2.2308305799248034,
      "grad_norm": 0.1722499281167984,
      "learning_rate": 0.0005537830446672744,
      "loss": 0.8507,
      "step": 4895
    },
    {
      "epoch": 2.233109262846075,
      "grad_norm": 0.17147760093212128,
      "learning_rate": 0.000553327256153145,
      "loss": 0.9107,
      "step": 4900
    },
    {
      "epoch": 2.2353879457673465,
      "grad_norm": 0.17574083805084229,
      "learning_rate": 0.0005528714676390155,
      "loss": 0.9162,
      "step": 4905
    },
    {
      "epoch": 2.237666628688618,
      "grad_norm": 0.1723356395959854,
      "learning_rate": 0.0005524156791248861,
      "loss": 0.8186,
      "step": 4910
    },
    {
      "epoch": 2.2399453116098895,
      "grad_norm": 0.19931329786777496,
      "learning_rate": 0.0005519598906107566,
      "loss": 0.823,
      "step": 4915
    },
    {
      "epoch": 2.242223994531161,
      "grad_norm": 0.17596307396888733,
      "learning_rate": 0.0005515041020966271,
      "loss": 0.862,
      "step": 4920
    },
    {
      "epoch": 2.2445026774524326,
      "grad_norm": 0.17456266283988953,
      "learning_rate": 0.0005510483135824977,
      "loss": 0.9339,
      "step": 4925
    },
    {
      "epoch": 2.246781360373704,
      "grad_norm": 0.17912563681602478,
      "learning_rate": 0.0005505925250683684,
      "loss": 0.8395,
      "step": 4930
    },
    {
      "epoch": 2.2490600432949757,
      "grad_norm": 0.17814290523529053,
      "learning_rate": 0.0005501367365542388,
      "loss": 0.8545,
      "step": 4935
    },
    {
      "epoch": 2.251338726216247,
      "grad_norm": 0.1634518802165985,
      "learning_rate": 0.0005496809480401094,
      "loss": 0.8213,
      "step": 4940
    },
    {
      "epoch": 2.2536174091375187,
      "grad_norm": 0.16118206083774567,
      "learning_rate": 0.00054922515952598,
      "loss": 0.9122,
      "step": 4945
    },
    {
      "epoch": 2.25589609205879,
      "grad_norm": 0.19122940301895142,
      "learning_rate": 0.0005487693710118505,
      "loss": 0.8683,
      "step": 4950
    },
    {
      "epoch": 2.2581747749800614,
      "grad_norm": 0.1760578751564026,
      "learning_rate": 0.0005483135824977211,
      "loss": 0.906,
      "step": 4955
    },
    {
      "epoch": 2.260453457901333,
      "grad_norm": 0.18412280082702637,
      "learning_rate": 0.0005478577939835916,
      "loss": 0.8742,
      "step": 4960
    },
    {
      "epoch": 2.2627321408226044,
      "grad_norm": 0.17714278399944305,
      "learning_rate": 0.0005474020054694621,
      "loss": 0.8546,
      "step": 4965
    },
    {
      "epoch": 2.265010823743876,
      "grad_norm": 0.1619085967540741,
      "learning_rate": 0.0005469462169553328,
      "loss": 0.8188,
      "step": 4970
    },
    {
      "epoch": 2.2672895066651475,
      "grad_norm": 0.18134498596191406,
      "learning_rate": 0.0005464904284412034,
      "loss": 0.8376,
      "step": 4975
    },
    {
      "epoch": 2.269568189586419,
      "grad_norm": 0.1711444854736328,
      "learning_rate": 0.0005460346399270738,
      "loss": 0.8611,
      "step": 4980
    },
    {
      "epoch": 2.2718468725076906,
      "grad_norm": 0.2099141776561737,
      "learning_rate": 0.0005455788514129444,
      "loss": 0.8717,
      "step": 4985
    },
    {
      "epoch": 2.274125555428962,
      "grad_norm": 0.17674103379249573,
      "learning_rate": 0.000545123062898815,
      "loss": 0.829,
      "step": 4990
    },
    {
      "epoch": 2.2764042383502336,
      "grad_norm": 0.18297821283340454,
      "learning_rate": 0.0005446672743846855,
      "loss": 0.8181,
      "step": 4995
    },
    {
      "epoch": 2.278682921271505,
      "grad_norm": 0.1700832098722458,
      "learning_rate": 0.000544211485870556,
      "loss": 0.7864,
      "step": 5000
    },
    {
      "epoch": 2.2809616041927767,
      "grad_norm": 0.17711618542671204,
      "learning_rate": 0.0005437556973564266,
      "loss": 0.8931,
      "step": 5005
    },
    {
      "epoch": 2.2832402871140483,
      "grad_norm": 0.17089320719242096,
      "learning_rate": 0.0005432999088422972,
      "loss": 0.82,
      "step": 5010
    },
    {
      "epoch": 2.2855189700353193,
      "grad_norm": 0.190344899892807,
      "learning_rate": 0.0005428441203281678,
      "loss": 0.8153,
      "step": 5015
    },
    {
      "epoch": 2.2877976529565913,
      "grad_norm": 0.1463843733072281,
      "learning_rate": 0.0005423883318140384,
      "loss": 0.8162,
      "step": 5020
    },
    {
      "epoch": 2.2900763358778624,
      "grad_norm": 0.17214345932006836,
      "learning_rate": 0.0005419325432999088,
      "loss": 0.8144,
      "step": 5025
    },
    {
      "epoch": 2.292355018799134,
      "grad_norm": 0.46854642033576965,
      "learning_rate": 0.0005414767547857794,
      "loss": 0.7952,
      "step": 5030
    },
    {
      "epoch": 2.2946337017204055,
      "grad_norm": 0.1570647805929184,
      "learning_rate": 0.00054102096627165,
      "loss": 0.8068,
      "step": 5035
    },
    {
      "epoch": 2.296912384641677,
      "grad_norm": 0.1742517203092575,
      "learning_rate": 0.0005405651777575205,
      "loss": 0.8603,
      "step": 5040
    },
    {
      "epoch": 2.2991910675629486,
      "grad_norm": 0.17733338475227356,
      "learning_rate": 0.0005401093892433911,
      "loss": 0.9148,
      "step": 5045
    },
    {
      "epoch": 2.30146975048422,
      "grad_norm": 0.1924598067998886,
      "learning_rate": 0.0005396536007292617,
      "loss": 0.8659,
      "step": 5050
    },
    {
      "epoch": 2.3037484334054916,
      "grad_norm": 0.19051869213581085,
      "learning_rate": 0.0005391978122151322,
      "loss": 0.9167,
      "step": 5055
    },
    {
      "epoch": 2.306027116326763,
      "grad_norm": 0.2416839897632599,
      "learning_rate": 0.0005387420237010028,
      "loss": 0.8163,
      "step": 5060
    },
    {
      "epoch": 2.3083057992480347,
      "grad_norm": 0.16717711091041565,
      "learning_rate": 0.0005382862351868732,
      "loss": 0.8454,
      "step": 5065
    },
    {
      "epoch": 2.3105844821693062,
      "grad_norm": 0.1855553835630417,
      "learning_rate": 0.0005378304466727438,
      "loss": 0.9032,
      "step": 5070
    },
    {
      "epoch": 2.3128631650905778,
      "grad_norm": 0.16294705867767334,
      "learning_rate": 0.0005373746581586144,
      "loss": 0.8582,
      "step": 5075
    },
    {
      "epoch": 2.3151418480118493,
      "grad_norm": 0.1718621402978897,
      "learning_rate": 0.000536918869644485,
      "loss": 0.8927,
      "step": 5080
    },
    {
      "epoch": 2.317420530933121,
      "grad_norm": 0.17399796843528748,
      "learning_rate": 0.0005364630811303556,
      "loss": 0.8803,
      "step": 5085
    },
    {
      "epoch": 2.319699213854392,
      "grad_norm": 0.18053872883319855,
      "learning_rate": 0.0005360072926162261,
      "loss": 0.8918,
      "step": 5090
    },
    {
      "epoch": 2.321977896775664,
      "grad_norm": 0.16018038988113403,
      "learning_rate": 0.0005355515041020966,
      "loss": 0.8295,
      "step": 5095
    },
    {
      "epoch": 2.324256579696935,
      "grad_norm": 0.1575305312871933,
      "learning_rate": 0.0005350957155879672,
      "loss": 0.9325,
      "step": 5100
    },
    {
      "epoch": 2.3265352626182065,
      "grad_norm": 0.17494922876358032,
      "learning_rate": 0.0005346399270738378,
      "loss": 0.9401,
      "step": 5105
    },
    {
      "epoch": 2.328813945539478,
      "grad_norm": 0.18240231275558472,
      "learning_rate": 0.0005341841385597082,
      "loss": 0.8813,
      "step": 5110
    },
    {
      "epoch": 2.3310926284607496,
      "grad_norm": 0.17424778640270233,
      "learning_rate": 0.0005337283500455788,
      "loss": 0.8756,
      "step": 5115
    },
    {
      "epoch": 2.333371311382021,
      "grad_norm": 0.17939330637454987,
      "learning_rate": 0.0005332725615314494,
      "loss": 0.8697,
      "step": 5120
    },
    {
      "epoch": 2.3356499943032927,
      "grad_norm": 0.18888403475284576,
      "learning_rate": 0.00053281677301732,
      "loss": 0.8945,
      "step": 5125
    },
    {
      "epoch": 2.337928677224564,
      "grad_norm": 0.18706217408180237,
      "learning_rate": 0.0005323609845031905,
      "loss": 0.919,
      "step": 5130
    },
    {
      "epoch": 2.3402073601458357,
      "grad_norm": 0.1618768274784088,
      "learning_rate": 0.0005319051959890611,
      "loss": 0.9,
      "step": 5135
    },
    {
      "epoch": 2.3424860430671073,
      "grad_norm": 0.1559925675392151,
      "learning_rate": 0.0005314494074749316,
      "loss": 0.879,
      "step": 5140
    },
    {
      "epoch": 2.344764725988379,
      "grad_norm": 0.17392151057720184,
      "learning_rate": 0.0005309936189608022,
      "loss": 0.8932,
      "step": 5145
    },
    {
      "epoch": 2.3470434089096504,
      "grad_norm": 0.21474000811576843,
      "learning_rate": 0.0005305378304466728,
      "loss": 0.7683,
      "step": 5150
    },
    {
      "epoch": 2.349322091830922,
      "grad_norm": 0.1838427186012268,
      "learning_rate": 0.0005300820419325432,
      "loss": 0.8635,
      "step": 5155
    },
    {
      "epoch": 2.3516007747521934,
      "grad_norm": 0.17832082509994507,
      "learning_rate": 0.0005296262534184139,
      "loss": 0.8626,
      "step": 5160
    },
    {
      "epoch": 2.3538794576734645,
      "grad_norm": 0.17060911655426025,
      "learning_rate": 0.0005291704649042845,
      "loss": 0.8482,
      "step": 5165
    },
    {
      "epoch": 2.356158140594736,
      "grad_norm": 0.163331538438797,
      "learning_rate": 0.000528714676390155,
      "loss": 0.8345,
      "step": 5170
    },
    {
      "epoch": 2.3584368235160076,
      "grad_norm": 0.1767714023590088,
      "learning_rate": 0.0005282588878760255,
      "loss": 0.8738,
      "step": 5175
    },
    {
      "epoch": 2.360715506437279,
      "grad_norm": 0.1852366179227829,
      "learning_rate": 0.0005278030993618961,
      "loss": 0.8977,
      "step": 5180
    },
    {
      "epoch": 2.3629941893585507,
      "grad_norm": 0.17974160611629486,
      "learning_rate": 0.0005273473108477666,
      "loss": 0.7893,
      "step": 5185
    },
    {
      "epoch": 2.365272872279822,
      "grad_norm": 0.17651158571243286,
      "learning_rate": 0.0005268915223336372,
      "loss": 0.8669,
      "step": 5190
    },
    {
      "epoch": 2.3675515552010937,
      "grad_norm": 0.17345625162124634,
      "learning_rate": 0.0005264357338195077,
      "loss": 0.8246,
      "step": 5195
    },
    {
      "epoch": 2.3698302381223653,
      "grad_norm": 0.16771124303340912,
      "learning_rate": 0.0005259799453053783,
      "loss": 0.7703,
      "step": 5200
    },
    {
      "epoch": 2.372108921043637,
      "grad_norm": 0.17862628400325775,
      "learning_rate": 0.0005255241567912489,
      "loss": 0.8684,
      "step": 5205
    },
    {
      "epoch": 2.3743876039649083,
      "grad_norm": 0.16083042323589325,
      "learning_rate": 0.0005250683682771195,
      "loss": 0.9107,
      "step": 5210
    },
    {
      "epoch": 2.37666628688618,
      "grad_norm": 0.18387651443481445,
      "learning_rate": 0.00052461257976299,
      "loss": 0.8205,
      "step": 5215
    },
    {
      "epoch": 2.3789449698074514,
      "grad_norm": 0.1959141343832016,
      "learning_rate": 0.0005241567912488605,
      "loss": 0.904,
      "step": 5220
    },
    {
      "epoch": 2.381223652728723,
      "grad_norm": 0.1563892364501953,
      "learning_rate": 0.0005237010027347311,
      "loss": 0.868,
      "step": 5225
    },
    {
      "epoch": 2.3835023356499945,
      "grad_norm": 0.1692073494195938,
      "learning_rate": 0.0005232452142206016,
      "loss": 0.8576,
      "step": 5230
    },
    {
      "epoch": 2.385781018571266,
      "grad_norm": 0.16255955398082733,
      "learning_rate": 0.0005227894257064722,
      "loss": 0.9579,
      "step": 5235
    },
    {
      "epoch": 2.388059701492537,
      "grad_norm": 0.20386090874671936,
      "learning_rate": 0.0005223336371923428,
      "loss": 0.9151,
      "step": 5240
    },
    {
      "epoch": 2.3903383844138086,
      "grad_norm": 0.17010395228862762,
      "learning_rate": 0.0005218778486782133,
      "loss": 0.8464,
      "step": 5245
    },
    {
      "epoch": 2.39261706733508,
      "grad_norm": 0.17043021321296692,
      "learning_rate": 0.0005214220601640839,
      "loss": 0.8666,
      "step": 5250
    },
    {
      "epoch": 2.3948957502563517,
      "grad_norm": 0.15128971636295319,
      "learning_rate": 0.0005209662716499545,
      "loss": 0.9082,
      "step": 5255
    },
    {
      "epoch": 2.3971744331776232,
      "grad_norm": 0.16904184222221375,
      "learning_rate": 0.0005205104831358249,
      "loss": 0.896,
      "step": 5260
    },
    {
      "epoch": 2.399453116098895,
      "grad_norm": 0.17150168120861053,
      "learning_rate": 0.0005200546946216955,
      "loss": 0.8155,
      "step": 5265
    },
    {
      "epoch": 2.4017317990201663,
      "grad_norm": 0.1771530658006668,
      "learning_rate": 0.0005195989061075661,
      "loss": 0.8302,
      "step": 5270
    },
    {
      "epoch": 2.404010481941438,
      "grad_norm": 0.16021674871444702,
      "learning_rate": 0.0005191431175934367,
      "loss": 0.7999,
      "step": 5275
    },
    {
      "epoch": 2.4062891648627094,
      "grad_norm": 0.15659289062023163,
      "learning_rate": 0.0005186873290793072,
      "loss": 0.9318,
      "step": 5280
    },
    {
      "epoch": 2.408567847783981,
      "grad_norm": 0.1787186712026596,
      "learning_rate": 0.0005182315405651778,
      "loss": 0.9476,
      "step": 5285
    },
    {
      "epoch": 2.4108465307052525,
      "grad_norm": 0.168868288397789,
      "learning_rate": 0.0005177757520510483,
      "loss": 0.8699,
      "step": 5290
    },
    {
      "epoch": 2.413125213626524,
      "grad_norm": 0.1768791526556015,
      "learning_rate": 0.0005173199635369189,
      "loss": 0.9274,
      "step": 5295
    },
    {
      "epoch": 2.4154038965477955,
      "grad_norm": 0.1663104146718979,
      "learning_rate": 0.0005168641750227895,
      "loss": 0.8439,
      "step": 5300
    },
    {
      "epoch": 2.417682579469067,
      "grad_norm": 0.16244174540042877,
      "learning_rate": 0.0005164083865086599,
      "loss": 0.8623,
      "step": 5305
    },
    {
      "epoch": 2.4199612623903386,
      "grad_norm": 0.16833113133907318,
      "learning_rate": 0.0005159525979945305,
      "loss": 0.8765,
      "step": 5310
    },
    {
      "epoch": 2.4222399453116097,
      "grad_norm": 0.14904046058654785,
      "learning_rate": 0.0005154968094804012,
      "loss": 0.8956,
      "step": 5315
    },
    {
      "epoch": 2.4245186282328812,
      "grad_norm": 0.16924381256103516,
      "learning_rate": 0.0005150410209662717,
      "loss": 0.8789,
      "step": 5320
    },
    {
      "epoch": 2.4267973111541528,
      "grad_norm": 0.20043769478797913,
      "learning_rate": 0.0005145852324521422,
      "loss": 0.9086,
      "step": 5325
    },
    {
      "epoch": 2.4290759940754243,
      "grad_norm": 0.15734876692295074,
      "learning_rate": 0.0005141294439380128,
      "loss": 0.8959,
      "step": 5330
    },
    {
      "epoch": 2.431354676996696,
      "grad_norm": 0.2888908386230469,
      "learning_rate": 0.0005136736554238833,
      "loss": 0.8687,
      "step": 5335
    },
    {
      "epoch": 2.4336333599179674,
      "grad_norm": 0.19162105023860931,
      "learning_rate": 0.0005132178669097539,
      "loss": 0.8962,
      "step": 5340
    },
    {
      "epoch": 2.435912042839239,
      "grad_norm": 0.1539369523525238,
      "learning_rate": 0.0005127620783956244,
      "loss": 0.8567,
      "step": 5345
    },
    {
      "epoch": 2.4381907257605104,
      "grad_norm": 0.15984082221984863,
      "learning_rate": 0.0005123062898814949,
      "loss": 0.8179,
      "step": 5350
    },
    {
      "epoch": 2.440469408681782,
      "grad_norm": 0.16426977515220642,
      "learning_rate": 0.0005118505013673656,
      "loss": 0.8412,
      "step": 5355
    },
    {
      "epoch": 2.4427480916030535,
      "grad_norm": 0.17793114483356476,
      "learning_rate": 0.0005113947128532362,
      "loss": 0.8791,
      "step": 5360
    },
    {
      "epoch": 2.445026774524325,
      "grad_norm": 0.1571628600358963,
      "learning_rate": 0.0005109389243391067,
      "loss": 0.8182,
      "step": 5365
    },
    {
      "epoch": 2.4473054574455966,
      "grad_norm": 0.16723427176475525,
      "learning_rate": 0.0005104831358249772,
      "loss": 0.8953,
      "step": 5370
    },
    {
      "epoch": 2.449584140366868,
      "grad_norm": 0.16477297246456146,
      "learning_rate": 0.0005100273473108478,
      "loss": 0.879,
      "step": 5375
    },
    {
      "epoch": 2.4518628232881396,
      "grad_norm": 0.1732284277677536,
      "learning_rate": 0.0005095715587967183,
      "loss": 0.876,
      "step": 5380
    },
    {
      "epoch": 2.454141506209411,
      "grad_norm": 0.20835743844509125,
      "learning_rate": 0.0005091157702825889,
      "loss": 0.8805,
      "step": 5385
    },
    {
      "epoch": 2.4564201891306823,
      "grad_norm": 0.18861909210681915,
      "learning_rate": 0.0005086599817684595,
      "loss": 0.7923,
      "step": 5390
    },
    {
      "epoch": 2.458698872051954,
      "grad_norm": 0.1856495887041092,
      "learning_rate": 0.00050820419325433,
      "loss": 0.8646,
      "step": 5395
    },
    {
      "epoch": 2.4609775549732253,
      "grad_norm": 0.1751953512430191,
      "learning_rate": 0.0005077484047402006,
      "loss": 0.8301,
      "step": 5400
    },
    {
      "epoch": 2.463256237894497,
      "grad_norm": 0.17087003588676453,
      "learning_rate": 0.0005072926162260712,
      "loss": 0.8162,
      "step": 5405
    },
    {
      "epoch": 2.4655349208157684,
      "grad_norm": 0.16652072966098785,
      "learning_rate": 0.0005068368277119416,
      "loss": 0.8542,
      "step": 5410
    },
    {
      "epoch": 2.46781360373704,
      "grad_norm": 0.19010858237743378,
      "learning_rate": 0.0005063810391978122,
      "loss": 0.9227,
      "step": 5415
    },
    {
      "epoch": 2.4700922866583115,
      "grad_norm": 0.19730021059513092,
      "learning_rate": 0.0005059252506836827,
      "loss": 0.9402,
      "step": 5420
    },
    {
      "epoch": 2.472370969579583,
      "grad_norm": 0.1531418412923813,
      "learning_rate": 0.0005054694621695533,
      "loss": 0.864,
      "step": 5425
    },
    {
      "epoch": 2.4746496525008546,
      "grad_norm": 0.17673714458942413,
      "learning_rate": 0.000505013673655424,
      "loss": 0.9233,
      "step": 5430
    },
    {
      "epoch": 2.476928335422126,
      "grad_norm": 0.17439110577106476,
      "learning_rate": 0.0005045578851412944,
      "loss": 0.9027,
      "step": 5435
    },
    {
      "epoch": 2.4792070183433976,
      "grad_norm": 0.17638450860977173,
      "learning_rate": 0.000504102096627165,
      "loss": 0.9139,
      "step": 5440
    },
    {
      "epoch": 2.481485701264669,
      "grad_norm": 0.1657152771949768,
      "learning_rate": 0.0005036463081130356,
      "loss": 0.816,
      "step": 5445
    },
    {
      "epoch": 2.4837643841859407,
      "grad_norm": 0.1870488077402115,
      "learning_rate": 0.0005031905195989061,
      "loss": 0.8698,
      "step": 5450
    },
    {
      "epoch": 2.486043067107212,
      "grad_norm": 0.18453526496887207,
      "learning_rate": 0.0005027347310847766,
      "loss": 0.8099,
      "step": 5455
    },
    {
      "epoch": 2.4883217500284838,
      "grad_norm": 0.16091476380825043,
      "learning_rate": 0.0005022789425706472,
      "loss": 0.8079,
      "step": 5460
    },
    {
      "epoch": 2.490600432949755,
      "grad_norm": 0.21073992550373077,
      "learning_rate": 0.0005018231540565177,
      "loss": 0.856,
      "step": 5465
    },
    {
      "epoch": 2.4928791158710264,
      "grad_norm": 0.17468470335006714,
      "learning_rate": 0.0005013673655423884,
      "loss": 0.8916,
      "step": 5470
    },
    {
      "epoch": 2.495157798792298,
      "grad_norm": 0.15710023045539856,
      "learning_rate": 0.0005009115770282589,
      "loss": 0.8984,
      "step": 5475
    },
    {
      "epoch": 2.4974364817135695,
      "grad_norm": 0.16861127316951752,
      "learning_rate": 0.0005004557885141294,
      "loss": 0.9002,
      "step": 5480
    },
    {
      "epoch": 2.499715164634841,
      "grad_norm": 0.1723538041114807,
      "learning_rate": 0.0005,
      "loss": 0.7865,
      "step": 5485
    },
    {
      "epoch": 2.5019938475561125,
      "grad_norm": 0.15879224240779877,
      "learning_rate": 0.0004995442114858706,
      "loss": 0.8286,
      "step": 5490
    },
    {
      "epoch": 2.504272530477384,
      "grad_norm": 0.18688710033893585,
      "learning_rate": 0.0004990884229717411,
      "loss": 0.903,
      "step": 5495
    },
    {
      "epoch": 2.5065512133986556,
      "grad_norm": 0.15713447332382202,
      "learning_rate": 0.0004986326344576117,
      "loss": 0.8033,
      "step": 5500
    },
    {
      "epoch": 2.508829896319927,
      "grad_norm": 0.16202066838741302,
      "learning_rate": 0.0004981768459434823,
      "loss": 0.893,
      "step": 5505
    },
    {
      "epoch": 2.5111085792411987,
      "grad_norm": 0.17544612288475037,
      "learning_rate": 0.0004977210574293528,
      "loss": 0.8393,
      "step": 5510
    },
    {
      "epoch": 2.51338726216247,
      "grad_norm": 0.15612761676311493,
      "learning_rate": 0.0004972652689152233,
      "loss": 0.8579,
      "step": 5515
    },
    {
      "epoch": 2.5156659450837418,
      "grad_norm": 0.1626867800951004,
      "learning_rate": 0.0004968094804010939,
      "loss": 0.9178,
      "step": 5520
    },
    {
      "epoch": 2.5179446280050133,
      "grad_norm": 0.1674978882074356,
      "learning_rate": 0.0004963536918869644,
      "loss": 0.8605,
      "step": 5525
    },
    {
      "epoch": 2.5202233109262844,
      "grad_norm": 0.16769374907016754,
      "learning_rate": 0.000495897903372835,
      "loss": 0.8748,
      "step": 5530
    },
    {
      "epoch": 2.5225019938475564,
      "grad_norm": 0.20011261105537415,
      "learning_rate": 0.0004954421148587056,
      "loss": 0.8764,
      "step": 5535
    },
    {
      "epoch": 2.5247806767688274,
      "grad_norm": 0.16570132970809937,
      "learning_rate": 0.0004949863263445761,
      "loss": 0.8909,
      "step": 5540
    },
    {
      "epoch": 2.527059359690099,
      "grad_norm": 0.17338639497756958,
      "learning_rate": 0.0004945305378304467,
      "loss": 0.8654,
      "step": 5545
    },
    {
      "epoch": 2.5293380426113705,
      "grad_norm": 0.17712652683258057,
      "learning_rate": 0.0004940747493163173,
      "loss": 0.8546,
      "step": 5550
    },
    {
      "epoch": 2.531616725532642,
      "grad_norm": 0.17984719574451447,
      "learning_rate": 0.0004936189608021878,
      "loss": 0.8834,
      "step": 5555
    },
    {
      "epoch": 2.5338954084539136,
      "grad_norm": 0.1498163640499115,
      "learning_rate": 0.0004931631722880583,
      "loss": 0.8449,
      "step": 5560
    },
    {
      "epoch": 2.536174091375185,
      "grad_norm": 0.1772141307592392,
      "learning_rate": 0.000492707383773929,
      "loss": 0.85,
      "step": 5565
    },
    {
      "epoch": 2.5384527742964567,
      "grad_norm": 0.1718757003545761,
      "learning_rate": 0.0004922515952597994,
      "loss": 0.8537,
      "step": 5570
    },
    {
      "epoch": 2.540731457217728,
      "grad_norm": 0.17159710824489594,
      "learning_rate": 0.00049179580674567,
      "loss": 0.9049,
      "step": 5575
    },
    {
      "epoch": 2.5430101401389997,
      "grad_norm": 0.16595697402954102,
      "learning_rate": 0.0004913400182315406,
      "loss": 0.8318,
      "step": 5580
    },
    {
      "epoch": 2.5452888230602713,
      "grad_norm": 0.15106014907360077,
      "learning_rate": 0.0004908842297174111,
      "loss": 0.8541,
      "step": 5585
    },
    {
      "epoch": 2.547567505981543,
      "grad_norm": 0.1525922417640686,
      "learning_rate": 0.0004904284412032817,
      "loss": 0.8582,
      "step": 5590
    },
    {
      "epoch": 2.5498461889028143,
      "grad_norm": 0.16300790011882782,
      "learning_rate": 0.0004899726526891523,
      "loss": 0.8721,
      "step": 5595
    },
    {
      "epoch": 2.552124871824086,
      "grad_norm": 0.1787836253643036,
      "learning_rate": 0.0004895168641750227,
      "loss": 0.8571,
      "step": 5600
    },
    {
      "epoch": 2.554403554745357,
      "grad_norm": 0.187198668718338,
      "learning_rate": 0.0004890610756608934,
      "loss": 0.8918,
      "step": 5605
    },
    {
      "epoch": 2.556682237666629,
      "grad_norm": 0.16850218176841736,
      "learning_rate": 0.0004886052871467639,
      "loss": 0.8216,
      "step": 5610
    },
    {
      "epoch": 2.5589609205879,
      "grad_norm": 0.17197003960609436,
      "learning_rate": 0.0004881494986326345,
      "loss": 0.8233,
      "step": 5615
    },
    {
      "epoch": 2.5612396035091716,
      "grad_norm": 0.16301792860031128,
      "learning_rate": 0.000487693710118505,
      "loss": 0.8727,
      "step": 5620
    },
    {
      "epoch": 2.563518286430443,
      "grad_norm": 0.16995175182819366,
      "learning_rate": 0.00048723792160437556,
      "loss": 0.8887,
      "step": 5625
    },
    {
      "epoch": 2.5657969693517146,
      "grad_norm": 0.1493826061487198,
      "learning_rate": 0.0004867821330902462,
      "loss": 0.8473,
      "step": 5630
    },
    {
      "epoch": 2.568075652272986,
      "grad_norm": 0.17759540677070618,
      "learning_rate": 0.0004863263445761167,
      "loss": 0.8827,
      "step": 5635
    },
    {
      "epoch": 2.5703543351942577,
      "grad_norm": 0.20876948535442352,
      "learning_rate": 0.00048587055606198726,
      "loss": 0.8476,
      "step": 5640
    },
    {
      "epoch": 2.5726330181155292,
      "grad_norm": 0.16441580653190613,
      "learning_rate": 0.0004854147675478578,
      "loss": 0.8403,
      "step": 5645
    },
    {
      "epoch": 2.574911701036801,
      "grad_norm": 0.1614457070827484,
      "learning_rate": 0.0004849589790337284,
      "loss": 0.8155,
      "step": 5650
    },
    {
      "epoch": 2.5771903839580723,
      "grad_norm": 0.154181569814682,
      "learning_rate": 0.0004845031905195989,
      "loss": 0.8611,
      "step": 5655
    },
    {
      "epoch": 2.579469066879344,
      "grad_norm": 0.1979924440383911,
      "learning_rate": 0.0004840474020054695,
      "loss": 0.8595,
      "step": 5660
    },
    {
      "epoch": 2.5817477498006154,
      "grad_norm": 0.1844630390405655,
      "learning_rate": 0.00048359161349134,
      "loss": 0.8858,
      "step": 5665
    },
    {
      "epoch": 2.5840264327218865,
      "grad_norm": 0.1557920128107071,
      "learning_rate": 0.0004831358249772106,
      "loss": 0.8454,
      "step": 5670
    },
    {
      "epoch": 2.5863051156431585,
      "grad_norm": 0.20742101967334747,
      "learning_rate": 0.0004826800364630811,
      "loss": 0.9022,
      "step": 5675
    },
    {
      "epoch": 2.5885837985644296,
      "grad_norm": 0.173944354057312,
      "learning_rate": 0.0004822242479489517,
      "loss": 0.8246,
      "step": 5680
    },
    {
      "epoch": 2.5908624814857015,
      "grad_norm": 0.1688641756772995,
      "learning_rate": 0.0004817684594348222,
      "loss": 0.8847,
      "step": 5685
    },
    {
      "epoch": 2.5931411644069726,
      "grad_norm": 0.1720995455980301,
      "learning_rate": 0.0004813126709206928,
      "loss": 0.8331,
      "step": 5690
    },
    {
      "epoch": 2.595419847328244,
      "grad_norm": 0.18185409903526306,
      "learning_rate": 0.0004808568824065634,
      "loss": 0.8604,
      "step": 5695
    },
    {
      "epoch": 2.5976985302495157,
      "grad_norm": 0.16210442781448364,
      "learning_rate": 0.0004804010938924339,
      "loss": 0.8989,
      "step": 5700
    },
    {
      "epoch": 2.5999772131707872,
      "grad_norm": 0.15564478933811188,
      "learning_rate": 0.00047994530537830447,
      "loss": 0.829,
      "step": 5705
    },
    {
      "epoch": 2.6022558960920588,
      "grad_norm": 0.18439176678657532,
      "learning_rate": 0.00047948951686417504,
      "loss": 0.9113,
      "step": 5710
    },
    {
      "epoch": 2.6045345790133303,
      "grad_norm": 0.18079765141010284,
      "learning_rate": 0.0004790337283500456,
      "loss": 0.8614,
      "step": 5715
    },
    {
      "epoch": 2.606813261934602,
      "grad_norm": 0.1666010171175003,
      "learning_rate": 0.0004785779398359161,
      "loss": 0.8032,
      "step": 5720
    },
    {
      "epoch": 2.6090919448558734,
      "grad_norm": 0.16662852466106415,
      "learning_rate": 0.00047812215132178674,
      "loss": 0.7929,
      "step": 5725
    },
    {
      "epoch": 2.611370627777145,
      "grad_norm": 0.16821521520614624,
      "learning_rate": 0.00047766636280765725,
      "loss": 0.8687,
      "step": 5730
    },
    {
      "epoch": 2.6136493106984164,
      "grad_norm": 0.1757127046585083,
      "learning_rate": 0.0004772105742935278,
      "loss": 0.8513,
      "step": 5735
    },
    {
      "epoch": 2.615927993619688,
      "grad_norm": 0.14896422624588013,
      "learning_rate": 0.00047675478577939833,
      "loss": 0.8887,
      "step": 5740
    },
    {
      "epoch": 2.618206676540959,
      "grad_norm": 0.17836593091487885,
      "learning_rate": 0.00047629899726526895,
      "loss": 0.8448,
      "step": 5745
    },
    {
      "epoch": 2.620485359462231,
      "grad_norm": 0.18394209444522858,
      "learning_rate": 0.00047584320875113947,
      "loss": 0.8114,
      "step": 5750
    },
    {
      "epoch": 2.622764042383502,
      "grad_norm": 0.16283968091011047,
      "learning_rate": 0.00047538742023701003,
      "loss": 0.8503,
      "step": 5755
    },
    {
      "epoch": 2.625042725304774,
      "grad_norm": 0.1649586707353592,
      "learning_rate": 0.0004749316317228806,
      "loss": 0.9032,
      "step": 5760
    },
    {
      "epoch": 2.627321408226045,
      "grad_norm": 0.16726933419704437,
      "learning_rate": 0.00047447584320875117,
      "loss": 0.8363,
      "step": 5765
    },
    {
      "epoch": 2.6296000911473167,
      "grad_norm": 0.15561071038246155,
      "learning_rate": 0.00047402005469462173,
      "loss": 0.8377,
      "step": 5770
    },
    {
      "epoch": 2.6318787740685883,
      "grad_norm": 0.16558541357517242,
      "learning_rate": 0.00047356426618049225,
      "loss": 0.8909,
      "step": 5775
    },
    {
      "epoch": 2.63415745698986,
      "grad_norm": 0.16311116516590118,
      "learning_rate": 0.0004731084776663628,
      "loss": 0.8632,
      "step": 5780
    },
    {
      "epoch": 2.6364361399111313,
      "grad_norm": 0.1742231547832489,
      "learning_rate": 0.0004726526891522334,
      "loss": 0.883,
      "step": 5785
    },
    {
      "epoch": 2.638714822832403,
      "grad_norm": 0.17087870836257935,
      "learning_rate": 0.00047219690063810395,
      "loss": 0.9098,
      "step": 5790
    },
    {
      "epoch": 2.6409935057536744,
      "grad_norm": 0.17152641713619232,
      "learning_rate": 0.00047174111212397446,
      "loss": 0.8368,
      "step": 5795
    },
    {
      "epoch": 2.643272188674946,
      "grad_norm": 0.17821350693702698,
      "learning_rate": 0.00047128532360984503,
      "loss": 0.8689,
      "step": 5800
    },
    {
      "epoch": 2.6455508715962175,
      "grad_norm": 0.16594406962394714,
      "learning_rate": 0.0004708295350957156,
      "loss": 0.801,
      "step": 5805
    },
    {
      "epoch": 2.647829554517489,
      "grad_norm": 0.1749199479818344,
      "learning_rate": 0.00047037374658158616,
      "loss": 0.8921,
      "step": 5810
    },
    {
      "epoch": 2.6501082374387606,
      "grad_norm": 0.16878943145275116,
      "learning_rate": 0.0004699179580674567,
      "loss": 0.8418,
      "step": 5815
    },
    {
      "epoch": 2.6523869203600317,
      "grad_norm": 0.1637088656425476,
      "learning_rate": 0.00046946216955332724,
      "loss": 0.932,
      "step": 5820
    },
    {
      "epoch": 2.6546656032813036,
      "grad_norm": 0.17245620489120483,
      "learning_rate": 0.00046900638103919786,
      "loss": 0.8512,
      "step": 5825
    },
    {
      "epoch": 2.6569442862025747,
      "grad_norm": 0.1904492825269699,
      "learning_rate": 0.0004685505925250684,
      "loss": 0.8379,
      "step": 5830
    },
    {
      "epoch": 2.6592229691238463,
      "grad_norm": 0.17859475314617157,
      "learning_rate": 0.00046809480401093894,
      "loss": 0.8445,
      "step": 5835
    },
    {
      "epoch": 2.661501652045118,
      "grad_norm": 0.1431664228439331,
      "learning_rate": 0.0004676390154968095,
      "loss": 0.8213,
      "step": 5840
    },
    {
      "epoch": 2.6637803349663893,
      "grad_norm": 0.17555345594882965,
      "learning_rate": 0.0004671832269826801,
      "loss": 0.9021,
      "step": 5845
    },
    {
      "epoch": 2.666059017887661,
      "grad_norm": 0.17271862924098969,
      "learning_rate": 0.0004667274384685506,
      "loss": 0.845,
      "step": 5850
    },
    {
      "epoch": 2.6683377008089324,
      "grad_norm": 0.1615799367427826,
      "learning_rate": 0.00046627164995442116,
      "loss": 0.9052,
      "step": 5855
    },
    {
      "epoch": 2.670616383730204,
      "grad_norm": 0.16159303486347198,
      "learning_rate": 0.0004658158614402917,
      "loss": 0.8511,
      "step": 5860
    },
    {
      "epoch": 2.6728950666514755,
      "grad_norm": 0.17571845650672913,
      "learning_rate": 0.0004653600729261623,
      "loss": 0.8036,
      "step": 5865
    },
    {
      "epoch": 2.675173749572747,
      "grad_norm": 0.196051687002182,
      "learning_rate": 0.0004649042844120328,
      "loss": 0.8748,
      "step": 5870
    },
    {
      "epoch": 2.6774524324940185,
      "grad_norm": 0.154865562915802,
      "learning_rate": 0.00046444849589790337,
      "loss": 0.7902,
      "step": 5875
    },
    {
      "epoch": 2.67973111541529,
      "grad_norm": 0.18565668165683746,
      "learning_rate": 0.00046399270738377394,
      "loss": 0.8497,
      "step": 5880
    },
    {
      "epoch": 2.6820097983365616,
      "grad_norm": 0.1770063191652298,
      "learning_rate": 0.0004635369188696445,
      "loss": 0.8794,
      "step": 5885
    },
    {
      "epoch": 2.684288481257833,
      "grad_norm": 0.17540746927261353,
      "learning_rate": 0.000463081130355515,
      "loss": 0.9222,
      "step": 5890
    },
    {
      "epoch": 2.6865671641791042,
      "grad_norm": 0.18140588700771332,
      "learning_rate": 0.0004626253418413856,
      "loss": 0.8314,
      "step": 5895
    },
    {
      "epoch": 2.688845847100376,
      "grad_norm": 0.18725313246250153,
      "learning_rate": 0.0004621695533272562,
      "loss": 0.8269,
      "step": 5900
    },
    {
      "epoch": 2.6911245300216473,
      "grad_norm": 0.1511690467596054,
      "learning_rate": 0.0004617137648131267,
      "loss": 0.7959,
      "step": 5905
    },
    {
      "epoch": 2.693403212942919,
      "grad_norm": 0.1635943055152893,
      "learning_rate": 0.0004612579762989973,
      "loss": 0.7946,
      "step": 5910
    },
    {
      "epoch": 2.6956818958641904,
      "grad_norm": 0.18631048500537872,
      "learning_rate": 0.0004608021877848678,
      "loss": 0.8932,
      "step": 5915
    },
    {
      "epoch": 2.697960578785462,
      "grad_norm": 0.1763763278722763,
      "learning_rate": 0.0004603463992707384,
      "loss": 0.8012,
      "step": 5920
    },
    {
      "epoch": 2.7002392617067335,
      "grad_norm": 0.15971915423870087,
      "learning_rate": 0.00045989061075660893,
      "loss": 0.8384,
      "step": 5925
    },
    {
      "epoch": 2.702517944628005,
      "grad_norm": 0.1726962924003601,
      "learning_rate": 0.0004594348222424795,
      "loss": 0.8026,
      "step": 5930
    },
    {
      "epoch": 2.7047966275492765,
      "grad_norm": 0.1782129555940628,
      "learning_rate": 0.00045897903372835,
      "loss": 0.8717,
      "step": 5935
    },
    {
      "epoch": 2.707075310470548,
      "grad_norm": 0.18406568467617035,
      "learning_rate": 0.00045852324521422063,
      "loss": 0.8667,
      "step": 5940
    },
    {
      "epoch": 2.7093539933918196,
      "grad_norm": 0.16638538241386414,
      "learning_rate": 0.00045806745670009115,
      "loss": 0.783,
      "step": 5945
    },
    {
      "epoch": 2.711632676313091,
      "grad_norm": 0.17255151271820068,
      "learning_rate": 0.0004576116681859617,
      "loss": 0.8137,
      "step": 5950
    },
    {
      "epoch": 2.7139113592343627,
      "grad_norm": 0.1473204344511032,
      "learning_rate": 0.0004571558796718322,
      "loss": 0.8363,
      "step": 5955
    },
    {
      "epoch": 2.716190042155634,
      "grad_norm": 0.17310091853141785,
      "learning_rate": 0.00045670009115770285,
      "loss": 0.8801,
      "step": 5960
    },
    {
      "epoch": 2.7184687250769057,
      "grad_norm": 0.17194075882434845,
      "learning_rate": 0.0004562443026435734,
      "loss": 0.8502,
      "step": 5965
    },
    {
      "epoch": 2.720747407998177,
      "grad_norm": 0.17561675608158112,
      "learning_rate": 0.00045578851412944393,
      "loss": 0.816,
      "step": 5970
    },
    {
      "epoch": 2.723026090919449,
      "grad_norm": 0.15405936539173126,
      "learning_rate": 0.00045533272561531455,
      "loss": 0.8143,
      "step": 5975
    },
    {
      "epoch": 2.72530477384072,
      "grad_norm": 0.18109099566936493,
      "learning_rate": 0.00045487693710118506,
      "loss": 0.8523,
      "step": 5980
    },
    {
      "epoch": 2.7275834567619914,
      "grad_norm": 0.16941308975219727,
      "learning_rate": 0.00045442114858705563,
      "loss": 0.8445,
      "step": 5985
    },
    {
      "epoch": 2.729862139683263,
      "grad_norm": 0.15534475445747375,
      "learning_rate": 0.00045396536007292614,
      "loss": 0.7651,
      "step": 5990
    },
    {
      "epoch": 2.7321408226045345,
      "grad_norm": 0.1870289295911789,
      "learning_rate": 0.00045350957155879676,
      "loss": 0.9021,
      "step": 5995
    },
    {
      "epoch": 2.734419505525806,
      "grad_norm": 0.18428045511245728,
      "learning_rate": 0.0004530537830446673,
      "loss": 0.8386,
      "step": 6000
    },
    {
      "epoch": 2.7366981884470776,
      "grad_norm": 0.1775941550731659,
      "learning_rate": 0.00045259799453053784,
      "loss": 0.8206,
      "step": 6005
    },
    {
      "epoch": 2.738976871368349,
      "grad_norm": 0.17400223016738892,
      "learning_rate": 0.00045214220601640836,
      "loss": 0.8353,
      "step": 6010
    },
    {
      "epoch": 2.7412555542896206,
      "grad_norm": 0.1575925648212433,
      "learning_rate": 0.000451686417502279,
      "loss": 0.7905,
      "step": 6015
    },
    {
      "epoch": 2.743534237210892,
      "grad_norm": 0.18783967196941376,
      "learning_rate": 0.0004512306289881495,
      "loss": 0.8279,
      "step": 6020
    },
    {
      "epoch": 2.7458129201321637,
      "grad_norm": 0.16594889760017395,
      "learning_rate": 0.00045077484047402006,
      "loss": 0.8458,
      "step": 6025
    },
    {
      "epoch": 2.7480916030534353,
      "grad_norm": 0.18554775416851044,
      "learning_rate": 0.0004503190519598906,
      "loss": 0.8736,
      "step": 6030
    },
    {
      "epoch": 2.750370285974707,
      "grad_norm": 0.18419231474399567,
      "learning_rate": 0.0004498632634457612,
      "loss": 0.8344,
      "step": 6035
    },
    {
      "epoch": 2.7526489688959783,
      "grad_norm": 0.17348957061767578,
      "learning_rate": 0.00044940747493163176,
      "loss": 0.8053,
      "step": 6040
    },
    {
      "epoch": 2.7549276518172494,
      "grad_norm": 0.2056141346693039,
      "learning_rate": 0.00044895168641750227,
      "loss": 0.8396,
      "step": 6045
    },
    {
      "epoch": 2.7572063347385214,
      "grad_norm": 0.16914640367031097,
      "learning_rate": 0.00044849589790337284,
      "loss": 0.8818,
      "step": 6050
    },
    {
      "epoch": 2.7594850176597925,
      "grad_norm": 0.16415922343730927,
      "learning_rate": 0.0004480401093892434,
      "loss": 0.8215,
      "step": 6055
    },
    {
      "epoch": 2.761763700581064,
      "grad_norm": 0.18955476582050323,
      "learning_rate": 0.00044758432087511397,
      "loss": 0.8605,
      "step": 6060
    },
    {
      "epoch": 2.7640423835023356,
      "grad_norm": 0.17876599729061127,
      "learning_rate": 0.0004471285323609845,
      "loss": 0.8791,
      "step": 6065
    },
    {
      "epoch": 2.766321066423607,
      "grad_norm": 0.19002869725227356,
      "learning_rate": 0.0004466727438468551,
      "loss": 0.8346,
      "step": 6070
    },
    {
      "epoch": 2.7685997493448786,
      "grad_norm": 0.1614873856306076,
      "learning_rate": 0.0004462169553327256,
      "loss": 0.823,
      "step": 6075
    },
    {
      "epoch": 2.77087843226615,
      "grad_norm": 0.1971643716096878,
      "learning_rate": 0.0004457611668185962,
      "loss": 0.8901,
      "step": 6080
    },
    {
      "epoch": 2.7731571151874217,
      "grad_norm": 0.1768244057893753,
      "learning_rate": 0.0004453053783044667,
      "loss": 0.8652,
      "step": 6085
    },
    {
      "epoch": 2.7754357981086932,
      "grad_norm": 0.1786467581987381,
      "learning_rate": 0.0004448495897903373,
      "loss": 0.8078,
      "step": 6090
    },
    {
      "epoch": 2.7777144810299648,
      "grad_norm": 0.16761501133441925,
      "learning_rate": 0.0004443938012762079,
      "loss": 0.831,
      "step": 6095
    },
    {
      "epoch": 2.7799931639512363,
      "grad_norm": 0.17251057922840118,
      "learning_rate": 0.0004439380127620784,
      "loss": 0.7599,
      "step": 6100
    },
    {
      "epoch": 2.782271846872508,
      "grad_norm": 0.18803241848945618,
      "learning_rate": 0.00044348222424794897,
      "loss": 0.8703,
      "step": 6105
    },
    {
      "epoch": 2.784550529793779,
      "grad_norm": 0.17286670207977295,
      "learning_rate": 0.00044302643573381953,
      "loss": 0.9327,
      "step": 6110
    },
    {
      "epoch": 2.786829212715051,
      "grad_norm": 0.18098905682563782,
      "learning_rate": 0.0004425706472196901,
      "loss": 0.8201,
      "step": 6115
    },
    {
      "epoch": 2.789107895636322,
      "grad_norm": 0.18954072892665863,
      "learning_rate": 0.0004421148587055606,
      "loss": 0.8841,
      "step": 6120
    },
    {
      "epoch": 2.791386578557594,
      "grad_norm": 0.16803675889968872,
      "learning_rate": 0.0004416590701914312,
      "loss": 0.8598,
      "step": 6125
    },
    {
      "epoch": 2.793665261478865,
      "grad_norm": 0.18732038140296936,
      "learning_rate": 0.00044120328167730175,
      "loss": 0.8074,
      "step": 6130
    },
    {
      "epoch": 2.7959439444001366,
      "grad_norm": 0.1799907237291336,
      "learning_rate": 0.0004407474931631723,
      "loss": 0.8489,
      "step": 6135
    },
    {
      "epoch": 2.798222627321408,
      "grad_norm": 0.17672206461429596,
      "learning_rate": 0.0004402917046490428,
      "loss": 0.8571,
      "step": 6140
    },
    {
      "epoch": 2.8005013102426797,
      "grad_norm": 0.16010257601737976,
      "learning_rate": 0.0004398359161349134,
      "loss": 0.856,
      "step": 6145
    },
    {
      "epoch": 2.802779993163951,
      "grad_norm": 0.1724633276462555,
      "learning_rate": 0.00043938012762078396,
      "loss": 0.8262,
      "step": 6150
    },
    {
      "epoch": 2.8050586760852227,
      "grad_norm": 0.20359134674072266,
      "learning_rate": 0.00043892433910665453,
      "loss": 0.9445,
      "step": 6155
    },
    {
      "epoch": 2.8073373590064943,
      "grad_norm": 0.17832736670970917,
      "learning_rate": 0.00043846855059252504,
      "loss": 0.8569,
      "step": 6160
    },
    {
      "epoch": 2.809616041927766,
      "grad_norm": 0.17010502517223358,
      "learning_rate": 0.0004380127620783956,
      "loss": 0.7793,
      "step": 6165
    },
    {
      "epoch": 2.8118947248490374,
      "grad_norm": 0.16850754618644714,
      "learning_rate": 0.00043755697356426623,
      "loss": 0.841,
      "step": 6170
    },
    {
      "epoch": 2.814173407770309,
      "grad_norm": 0.18616923689842224,
      "learning_rate": 0.00043710118505013674,
      "loss": 0.8137,
      "step": 6175
    },
    {
      "epoch": 2.8164520906915804,
      "grad_norm": 0.15551060438156128,
      "learning_rate": 0.0004366453965360073,
      "loss": 0.779,
      "step": 6180
    },
    {
      "epoch": 2.8187307736128515,
      "grad_norm": 0.1506740301847458,
      "learning_rate": 0.0004361896080218778,
      "loss": 0.8722,
      "step": 6185
    },
    {
      "epoch": 2.8210094565341235,
      "grad_norm": 0.17994122207164764,
      "learning_rate": 0.00043573381950774844,
      "loss": 0.8249,
      "step": 6190
    },
    {
      "epoch": 2.8232881394553946,
      "grad_norm": 0.20062515139579773,
      "learning_rate": 0.00043527803099361896,
      "loss": 0.8684,
      "step": 6195
    },
    {
      "epoch": 2.825566822376666,
      "grad_norm": 0.16688662767410278,
      "learning_rate": 0.0004348222424794895,
      "loss": 0.8801,
      "step": 6200
    },
    {
      "epoch": 2.8278455052979377,
      "grad_norm": 0.19220282137393951,
      "learning_rate": 0.0004343664539653601,
      "loss": 0.8452,
      "step": 6205
    },
    {
      "epoch": 2.830124188219209,
      "grad_norm": 0.18624351918697357,
      "learning_rate": 0.00043391066545123066,
      "loss": 0.8581,
      "step": 6210
    },
    {
      "epoch": 2.8324028711404807,
      "grad_norm": 0.19454415142536163,
      "learning_rate": 0.00043345487693710117,
      "loss": 0.925,
      "step": 6215
    },
    {
      "epoch": 2.8346815540617523,
      "grad_norm": 0.1761370748281479,
      "learning_rate": 0.00043299908842297174,
      "loss": 0.8734,
      "step": 6220
    },
    {
      "epoch": 2.836960236983024,
      "grad_norm": 0.1852719783782959,
      "learning_rate": 0.0004325432999088423,
      "loss": 0.8113,
      "step": 6225
    },
    {
      "epoch": 2.8392389199042953,
      "grad_norm": 0.16927491128444672,
      "learning_rate": 0.00043208751139471287,
      "loss": 0.8788,
      "step": 6230
    },
    {
      "epoch": 2.841517602825567,
      "grad_norm": 0.18596765398979187,
      "learning_rate": 0.00043163172288058344,
      "loss": 0.8484,
      "step": 6235
    },
    {
      "epoch": 2.8437962857468384,
      "grad_norm": 0.1723233312368393,
      "learning_rate": 0.00043117593436645395,
      "loss": 0.7459,
      "step": 6240
    },
    {
      "epoch": 2.84607496866811,
      "grad_norm": 0.19720837473869324,
      "learning_rate": 0.00043072014585232457,
      "loss": 0.838,
      "step": 6245
    },
    {
      "epoch": 2.8483536515893815,
      "grad_norm": 0.18465320765972137,
      "learning_rate": 0.0004302643573381951,
      "loss": 0.8411,
      "step": 6250
    },
    {
      "epoch": 2.850632334510653,
      "grad_norm": 0.16500990092754364,
      "learning_rate": 0.00042980856882406565,
      "loss": 0.8419,
      "step": 6255
    },
    {
      "epoch": 2.852911017431924,
      "grad_norm": 0.19199632108211517,
      "learning_rate": 0.00042935278030993616,
      "loss": 0.8302,
      "step": 6260
    },
    {
      "epoch": 2.855189700353196,
      "grad_norm": 0.16904693841934204,
      "learning_rate": 0.0004288969917958068,
      "loss": 0.8475,
      "step": 6265
    },
    {
      "epoch": 2.857468383274467,
      "grad_norm": 0.1623208224773407,
      "learning_rate": 0.0004284412032816773,
      "loss": 0.8084,
      "step": 6270
    },
    {
      "epoch": 2.8597470661957387,
      "grad_norm": 0.16605374217033386,
      "learning_rate": 0.00042798541476754787,
      "loss": 0.8101,
      "step": 6275
    },
    {
      "epoch": 2.8620257491170102,
      "grad_norm": 0.1704942286014557,
      "learning_rate": 0.0004275296262534184,
      "loss": 0.8437,
      "step": 6280
    },
    {
      "epoch": 2.864304432038282,
      "grad_norm": 0.1497839391231537,
      "learning_rate": 0.000427073837739289,
      "loss": 0.8522,
      "step": 6285
    },
    {
      "epoch": 2.8665831149595533,
      "grad_norm": 0.1763232797384262,
      "learning_rate": 0.0004266180492251595,
      "loss": 0.8416,
      "step": 6290
    },
    {
      "epoch": 2.868861797880825,
      "grad_norm": 0.19564968347549438,
      "learning_rate": 0.0004261622607110301,
      "loss": 0.8338,
      "step": 6295
    },
    {
      "epoch": 2.8711404808020964,
      "grad_norm": 0.18313436210155487,
      "learning_rate": 0.0004257064721969007,
      "loss": 0.9072,
      "step": 6300
    },
    {
      "epoch": 2.873419163723368,
      "grad_norm": 0.16996125876903534,
      "learning_rate": 0.0004252506836827712,
      "loss": 0.8302,
      "step": 6305
    },
    {
      "epoch": 2.8756978466446395,
      "grad_norm": 0.17365510761737823,
      "learning_rate": 0.0004247948951686418,
      "loss": 0.8418,
      "step": 6310
    },
    {
      "epoch": 2.877976529565911,
      "grad_norm": 0.1682317852973938,
      "learning_rate": 0.0004243391066545123,
      "loss": 0.8403,
      "step": 6315
    },
    {
      "epoch": 2.8802552124871825,
      "grad_norm": 0.1673637330532074,
      "learning_rate": 0.0004238833181403829,
      "loss": 0.9151,
      "step": 6320
    },
    {
      "epoch": 2.882533895408454,
      "grad_norm": 0.18450206518173218,
      "learning_rate": 0.00042342752962625343,
      "loss": 0.8585,
      "step": 6325
    },
    {
      "epoch": 2.8848125783297256,
      "grad_norm": 0.1860840767621994,
      "learning_rate": 0.000422971741112124,
      "loss": 0.8146,
      "step": 6330
    },
    {
      "epoch": 2.8870912612509967,
      "grad_norm": 0.17673859000205994,
      "learning_rate": 0.0004225159525979945,
      "loss": 0.8749,
      "step": 6335
    },
    {
      "epoch": 2.8893699441722687,
      "grad_norm": 0.17858323454856873,
      "learning_rate": 0.00042206016408386513,
      "loss": 0.8563,
      "step": 6340
    },
    {
      "epoch": 2.8916486270935398,
      "grad_norm": 0.14837989211082458,
      "learning_rate": 0.00042160437556973564,
      "loss": 0.789,
      "step": 6345
    },
    {
      "epoch": 2.8939273100148113,
      "grad_norm": 0.16293473541736603,
      "learning_rate": 0.0004211485870556062,
      "loss": 0.896,
      "step": 6350
    },
    {
      "epoch": 2.896205992936083,
      "grad_norm": 0.168696328997612,
      "learning_rate": 0.0004206927985414767,
      "loss": 0.8754,
      "step": 6355
    },
    {
      "epoch": 2.8984846758573544,
      "grad_norm": 0.1751379370689392,
      "learning_rate": 0.00042023701002734734,
      "loss": 0.8922,
      "step": 6360
    },
    {
      "epoch": 2.900763358778626,
      "grad_norm": 0.17306287586688995,
      "learning_rate": 0.00041978122151321785,
      "loss": 0.86,
      "step": 6365
    },
    {
      "epoch": 2.9030420416998974,
      "grad_norm": 0.16396453976631165,
      "learning_rate": 0.0004193254329990884,
      "loss": 0.8168,
      "step": 6370
    },
    {
      "epoch": 2.905320724621169,
      "grad_norm": 0.16457971930503845,
      "learning_rate": 0.000418869644484959,
      "loss": 0.8597,
      "step": 6375
    },
    {
      "epoch": 2.9075994075424405,
      "grad_norm": 0.17672334611415863,
      "learning_rate": 0.00041841385597082956,
      "loss": 0.8296,
      "step": 6380
    },
    {
      "epoch": 2.909878090463712,
      "grad_norm": 0.18257369101047516,
      "learning_rate": 0.0004179580674567001,
      "loss": 0.8855,
      "step": 6385
    },
    {
      "epoch": 2.9121567733849836,
      "grad_norm": 0.18307387828826904,
      "learning_rate": 0.00041750227894257064,
      "loss": 0.8888,
      "step": 6390
    },
    {
      "epoch": 2.914435456306255,
      "grad_norm": 0.18039192259311676,
      "learning_rate": 0.0004170464904284412,
      "loss": 0.8192,
      "step": 6395
    },
    {
      "epoch": 2.9167141392275266,
      "grad_norm": 0.15597765147686005,
      "learning_rate": 0.00041659070191431177,
      "loss": 0.8592,
      "step": 6400
    },
    {
      "epoch": 2.918992822148798,
      "grad_norm": 0.1606069952249527,
      "learning_rate": 0.00041613491340018234,
      "loss": 0.8352,
      "step": 6405
    },
    {
      "epoch": 2.9212715050700693,
      "grad_norm": 0.20136839151382446,
      "learning_rate": 0.00041567912488605285,
      "loss": 0.8688,
      "step": 6410
    },
    {
      "epoch": 2.9235501879913413,
      "grad_norm": 0.20170100033283234,
      "learning_rate": 0.0004152233363719234,
      "loss": 0.8906,
      "step": 6415
    },
    {
      "epoch": 2.9258288709126123,
      "grad_norm": 0.17264136672019958,
      "learning_rate": 0.000414767547857794,
      "loss": 0.8311,
      "step": 6420
    },
    {
      "epoch": 2.928107553833884,
      "grad_norm": 0.21434877812862396,
      "learning_rate": 0.00041431175934366455,
      "loss": 0.8819,
      "step": 6425
    },
    {
      "epoch": 2.9303862367551554,
      "grad_norm": 0.1725366711616516,
      "learning_rate": 0.00041385597082953506,
      "loss": 0.8743,
      "step": 6430
    },
    {
      "epoch": 2.932664919676427,
      "grad_norm": 0.193253293633461,
      "learning_rate": 0.0004134001823154057,
      "loss": 0.8371,
      "step": 6435
    },
    {
      "epoch": 2.9349436025976985,
      "grad_norm": 0.17729967832565308,
      "learning_rate": 0.00041294439380127625,
      "loss": 0.8829,
      "step": 6440
    },
    {
      "epoch": 2.93722228551897,
      "grad_norm": 0.15186935663223267,
      "learning_rate": 0.00041248860528714676,
      "loss": 0.8292,
      "step": 6445
    },
    {
      "epoch": 2.9395009684402416,
      "grad_norm": 0.16887259483337402,
      "learning_rate": 0.00041203281677301733,
      "loss": 0.782,
      "step": 6450
    },
    {
      "epoch": 2.941779651361513,
      "grad_norm": 0.14684493839740753,
      "learning_rate": 0.0004115770282588879,
      "loss": 0.8068,
      "step": 6455
    },
    {
      "epoch": 2.9440583342827846,
      "grad_norm": 0.19569823145866394,
      "learning_rate": 0.00041112123974475847,
      "loss": 0.8643,
      "step": 6460
    },
    {
      "epoch": 2.946337017204056,
      "grad_norm": 0.17984648048877716,
      "learning_rate": 0.000410665451230629,
      "loss": 0.8832,
      "step": 6465
    },
    {
      "epoch": 2.9486157001253277,
      "grad_norm": 0.1672552227973938,
      "learning_rate": 0.00041020966271649955,
      "loss": 0.8766,
      "step": 6470
    },
    {
      "epoch": 2.950894383046599,
      "grad_norm": 0.17012783885002136,
      "learning_rate": 0.0004097538742023701,
      "loss": 0.8774,
      "step": 6475
    },
    {
      "epoch": 2.9531730659678708,
      "grad_norm": 0.18011249601840973,
      "learning_rate": 0.0004092980856882407,
      "loss": 0.8822,
      "step": 6480
    },
    {
      "epoch": 2.955451748889142,
      "grad_norm": 0.1676676869392395,
      "learning_rate": 0.0004088422971741112,
      "loss": 0.8592,
      "step": 6485
    },
    {
      "epoch": 2.957730431810414,
      "grad_norm": 0.21603459119796753,
      "learning_rate": 0.00040838650865998176,
      "loss": 0.9162,
      "step": 6490
    },
    {
      "epoch": 2.960009114731685,
      "grad_norm": 0.19084329903125763,
      "learning_rate": 0.0004079307201458523,
      "loss": 0.8966,
      "step": 6495
    },
    {
      "epoch": 2.9622877976529565,
      "grad_norm": 0.1674988865852356,
      "learning_rate": 0.0004074749316317229,
      "loss": 0.8758,
      "step": 6500
    },
    {
      "epoch": 2.964566480574228,
      "grad_norm": 0.1778041571378708,
      "learning_rate": 0.00040701914311759346,
      "loss": 0.8784,
      "step": 6505
    },
    {
      "epoch": 2.9668451634954995,
      "grad_norm": 0.16180092096328735,
      "learning_rate": 0.000406563354603464,
      "loss": 0.831,
      "step": 6510
    },
    {
      "epoch": 2.969123846416771,
      "grad_norm": 0.20994462072849274,
      "learning_rate": 0.0004061075660893346,
      "loss": 0.9013,
      "step": 6515
    },
    {
      "epoch": 2.9714025293380426,
      "grad_norm": 0.18770933151245117,
      "learning_rate": 0.0004056517775752051,
      "loss": 0.8339,
      "step": 6520
    },
    {
      "epoch": 2.973681212259314,
      "grad_norm": 0.185061514377594,
      "learning_rate": 0.0004051959890610757,
      "loss": 0.8525,
      "step": 6525
    },
    {
      "epoch": 2.9759598951805857,
      "grad_norm": 0.1783871203660965,
      "learning_rate": 0.0004047402005469462,
      "loss": 0.8604,
      "step": 6530
    },
    {
      "epoch": 2.978238578101857,
      "grad_norm": 0.185777947306633,
      "learning_rate": 0.0004042844120328168,
      "loss": 0.8201,
      "step": 6535
    },
    {
      "epoch": 2.9805172610231287,
      "grad_norm": 0.16444022953510284,
      "learning_rate": 0.0004038286235186873,
      "loss": 0.8599,
      "step": 6540
    },
    {
      "epoch": 2.9827959439444003,
      "grad_norm": 0.1546427607536316,
      "learning_rate": 0.0004033728350045579,
      "loss": 0.8224,
      "step": 6545
    },
    {
      "epoch": 2.9850746268656714,
      "grad_norm": 0.16972245275974274,
      "learning_rate": 0.00040291704649042846,
      "loss": 0.8145,
      "step": 6550
    },
    {
      "epoch": 2.9873533097869434,
      "grad_norm": 0.16800124943256378,
      "learning_rate": 0.000402461257976299,
      "loss": 0.8334,
      "step": 6555
    },
    {
      "epoch": 2.9896319927082144,
      "grad_norm": 0.18725848197937012,
      "learning_rate": 0.00040200546946216954,
      "loss": 0.8578,
      "step": 6560
    },
    {
      "epoch": 2.9919106756294864,
      "grad_norm": 0.17578530311584473,
      "learning_rate": 0.0004015496809480401,
      "loss": 0.9137,
      "step": 6565
    },
    {
      "epoch": 2.9941893585507575,
      "grad_norm": 0.16925866901874542,
      "learning_rate": 0.0004010938924339107,
      "loss": 0.8422,
      "step": 6570
    },
    {
      "epoch": 2.996468041472029,
      "grad_norm": 0.17563889920711517,
      "learning_rate": 0.00040063810391978124,
      "loss": 0.9405,
      "step": 6575
    },
    {
      "epoch": 2.9987467243933006,
      "grad_norm": 0.1588125377893448,
      "learning_rate": 0.0004001823154056518,
      "loss": 0.8882,
      "step": 6580
    },
    {
      "epoch": 2.9996581975618093,
      "eval_loss": 0.754733145236969,
      "eval_runtime": 794.9373,
      "eval_samples_per_second": 25.235,
      "eval_steps_per_second": 3.155,
      "step": 6582
    },
    {
      "epoch": 3.001025407314572,
      "grad_norm": 0.15999318659305573,
      "learning_rate": 0.0003997265268915223,
      "loss": 0.8434,
      "step": 6585
    },
    {
      "epoch": 3.0033040902358437,
      "grad_norm": 0.17605014145374298,
      "learning_rate": 0.00039927073837739294,
      "loss": 0.8936,
      "step": 6590
    },
    {
      "epoch": 3.005582773157115,
      "grad_norm": 0.16531217098236084,
      "learning_rate": 0.00039881494986326345,
      "loss": 0.8343,
      "step": 6595
    },
    {
      "epoch": 3.0078614560783867,
      "grad_norm": 0.19243167340755463,
      "learning_rate": 0.000398359161349134,
      "loss": 0.8442,
      "step": 6600
    },
    {
      "epoch": 3.0101401389996583,
      "grad_norm": 0.18625321984291077,
      "learning_rate": 0.00039790337283500453,
      "loss": 0.868,
      "step": 6605
    },
    {
      "epoch": 3.01241882192093,
      "grad_norm": 0.16176968812942505,
      "learning_rate": 0.00039744758432087515,
      "loss": 0.7935,
      "step": 6610
    },
    {
      "epoch": 3.0146975048422013,
      "grad_norm": 0.17126548290252686,
      "learning_rate": 0.00039699179580674566,
      "loss": 0.8963,
      "step": 6615
    },
    {
      "epoch": 3.016976187763473,
      "grad_norm": 0.17107173800468445,
      "learning_rate": 0.00039653600729261623,
      "loss": 0.8353,
      "step": 6620
    },
    {
      "epoch": 3.0192548706847444,
      "grad_norm": 0.18016766011714935,
      "learning_rate": 0.00039608021877848674,
      "loss": 0.8114,
      "step": 6625
    },
    {
      "epoch": 3.021533553606016,
      "grad_norm": 0.18739308416843414,
      "learning_rate": 0.00039562443026435737,
      "loss": 0.842,
      "step": 6630
    },
    {
      "epoch": 3.023812236527287,
      "grad_norm": 0.19388347864151,
      "learning_rate": 0.0003951686417502279,
      "loss": 0.8321,
      "step": 6635
    },
    {
      "epoch": 3.0260909194485586,
      "grad_norm": 0.19979141652584076,
      "learning_rate": 0.00039471285323609844,
      "loss": 0.8504,
      "step": 6640
    },
    {
      "epoch": 3.02836960236983,
      "grad_norm": 0.18411777913570404,
      "learning_rate": 0.000394257064721969,
      "loss": 0.8586,
      "step": 6645
    },
    {
      "epoch": 3.0306482852911016,
      "grad_norm": 0.160859152674675,
      "learning_rate": 0.0003938012762078396,
      "loss": 0.7447,
      "step": 6650
    },
    {
      "epoch": 3.032926968212373,
      "grad_norm": 0.19017347693443298,
      "learning_rate": 0.00039334548769371015,
      "loss": 0.8157,
      "step": 6655
    },
    {
      "epoch": 3.0352056511336447,
      "grad_norm": 0.1854352504014969,
      "learning_rate": 0.00039288969917958066,
      "loss": 0.8758,
      "step": 6660
    },
    {
      "epoch": 3.0374843340549162,
      "grad_norm": 0.16716042160987854,
      "learning_rate": 0.0003924339106654513,
      "loss": 0.8525,
      "step": 6665
    },
    {
      "epoch": 3.039763016976188,
      "grad_norm": 0.17320497334003448,
      "learning_rate": 0.0003919781221513218,
      "loss": 0.8452,
      "step": 6670
    },
    {
      "epoch": 3.0420416998974593,
      "grad_norm": 0.17579644918441772,
      "learning_rate": 0.00039152233363719236,
      "loss": 0.8203,
      "step": 6675
    },
    {
      "epoch": 3.044320382818731,
      "grad_norm": 0.17107680439949036,
      "learning_rate": 0.00039106654512306287,
      "loss": 0.8682,
      "step": 6680
    },
    {
      "epoch": 3.0465990657400024,
      "grad_norm": 0.20275619626045227,
      "learning_rate": 0.0003906107566089335,
      "loss": 0.8336,
      "step": 6685
    },
    {
      "epoch": 3.048877748661274,
      "grad_norm": 0.17817659676074982,
      "learning_rate": 0.000390154968094804,
      "loss": 0.8402,
      "step": 6690
    },
    {
      "epoch": 3.0511564315825455,
      "grad_norm": 0.17806299030780792,
      "learning_rate": 0.0003896991795806746,
      "loss": 0.826,
      "step": 6695
    },
    {
      "epoch": 3.053435114503817,
      "grad_norm": 0.15345259010791779,
      "learning_rate": 0.0003892433910665451,
      "loss": 0.8057,
      "step": 6700
    },
    {
      "epoch": 3.055713797425088,
      "grad_norm": 0.15763697028160095,
      "learning_rate": 0.0003887876025524157,
      "loss": 0.851,
      "step": 6705
    },
    {
      "epoch": 3.0579924803463596,
      "grad_norm": 0.1791825145483017,
      "learning_rate": 0.0003883318140382863,
      "loss": 0.8759,
      "step": 6710
    },
    {
      "epoch": 3.060271163267631,
      "grad_norm": 0.1722283810377121,
      "learning_rate": 0.0003878760255241568,
      "loss": 0.8207,
      "step": 6715
    },
    {
      "epoch": 3.0625498461889027,
      "grad_norm": 0.14957213401794434,
      "learning_rate": 0.00038742023701002735,
      "loss": 0.794,
      "step": 6720
    },
    {
      "epoch": 3.0648285291101742,
      "grad_norm": 0.2060423344373703,
      "learning_rate": 0.0003869644484958979,
      "loss": 0.8602,
      "step": 6725
    },
    {
      "epoch": 3.0671072120314458,
      "grad_norm": 0.16300448775291443,
      "learning_rate": 0.0003865086599817685,
      "loss": 0.8586,
      "step": 6730
    },
    {
      "epoch": 3.0693858949527173,
      "grad_norm": 0.15720215439796448,
      "learning_rate": 0.000386052871467639,
      "loss": 0.793,
      "step": 6735
    },
    {
      "epoch": 3.071664577873989,
      "grad_norm": 0.17619894444942474,
      "learning_rate": 0.00038559708295350957,
      "loss": 0.8346,
      "step": 6740
    },
    {
      "epoch": 3.0739432607952604,
      "grad_norm": 0.15369026362895966,
      "learning_rate": 0.00038514129443938014,
      "loss": 0.7999,
      "step": 6745
    },
    {
      "epoch": 3.076221943716532,
      "grad_norm": 0.17032456398010254,
      "learning_rate": 0.0003846855059252507,
      "loss": 0.8163,
      "step": 6750
    },
    {
      "epoch": 3.0785006266378034,
      "grad_norm": 0.1727592945098877,
      "learning_rate": 0.0003842297174111212,
      "loss": 0.8496,
      "step": 6755
    },
    {
      "epoch": 3.080779309559075,
      "grad_norm": 0.18886956572532654,
      "learning_rate": 0.0003837739288969918,
      "loss": 0.8493,
      "step": 6760
    },
    {
      "epoch": 3.0830579924803465,
      "grad_norm": 0.15807557106018066,
      "learning_rate": 0.00038331814038286235,
      "loss": 0.8502,
      "step": 6765
    },
    {
      "epoch": 3.085336675401618,
      "grad_norm": 0.15709257125854492,
      "learning_rate": 0.0003828623518687329,
      "loss": 0.8656,
      "step": 6770
    },
    {
      "epoch": 3.0876153583228896,
      "grad_norm": 0.19681695103645325,
      "learning_rate": 0.0003824065633546035,
      "loss": 0.8903,
      "step": 6775
    },
    {
      "epoch": 3.0898940412441607,
      "grad_norm": 0.19112835824489594,
      "learning_rate": 0.00038195077484047405,
      "loss": 0.807,
      "step": 6780
    },
    {
      "epoch": 3.092172724165432,
      "grad_norm": 0.2122267633676529,
      "learning_rate": 0.0003814949863263446,
      "loss": 0.8407,
      "step": 6785
    },
    {
      "epoch": 3.0944514070867037,
      "grad_norm": 0.17773722112178802,
      "learning_rate": 0.00038103919781221513,
      "loss": 0.8304,
      "step": 6790
    },
    {
      "epoch": 3.0967300900079753,
      "grad_norm": 0.1679307371377945,
      "learning_rate": 0.0003805834092980857,
      "loss": 0.7909,
      "step": 6795
    },
    {
      "epoch": 3.099008772929247,
      "grad_norm": 0.16170838475227356,
      "learning_rate": 0.00038012762078395626,
      "loss": 0.8735,
      "step": 6800
    },
    {
      "epoch": 3.1012874558505183,
      "grad_norm": 0.16032443940639496,
      "learning_rate": 0.00037967183226982683,
      "loss": 0.7968,
      "step": 6805
    },
    {
      "epoch": 3.10356613877179,
      "grad_norm": 0.16822518408298492,
      "learning_rate": 0.00037921604375569734,
      "loss": 0.8088,
      "step": 6810
    },
    {
      "epoch": 3.1058448216930614,
      "grad_norm": 0.174311101436615,
      "learning_rate": 0.0003787602552415679,
      "loss": 0.8467,
      "step": 6815
    },
    {
      "epoch": 3.108123504614333,
      "grad_norm": 0.14927728474140167,
      "learning_rate": 0.0003783044667274385,
      "loss": 0.8075,
      "step": 6820
    },
    {
      "epoch": 3.1104021875356045,
      "grad_norm": 0.16700313985347748,
      "learning_rate": 0.00037784867821330905,
      "loss": 0.7787,
      "step": 6825
    },
    {
      "epoch": 3.112680870456876,
      "grad_norm": 0.2004210650920868,
      "learning_rate": 0.00037739288969917956,
      "loss": 0.8744,
      "step": 6830
    },
    {
      "epoch": 3.1149595533781476,
      "grad_norm": 0.17838025093078613,
      "learning_rate": 0.0003769371011850501,
      "loss": 0.8718,
      "step": 6835
    },
    {
      "epoch": 3.117238236299419,
      "grad_norm": 0.2117110788822174,
      "learning_rate": 0.00037648131267092075,
      "loss": 0.8645,
      "step": 6840
    },
    {
      "epoch": 3.1195169192206906,
      "grad_norm": 0.1851588487625122,
      "learning_rate": 0.00037602552415679126,
      "loss": 0.8223,
      "step": 6845
    },
    {
      "epoch": 3.1217956021419617,
      "grad_norm": 0.1621265411376953,
      "learning_rate": 0.0003755697356426618,
      "loss": 0.8634,
      "step": 6850
    },
    {
      "epoch": 3.1240742850632333,
      "grad_norm": 0.17250627279281616,
      "learning_rate": 0.00037511394712853234,
      "loss": 0.854,
      "step": 6855
    },
    {
      "epoch": 3.126352967984505,
      "grad_norm": 0.1817646473646164,
      "learning_rate": 0.00037465815861440296,
      "loss": 0.851,
      "step": 6860
    },
    {
      "epoch": 3.1286316509057763,
      "grad_norm": 0.1755882352590561,
      "learning_rate": 0.0003742023701002735,
      "loss": 0.8667,
      "step": 6865
    },
    {
      "epoch": 3.130910333827048,
      "grad_norm": 0.16853031516075134,
      "learning_rate": 0.00037374658158614404,
      "loss": 0.8617,
      "step": 6870
    },
    {
      "epoch": 3.1331890167483194,
      "grad_norm": 0.17653200030326843,
      "learning_rate": 0.00037329079307201455,
      "loss": 0.8509,
      "step": 6875
    },
    {
      "epoch": 3.135467699669591,
      "grad_norm": 0.1795104593038559,
      "learning_rate": 0.0003728350045578852,
      "loss": 0.8459,
      "step": 6880
    },
    {
      "epoch": 3.1377463825908625,
      "grad_norm": 0.17280274629592896,
      "learning_rate": 0.0003723792160437557,
      "loss": 0.8476,
      "step": 6885
    },
    {
      "epoch": 3.140025065512134,
      "grad_norm": 0.19130447506904602,
      "learning_rate": 0.00037192342752962625,
      "loss": 0.87,
      "step": 6890
    },
    {
      "epoch": 3.1423037484334055,
      "grad_norm": 0.18482400476932526,
      "learning_rate": 0.0003714676390154968,
      "loss": 0.7731,
      "step": 6895
    },
    {
      "epoch": 3.144582431354677,
      "grad_norm": 0.17722401022911072,
      "learning_rate": 0.0003710118505013674,
      "loss": 0.7987,
      "step": 6900
    },
    {
      "epoch": 3.1468611142759486,
      "grad_norm": 0.172243133187294,
      "learning_rate": 0.0003705560619872379,
      "loss": 0.8314,
      "step": 6905
    },
    {
      "epoch": 3.14913979719722,
      "grad_norm": 0.18561777472496033,
      "learning_rate": 0.00037010027347310847,
      "loss": 0.9283,
      "step": 6910
    },
    {
      "epoch": 3.1514184801184917,
      "grad_norm": 0.17194205522537231,
      "learning_rate": 0.0003696444849589791,
      "loss": 0.8232,
      "step": 6915
    },
    {
      "epoch": 3.153697163039763,
      "grad_norm": 0.1583985984325409,
      "learning_rate": 0.0003691886964448496,
      "loss": 0.7869,
      "step": 6920
    },
    {
      "epoch": 3.1559758459610343,
      "grad_norm": 0.1695617139339447,
      "learning_rate": 0.00036873290793072017,
      "loss": 0.8063,
      "step": 6925
    },
    {
      "epoch": 3.158254528882306,
      "grad_norm": 0.17172521352767944,
      "learning_rate": 0.0003682771194165907,
      "loss": 0.8502,
      "step": 6930
    },
    {
      "epoch": 3.1605332118035774,
      "grad_norm": 0.18582892417907715,
      "learning_rate": 0.0003678213309024613,
      "loss": 0.8329,
      "step": 6935
    },
    {
      "epoch": 3.162811894724849,
      "grad_norm": 0.20878811180591583,
      "learning_rate": 0.0003673655423883318,
      "loss": 0.8357,
      "step": 6940
    },
    {
      "epoch": 3.1650905776461205,
      "grad_norm": 0.18383310735225677,
      "learning_rate": 0.0003669097538742024,
      "loss": 0.8407,
      "step": 6945
    },
    {
      "epoch": 3.167369260567392,
      "grad_norm": 0.17019106447696686,
      "learning_rate": 0.0003664539653600729,
      "loss": 0.842,
      "step": 6950
    },
    {
      "epoch": 3.1696479434886635,
      "grad_norm": 0.1547420471906662,
      "learning_rate": 0.0003659981768459435,
      "loss": 0.828,
      "step": 6955
    },
    {
      "epoch": 3.171926626409935,
      "grad_norm": 0.1777871549129486,
      "learning_rate": 0.00036554238833181403,
      "loss": 0.8136,
      "step": 6960
    },
    {
      "epoch": 3.1742053093312066,
      "grad_norm": 0.19382266700267792,
      "learning_rate": 0.0003650865998176846,
      "loss": 0.9174,
      "step": 6965
    },
    {
      "epoch": 3.176483992252478,
      "grad_norm": 0.1803794801235199,
      "learning_rate": 0.0003646308113035551,
      "loss": 0.853,
      "step": 6970
    },
    {
      "epoch": 3.1787626751737497,
      "grad_norm": 0.18541662395000458,
      "learning_rate": 0.00036417502278942573,
      "loss": 0.8775,
      "step": 6975
    },
    {
      "epoch": 3.181041358095021,
      "grad_norm": 0.1673182100057602,
      "learning_rate": 0.0003637192342752963,
      "loss": 0.8458,
      "step": 6980
    },
    {
      "epoch": 3.1833200410162927,
      "grad_norm": 0.17872317135334015,
      "learning_rate": 0.0003632634457611668,
      "loss": 0.8105,
      "step": 6985
    },
    {
      "epoch": 3.1855987239375643,
      "grad_norm": 0.18013374507427216,
      "learning_rate": 0.0003628076572470374,
      "loss": 0.8563,
      "step": 6990
    },
    {
      "epoch": 3.187877406858836,
      "grad_norm": 0.19720971584320068,
      "learning_rate": 0.00036235186873290794,
      "loss": 0.8258,
      "step": 6995
    },
    {
      "epoch": 3.190156089780107,
      "grad_norm": 0.17738184332847595,
      "learning_rate": 0.0003618960802187785,
      "loss": 0.8467,
      "step": 7000
    },
    {
      "epoch": 3.1924347727013784,
      "grad_norm": 0.176728293299675,
      "learning_rate": 0.000361440291704649,
      "loss": 0.8323,
      "step": 7005
    },
    {
      "epoch": 3.19471345562265,
      "grad_norm": 0.18719302117824554,
      "learning_rate": 0.00036098450319051965,
      "loss": 0.86,
      "step": 7010
    },
    {
      "epoch": 3.1969921385439215,
      "grad_norm": 0.2066560983657837,
      "learning_rate": 0.00036052871467639016,
      "loss": 0.8249,
      "step": 7015
    },
    {
      "epoch": 3.199270821465193,
      "grad_norm": 0.17607711255550385,
      "learning_rate": 0.0003600729261622607,
      "loss": 0.846,
      "step": 7020
    },
    {
      "epoch": 3.2015495043864646,
      "grad_norm": 0.18480610847473145,
      "learning_rate": 0.00035961713764813124,
      "loss": 0.8242,
      "step": 7025
    },
    {
      "epoch": 3.203828187307736,
      "grad_norm": 0.17401757836341858,
      "learning_rate": 0.00035916134913400186,
      "loss": 0.7767,
      "step": 7030
    },
    {
      "epoch": 3.2061068702290076,
      "grad_norm": 0.1839171200990677,
      "learning_rate": 0.00035870556061987237,
      "loss": 0.8498,
      "step": 7035
    },
    {
      "epoch": 3.208385553150279,
      "grad_norm": 0.14848734438419342,
      "learning_rate": 0.00035824977210574294,
      "loss": 0.7894,
      "step": 7040
    },
    {
      "epoch": 3.2106642360715507,
      "grad_norm": 0.18735945224761963,
      "learning_rate": 0.0003577939835916135,
      "loss": 0.8427,
      "step": 7045
    },
    {
      "epoch": 3.2129429189928222,
      "grad_norm": 0.1561369150876999,
      "learning_rate": 0.0003573381950774841,
      "loss": 0.9006,
      "step": 7050
    },
    {
      "epoch": 3.215221601914094,
      "grad_norm": 0.2003459334373474,
      "learning_rate": 0.00035688240656335464,
      "loss": 0.8632,
      "step": 7055
    },
    {
      "epoch": 3.2175002848353653,
      "grad_norm": 0.17707708477973938,
      "learning_rate": 0.00035642661804922515,
      "loss": 0.8503,
      "step": 7060
    },
    {
      "epoch": 3.219778967756637,
      "grad_norm": 0.18952177464962006,
      "learning_rate": 0.0003559708295350957,
      "loss": 0.8179,
      "step": 7065
    },
    {
      "epoch": 3.2220576506779084,
      "grad_norm": 0.1892845183610916,
      "learning_rate": 0.0003555150410209663,
      "loss": 0.8498,
      "step": 7070
    },
    {
      "epoch": 3.2243363335991795,
      "grad_norm": 0.1721300631761551,
      "learning_rate": 0.00035505925250683685,
      "loss": 0.7949,
      "step": 7075
    },
    {
      "epoch": 3.226615016520451,
      "grad_norm": 0.1829933524131775,
      "learning_rate": 0.00035460346399270737,
      "loss": 0.7977,
      "step": 7080
    },
    {
      "epoch": 3.2288936994417226,
      "grad_norm": 0.20129309594631195,
      "learning_rate": 0.00035414767547857793,
      "loss": 0.8585,
      "step": 7085
    },
    {
      "epoch": 3.231172382362994,
      "grad_norm": 0.16750481724739075,
      "learning_rate": 0.0003536918869644485,
      "loss": 0.8164,
      "step": 7090
    },
    {
      "epoch": 3.2334510652842656,
      "grad_norm": 0.17512297630310059,
      "learning_rate": 0.00035323609845031907,
      "loss": 0.8739,
      "step": 7095
    },
    {
      "epoch": 3.235729748205537,
      "grad_norm": 0.23082250356674194,
      "learning_rate": 0.0003527803099361896,
      "loss": 0.8765,
      "step": 7100
    },
    {
      "epoch": 3.2380084311268087,
      "grad_norm": 0.16904346644878387,
      "learning_rate": 0.00035232452142206015,
      "loss": 0.8799,
      "step": 7105
    },
    {
      "epoch": 3.2402871140480802,
      "grad_norm": 0.17499656975269318,
      "learning_rate": 0.0003518687329079307,
      "loss": 0.7919,
      "step": 7110
    },
    {
      "epoch": 3.2425657969693518,
      "grad_norm": 0.1656392365694046,
      "learning_rate": 0.0003514129443938013,
      "loss": 0.8422,
      "step": 7115
    },
    {
      "epoch": 3.2448444798906233,
      "grad_norm": 0.18135982751846313,
      "learning_rate": 0.00035095715587967185,
      "loss": 0.8598,
      "step": 7120
    },
    {
      "epoch": 3.247123162811895,
      "grad_norm": 0.1980571746826172,
      "learning_rate": 0.0003505013673655424,
      "loss": 0.8905,
      "step": 7125
    },
    {
      "epoch": 3.2494018457331664,
      "grad_norm": 0.16181880235671997,
      "learning_rate": 0.000350045578851413,
      "loss": 0.8111,
      "step": 7130
    },
    {
      "epoch": 3.251680528654438,
      "grad_norm": 0.1792164295911789,
      "learning_rate": 0.0003495897903372835,
      "loss": 0.8605,
      "step": 7135
    },
    {
      "epoch": 3.253959211575709,
      "grad_norm": 0.1920541524887085,
      "learning_rate": 0.00034913400182315406,
      "loss": 0.8628,
      "step": 7140
    },
    {
      "epoch": 3.256237894496981,
      "grad_norm": 0.18252016603946686,
      "learning_rate": 0.00034867821330902463,
      "loss": 0.8777,
      "step": 7145
    },
    {
      "epoch": 3.258516577418252,
      "grad_norm": 0.16318581998348236,
      "learning_rate": 0.0003482224247948952,
      "loss": 0.7911,
      "step": 7150
    },
    {
      "epoch": 3.2607952603395236,
      "grad_norm": 0.16857704520225525,
      "learning_rate": 0.0003477666362807657,
      "loss": 0.8156,
      "step": 7155
    },
    {
      "epoch": 3.263073943260795,
      "grad_norm": 0.1776125133037567,
      "learning_rate": 0.0003473108477666363,
      "loss": 0.8499,
      "step": 7160
    },
    {
      "epoch": 3.2653526261820667,
      "grad_norm": 0.1874619424343109,
      "learning_rate": 0.00034685505925250684,
      "loss": 0.8526,
      "step": 7165
    },
    {
      "epoch": 3.267631309103338,
      "grad_norm": 0.1652565896511078,
      "learning_rate": 0.0003463992707383774,
      "loss": 0.7944,
      "step": 7170
    },
    {
      "epoch": 3.2699099920246097,
      "grad_norm": 0.16472263634204865,
      "learning_rate": 0.0003459434822242479,
      "loss": 0.8339,
      "step": 7175
    },
    {
      "epoch": 3.2721886749458813,
      "grad_norm": 0.16553710401058197,
      "learning_rate": 0.0003454876937101185,
      "loss": 0.8971,
      "step": 7180
    },
    {
      "epoch": 3.274467357867153,
      "grad_norm": 0.16051575541496277,
      "learning_rate": 0.0003450319051959891,
      "loss": 0.8264,
      "step": 7185
    },
    {
      "epoch": 3.2767460407884244,
      "grad_norm": 0.1630861759185791,
      "learning_rate": 0.0003445761166818596,
      "loss": 0.8589,
      "step": 7190
    },
    {
      "epoch": 3.279024723709696,
      "grad_norm": 0.17415867745876312,
      "learning_rate": 0.0003441203281677302,
      "loss": 0.8712,
      "step": 7195
    },
    {
      "epoch": 3.2813034066309674,
      "grad_norm": 0.16575652360916138,
      "learning_rate": 0.0003436645396536007,
      "loss": 0.7887,
      "step": 7200
    },
    {
      "epoch": 3.283582089552239,
      "grad_norm": 0.19088324904441833,
      "learning_rate": 0.0003432087511394713,
      "loss": 0.97,
      "step": 7205
    },
    {
      "epoch": 3.2858607724735105,
      "grad_norm": 0.16956408321857452,
      "learning_rate": 0.00034275296262534184,
      "loss": 0.7848,
      "step": 7210
    },
    {
      "epoch": 3.2881394553947816,
      "grad_norm": 0.16702841222286224,
      "learning_rate": 0.0003422971741112124,
      "loss": 0.8582,
      "step": 7215
    },
    {
      "epoch": 3.2904181383160536,
      "grad_norm": 0.17185093462467194,
      "learning_rate": 0.0003418413855970829,
      "loss": 0.8978,
      "step": 7220
    },
    {
      "epoch": 3.2926968212373247,
      "grad_norm": 0.19242608547210693,
      "learning_rate": 0.00034138559708295354,
      "loss": 0.7906,
      "step": 7225
    },
    {
      "epoch": 3.294975504158596,
      "grad_norm": 0.17279627919197083,
      "learning_rate": 0.00034092980856882405,
      "loss": 0.8687,
      "step": 7230
    },
    {
      "epoch": 3.2972541870798677,
      "grad_norm": 0.18166598677635193,
      "learning_rate": 0.0003404740200546946,
      "loss": 0.8361,
      "step": 7235
    },
    {
      "epoch": 3.2995328700011393,
      "grad_norm": 0.14589400589466095,
      "learning_rate": 0.00034001823154056513,
      "loss": 0.7968,
      "step": 7240
    },
    {
      "epoch": 3.301811552922411,
      "grad_norm": 0.17622195184230804,
      "learning_rate": 0.00033956244302643575,
      "loss": 0.8569,
      "step": 7245
    },
    {
      "epoch": 3.3040902358436823,
      "grad_norm": 0.1850711554288864,
      "learning_rate": 0.0003391066545123063,
      "loss": 0.8357,
      "step": 7250
    },
    {
      "epoch": 3.306368918764954,
      "grad_norm": 0.1899634748697281,
      "learning_rate": 0.00033865086599817683,
      "loss": 0.9143,
      "step": 7255
    },
    {
      "epoch": 3.3086476016862254,
      "grad_norm": 0.17326493561267853,
      "learning_rate": 0.00033819507748404745,
      "loss": 0.8375,
      "step": 7260
    },
    {
      "epoch": 3.310926284607497,
      "grad_norm": 0.20841029286384583,
      "learning_rate": 0.00033773928896991797,
      "loss": 0.8275,
      "step": 7265
    },
    {
      "epoch": 3.3132049675287685,
      "grad_norm": 0.18909209966659546,
      "learning_rate": 0.00033728350045578853,
      "loss": 0.8456,
      "step": 7270
    },
    {
      "epoch": 3.31548365045004,
      "grad_norm": 0.1935584843158722,
      "learning_rate": 0.00033682771194165905,
      "loss": 0.8648,
      "step": 7275
    },
    {
      "epoch": 3.3177623333713115,
      "grad_norm": 0.17231173813343048,
      "learning_rate": 0.00033637192342752967,
      "loss": 0.8077,
      "step": 7280
    },
    {
      "epoch": 3.320041016292583,
      "grad_norm": 0.19538851082324982,
      "learning_rate": 0.0003359161349134002,
      "loss": 0.8423,
      "step": 7285
    },
    {
      "epoch": 3.322319699213854,
      "grad_norm": 0.18612883985042572,
      "learning_rate": 0.00033546034639927075,
      "loss": 0.8169,
      "step": 7290
    },
    {
      "epoch": 3.3245983821351257,
      "grad_norm": 0.19061028957366943,
      "learning_rate": 0.00033500455788514126,
      "loss": 0.8898,
      "step": 7295
    },
    {
      "epoch": 3.3268770650563972,
      "grad_norm": 0.16168498992919922,
      "learning_rate": 0.0003345487693710119,
      "loss": 0.8298,
      "step": 7300
    },
    {
      "epoch": 3.3291557479776688,
      "grad_norm": 0.1770395189523697,
      "learning_rate": 0.0003340929808568824,
      "loss": 0.8134,
      "step": 7305
    },
    {
      "epoch": 3.3314344308989403,
      "grad_norm": 0.1627729833126068,
      "learning_rate": 0.00033363719234275296,
      "loss": 0.8149,
      "step": 7310
    },
    {
      "epoch": 3.333713113820212,
      "grad_norm": 0.18336576223373413,
      "learning_rate": 0.00033318140382862353,
      "loss": 0.8487,
      "step": 7315
    },
    {
      "epoch": 3.3359917967414834,
      "grad_norm": 0.19315457344055176,
      "learning_rate": 0.0003327256153144941,
      "loss": 0.8165,
      "step": 7320
    },
    {
      "epoch": 3.338270479662755,
      "grad_norm": 0.17475520074367523,
      "learning_rate": 0.00033226982680036466,
      "loss": 0.8735,
      "step": 7325
    },
    {
      "epoch": 3.3405491625840265,
      "grad_norm": 0.1877911239862442,
      "learning_rate": 0.0003318140382862352,
      "loss": 0.8467,
      "step": 7330
    },
    {
      "epoch": 3.342827845505298,
      "grad_norm": 0.1752471774816513,
      "learning_rate": 0.00033135824977210574,
      "loss": 0.8376,
      "step": 7335
    },
    {
      "epoch": 3.3451065284265695,
      "grad_norm": 0.18017421662807465,
      "learning_rate": 0.0003309024612579763,
      "loss": 0.8074,
      "step": 7340
    },
    {
      "epoch": 3.347385211347841,
      "grad_norm": 0.18033961951732635,
      "learning_rate": 0.0003304466727438469,
      "loss": 0.8883,
      "step": 7345
    },
    {
      "epoch": 3.3496638942691126,
      "grad_norm": 0.1724841594696045,
      "learning_rate": 0.0003299908842297174,
      "loss": 0.8137,
      "step": 7350
    },
    {
      "epoch": 3.351942577190384,
      "grad_norm": 0.17905396223068237,
      "learning_rate": 0.000329535095715588,
      "loss": 0.8788,
      "step": 7355
    },
    {
      "epoch": 3.3542212601116557,
      "grad_norm": 0.18478938937187195,
      "learning_rate": 0.0003290793072014585,
      "loss": 0.8678,
      "step": 7360
    },
    {
      "epoch": 3.3564999430329268,
      "grad_norm": 0.1797787845134735,
      "learning_rate": 0.0003286235186873291,
      "loss": 0.8686,
      "step": 7365
    },
    {
      "epoch": 3.3587786259541983,
      "grad_norm": 0.17118895053863525,
      "learning_rate": 0.0003281677301731996,
      "loss": 0.8318,
      "step": 7370
    },
    {
      "epoch": 3.36105730887547,
      "grad_norm": 0.17778101563453674,
      "learning_rate": 0.0003277119416590702,
      "loss": 0.8973,
      "step": 7375
    },
    {
      "epoch": 3.3633359917967414,
      "grad_norm": 0.16623695194721222,
      "learning_rate": 0.00032725615314494074,
      "loss": 0.7389,
      "step": 7380
    },
    {
      "epoch": 3.365614674718013,
      "grad_norm": 0.1867884248495102,
      "learning_rate": 0.0003268003646308113,
      "loss": 0.8797,
      "step": 7385
    },
    {
      "epoch": 3.3678933576392844,
      "grad_norm": 0.17608945071697235,
      "learning_rate": 0.00032634457611668187,
      "loss": 0.8135,
      "step": 7390
    },
    {
      "epoch": 3.370172040560556,
      "grad_norm": 0.19102953374385834,
      "learning_rate": 0.00032588878760255244,
      "loss": 0.8258,
      "step": 7395
    },
    {
      "epoch": 3.3724507234818275,
      "grad_norm": 0.16205397248268127,
      "learning_rate": 0.000325432999088423,
      "loss": 0.8321,
      "step": 7400
    },
    {
      "epoch": 3.374729406403099,
      "grad_norm": 0.189138263463974,
      "learning_rate": 0.0003249772105742935,
      "loss": 0.8881,
      "step": 7405
    },
    {
      "epoch": 3.3770080893243706,
      "grad_norm": 0.16444922983646393,
      "learning_rate": 0.0003245214220601641,
      "loss": 0.7793,
      "step": 7410
    },
    {
      "epoch": 3.379286772245642,
      "grad_norm": 0.16195382177829742,
      "learning_rate": 0.00032406563354603465,
      "loss": 0.8424,
      "step": 7415
    },
    {
      "epoch": 3.3815654551669136,
      "grad_norm": 0.2392832338809967,
      "learning_rate": 0.0003236098450319052,
      "loss": 0.8035,
      "step": 7420
    },
    {
      "epoch": 3.383844138088185,
      "grad_norm": 0.1685950607061386,
      "learning_rate": 0.00032315405651777573,
      "loss": 0.8282,
      "step": 7425
    },
    {
      "epoch": 3.3861228210094567,
      "grad_norm": 0.17545591294765472,
      "learning_rate": 0.0003226982680036463,
      "loss": 0.8854,
      "step": 7430
    },
    {
      "epoch": 3.3884015039307283,
      "grad_norm": 0.16179555654525757,
      "learning_rate": 0.00032224247948951687,
      "loss": 0.7676,
      "step": 7435
    },
    {
      "epoch": 3.3906801868519993,
      "grad_norm": 0.17569930851459503,
      "learning_rate": 0.00032178669097538743,
      "loss": 0.8279,
      "step": 7440
    },
    {
      "epoch": 3.392958869773271,
      "grad_norm": 0.1675688475370407,
      "learning_rate": 0.00032133090246125795,
      "loss": 0.853,
      "step": 7445
    },
    {
      "epoch": 3.3952375526945424,
      "grad_norm": 0.1754564642906189,
      "learning_rate": 0.0003208751139471285,
      "loss": 0.7447,
      "step": 7450
    },
    {
      "epoch": 3.397516235615814,
      "grad_norm": 0.20335237681865692,
      "learning_rate": 0.00032041932543299913,
      "loss": 0.8748,
      "step": 7455
    },
    {
      "epoch": 3.3997949185370855,
      "grad_norm": 0.19580213725566864,
      "learning_rate": 0.00031996353691886965,
      "loss": 0.8295,
      "step": 7460
    },
    {
      "epoch": 3.402073601458357,
      "grad_norm": 0.18362058699131012,
      "learning_rate": 0.0003195077484047402,
      "loss": 0.9037,
      "step": 7465
    },
    {
      "epoch": 3.4043522843796286,
      "grad_norm": 0.1899476796388626,
      "learning_rate": 0.00031905195989061073,
      "loss": 0.8744,
      "step": 7470
    },
    {
      "epoch": 3.4066309673009,
      "grad_norm": 0.17347311973571777,
      "learning_rate": 0.00031859617137648135,
      "loss": 0.826,
      "step": 7475
    },
    {
      "epoch": 3.4089096502221716,
      "grad_norm": 0.16995397210121155,
      "learning_rate": 0.00031814038286235186,
      "loss": 0.7777,
      "step": 7480
    },
    {
      "epoch": 3.411188333143443,
      "grad_norm": 0.18200361728668213,
      "learning_rate": 0.00031768459434822243,
      "loss": 0.811,
      "step": 7485
    },
    {
      "epoch": 3.4134670160647147,
      "grad_norm": 0.17259609699249268,
      "learning_rate": 0.000317228805834093,
      "loss": 0.8426,
      "step": 7490
    },
    {
      "epoch": 3.4157456989859862,
      "grad_norm": 0.18420042097568512,
      "learning_rate": 0.00031677301731996356,
      "loss": 0.8734,
      "step": 7495
    },
    {
      "epoch": 3.4180243819072578,
      "grad_norm": 0.16296936571598053,
      "learning_rate": 0.0003163172288058341,
      "loss": 0.8454,
      "step": 7500
    },
    {
      "epoch": 3.420303064828529,
      "grad_norm": 0.1705963909626007,
      "learning_rate": 0.00031586144029170464,
      "loss": 0.8044,
      "step": 7505
    },
    {
      "epoch": 3.422581747749801,
      "grad_norm": 0.18582500517368317,
      "learning_rate": 0.0003154056517775752,
      "loss": 0.8433,
      "step": 7510
    },
    {
      "epoch": 3.424860430671072,
      "grad_norm": 0.18272888660430908,
      "learning_rate": 0.0003149498632634458,
      "loss": 0.8456,
      "step": 7515
    },
    {
      "epoch": 3.4271391135923435,
      "grad_norm": 0.17577031254768372,
      "learning_rate": 0.00031449407474931634,
      "loss": 0.8498,
      "step": 7520
    },
    {
      "epoch": 3.429417796513615,
      "grad_norm": 0.1715690940618515,
      "learning_rate": 0.00031403828623518686,
      "loss": 0.862,
      "step": 7525
    },
    {
      "epoch": 3.4316964794348865,
      "grad_norm": 0.1766778975725174,
      "learning_rate": 0.0003135824977210575,
      "loss": 0.7679,
      "step": 7530
    },
    {
      "epoch": 3.433975162356158,
      "grad_norm": 0.16078558564186096,
      "learning_rate": 0.000313126709206928,
      "loss": 0.8417,
      "step": 7535
    },
    {
      "epoch": 3.4362538452774296,
      "grad_norm": 0.16635319590568542,
      "learning_rate": 0.00031267092069279856,
      "loss": 0.784,
      "step": 7540
    },
    {
      "epoch": 3.438532528198701,
      "grad_norm": 0.18119028210639954,
      "learning_rate": 0.00031221513217866907,
      "loss": 0.8202,
      "step": 7545
    },
    {
      "epoch": 3.4408112111199727,
      "grad_norm": 0.1715695559978485,
      "learning_rate": 0.0003117593436645397,
      "loss": 0.803,
      "step": 7550
    },
    {
      "epoch": 3.443089894041244,
      "grad_norm": 0.15334461629390717,
      "learning_rate": 0.0003113035551504102,
      "loss": 0.8609,
      "step": 7555
    },
    {
      "epoch": 3.4453685769625157,
      "grad_norm": 0.1754458248615265,
      "learning_rate": 0.00031084776663628077,
      "loss": 0.8604,
      "step": 7560
    },
    {
      "epoch": 3.4476472598837873,
      "grad_norm": 0.1725575178861618,
      "learning_rate": 0.0003103919781221513,
      "loss": 0.8235,
      "step": 7565
    },
    {
      "epoch": 3.449925942805059,
      "grad_norm": 0.17478330433368683,
      "learning_rate": 0.0003099361896080219,
      "loss": 0.8114,
      "step": 7570
    },
    {
      "epoch": 3.4522046257263304,
      "grad_norm": 0.186906099319458,
      "learning_rate": 0.0003094804010938924,
      "loss": 0.892,
      "step": 7575
    },
    {
      "epoch": 3.4544833086476014,
      "grad_norm": 0.16748978197574615,
      "learning_rate": 0.000309024612579763,
      "loss": 0.8224,
      "step": 7580
    },
    {
      "epoch": 3.4567619915688734,
      "grad_norm": 0.1791868805885315,
      "learning_rate": 0.0003085688240656336,
      "loss": 0.8638,
      "step": 7585
    },
    {
      "epoch": 3.4590406744901445,
      "grad_norm": 0.1780933141708374,
      "learning_rate": 0.0003081130355515041,
      "loss": 0.8168,
      "step": 7590
    },
    {
      "epoch": 3.461319357411416,
      "grad_norm": 0.1866525411605835,
      "learning_rate": 0.0003076572470373747,
      "loss": 0.8871,
      "step": 7595
    },
    {
      "epoch": 3.4635980403326876,
      "grad_norm": 0.15648670494556427,
      "learning_rate": 0.0003072014585232452,
      "loss": 0.795,
      "step": 7600
    },
    {
      "epoch": 3.465876723253959,
      "grad_norm": 0.19254961609840393,
      "learning_rate": 0.0003067456700091158,
      "loss": 0.8452,
      "step": 7605
    },
    {
      "epoch": 3.4681554061752307,
      "grad_norm": 0.1933857798576355,
      "learning_rate": 0.00030628988149498633,
      "loss": 0.9499,
      "step": 7610
    },
    {
      "epoch": 3.470434089096502,
      "grad_norm": 0.18758998811244965,
      "learning_rate": 0.0003058340929808569,
      "loss": 0.8281,
      "step": 7615
    },
    {
      "epoch": 3.4727127720177737,
      "grad_norm": 0.19231821596622467,
      "learning_rate": 0.0003053783044667274,
      "loss": 0.8592,
      "step": 7620
    },
    {
      "epoch": 3.4749914549390453,
      "grad_norm": 0.165611132979393,
      "learning_rate": 0.00030492251595259803,
      "loss": 0.8554,
      "step": 7625
    },
    {
      "epoch": 3.477270137860317,
      "grad_norm": 0.17556308209896088,
      "learning_rate": 0.00030446672743846855,
      "loss": 0.8469,
      "step": 7630
    },
    {
      "epoch": 3.4795488207815883,
      "grad_norm": 0.1726573407649994,
      "learning_rate": 0.0003040109389243391,
      "loss": 0.8456,
      "step": 7635
    },
    {
      "epoch": 3.48182750370286,
      "grad_norm": 0.20010845363140106,
      "learning_rate": 0.0003035551504102096,
      "loss": 0.8415,
      "step": 7640
    },
    {
      "epoch": 3.4841061866241314,
      "grad_norm": 0.1682330071926117,
      "learning_rate": 0.00030309936189608025,
      "loss": 0.7774,
      "step": 7645
    },
    {
      "epoch": 3.486384869545403,
      "grad_norm": 0.17919252812862396,
      "learning_rate": 0.00030264357338195076,
      "loss": 0.8596,
      "step": 7650
    },
    {
      "epoch": 3.488663552466674,
      "grad_norm": 0.2226860225200653,
      "learning_rate": 0.00030218778486782133,
      "loss": 0.7914,
      "step": 7655
    },
    {
      "epoch": 3.490942235387946,
      "grad_norm": 0.18709532916545868,
      "learning_rate": 0.0003017319963536919,
      "loss": 0.8147,
      "step": 7660
    },
    {
      "epoch": 3.493220918309217,
      "grad_norm": 0.1635298728942871,
      "learning_rate": 0.00030127620783956246,
      "loss": 0.8389,
      "step": 7665
    },
    {
      "epoch": 3.4954996012304886,
      "grad_norm": 0.1702025830745697,
      "learning_rate": 0.00030082041932543303,
      "loss": 0.8094,
      "step": 7670
    },
    {
      "epoch": 3.49777828415176,
      "grad_norm": 0.179697647690773,
      "learning_rate": 0.00030036463081130354,
      "loss": 0.8947,
      "step": 7675
    },
    {
      "epoch": 3.5000569670730317,
      "grad_norm": 0.18172559142112732,
      "learning_rate": 0.0002999088422971741,
      "loss": 0.8417,
      "step": 7680
    },
    {
      "epoch": 3.5023356499943032,
      "grad_norm": 0.18035045266151428,
      "learning_rate": 0.0002994530537830447,
      "loss": 0.8242,
      "step": 7685
    },
    {
      "epoch": 3.504614332915575,
      "grad_norm": 0.20050883293151855,
      "learning_rate": 0.00029899726526891524,
      "loss": 0.8304,
      "step": 7690
    },
    {
      "epoch": 3.5068930158368463,
      "grad_norm": 0.18385964632034302,
      "learning_rate": 0.00029854147675478576,
      "loss": 0.8327,
      "step": 7695
    },
    {
      "epoch": 3.509171698758118,
      "grad_norm": 0.1663362979888916,
      "learning_rate": 0.0002980856882406563,
      "loss": 0.8988,
      "step": 7700
    },
    {
      "epoch": 3.5114503816793894,
      "grad_norm": 0.1812349259853363,
      "learning_rate": 0.0002976298997265269,
      "loss": 0.8245,
      "step": 7705
    },
    {
      "epoch": 3.513729064600661,
      "grad_norm": 0.2054688185453415,
      "learning_rate": 0.00029717411121239746,
      "loss": 0.9629,
      "step": 7710
    },
    {
      "epoch": 3.5160077475219325,
      "grad_norm": 0.16273410618305206,
      "learning_rate": 0.00029671832269826797,
      "loss": 0.8493,
      "step": 7715
    },
    {
      "epoch": 3.518286430443204,
      "grad_norm": 0.16396375000476837,
      "learning_rate": 0.0002962625341841386,
      "loss": 0.8369,
      "step": 7720
    },
    {
      "epoch": 3.5205651133644755,
      "grad_norm": 0.20590944588184357,
      "learning_rate": 0.00029580674567000916,
      "loss": 0.8392,
      "step": 7725
    },
    {
      "epoch": 3.5228437962857466,
      "grad_norm": 0.20952290296554565,
      "learning_rate": 0.00029535095715587967,
      "loss": 0.8131,
      "step": 7730
    },
    {
      "epoch": 3.5251224792070186,
      "grad_norm": 0.18202608823776245,
      "learning_rate": 0.00029489516864175024,
      "loss": 0.8099,
      "step": 7735
    },
    {
      "epoch": 3.5274011621282897,
      "grad_norm": 0.1681833267211914,
      "learning_rate": 0.0002944393801276208,
      "loss": 0.8982,
      "step": 7740
    },
    {
      "epoch": 3.5296798450495612,
      "grad_norm": 0.17132414877414703,
      "learning_rate": 0.00029398359161349137,
      "loss": 0.8187,
      "step": 7745
    },
    {
      "epoch": 3.5319585279708328,
      "grad_norm": 0.16040608286857605,
      "learning_rate": 0.0002935278030993619,
      "loss": 0.8232,
      "step": 7750
    },
    {
      "epoch": 3.5342372108921043,
      "grad_norm": 0.1719905138015747,
      "learning_rate": 0.00029307201458523245,
      "loss": 0.8567,
      "step": 7755
    },
    {
      "epoch": 3.536515893813376,
      "grad_norm": 0.19657543301582336,
      "learning_rate": 0.000292616226071103,
      "loss": 0.8476,
      "step": 7760
    },
    {
      "epoch": 3.5387945767346474,
      "grad_norm": 0.19031554460525513,
      "learning_rate": 0.0002921604375569736,
      "loss": 0.851,
      "step": 7765
    },
    {
      "epoch": 3.541073259655919,
      "grad_norm": 0.17481747269630432,
      "learning_rate": 0.0002917046490428441,
      "loss": 0.8714,
      "step": 7770
    },
    {
      "epoch": 3.5433519425771904,
      "grad_norm": 0.20958498120307922,
      "learning_rate": 0.00029124886052871467,
      "loss": 0.8104,
      "step": 7775
    },
    {
      "epoch": 3.545630625498462,
      "grad_norm": 0.1759794056415558,
      "learning_rate": 0.00029079307201458523,
      "loss": 0.8342,
      "step": 7780
    },
    {
      "epoch": 3.5479093084197335,
      "grad_norm": 0.1684940755367279,
      "learning_rate": 0.0002903372835004558,
      "loss": 0.8326,
      "step": 7785
    },
    {
      "epoch": 3.550187991341005,
      "grad_norm": 0.16695864498615265,
      "learning_rate": 0.00028988149498632637,
      "loss": 0.8459,
      "step": 7790
    },
    {
      "epoch": 3.552466674262276,
      "grad_norm": 0.18834036588668823,
      "learning_rate": 0.0002894257064721969,
      "loss": 0.8366,
      "step": 7795
    },
    {
      "epoch": 3.554745357183548,
      "grad_norm": 0.19306346774101257,
      "learning_rate": 0.0002889699179580675,
      "loss": 0.8641,
      "step": 7800
    },
    {
      "epoch": 3.557024040104819,
      "grad_norm": 0.1545620560646057,
      "learning_rate": 0.000288514129443938,
      "loss": 0.7576,
      "step": 7805
    },
    {
      "epoch": 3.559302723026091,
      "grad_norm": 0.19025957584381104,
      "learning_rate": 0.0002880583409298086,
      "loss": 0.8973,
      "step": 7810
    },
    {
      "epoch": 3.5615814059473623,
      "grad_norm": 0.16747674345970154,
      "learning_rate": 0.0002876025524156791,
      "loss": 0.8175,
      "step": 7815
    },
    {
      "epoch": 3.563860088868634,
      "grad_norm": 0.16867539286613464,
      "learning_rate": 0.0002871467639015497,
      "loss": 0.8109,
      "step": 7820
    },
    {
      "epoch": 3.5661387717899053,
      "grad_norm": 0.16883184015750885,
      "learning_rate": 0.00028669097538742023,
      "loss": 0.8537,
      "step": 7825
    },
    {
      "epoch": 3.568417454711177,
      "grad_norm": 0.17939797043800354,
      "learning_rate": 0.0002862351868732908,
      "loss": 0.8474,
      "step": 7830
    },
    {
      "epoch": 3.5706961376324484,
      "grad_norm": 0.2229086309671402,
      "learning_rate": 0.00028577939835916136,
      "loss": 0.8225,
      "step": 7835
    },
    {
      "epoch": 3.57297482055372,
      "grad_norm": 0.1678701490163803,
      "learning_rate": 0.00028532360984503193,
      "loss": 0.8903,
      "step": 7840
    },
    {
      "epoch": 3.5752535034749915,
      "grad_norm": 0.1678543984889984,
      "learning_rate": 0.00028486782133090244,
      "loss": 0.8352,
      "step": 7845
    },
    {
      "epoch": 3.577532186396263,
      "grad_norm": 0.16523940861225128,
      "learning_rate": 0.000284412032816773,
      "loss": 0.8025,
      "step": 7850
    },
    {
      "epoch": 3.5798108693175346,
      "grad_norm": 0.17730800807476044,
      "learning_rate": 0.0002839562443026436,
      "loss": 0.8716,
      "step": 7855
    },
    {
      "epoch": 3.582089552238806,
      "grad_norm": 0.18037132918834686,
      "learning_rate": 0.00028350045578851414,
      "loss": 0.8375,
      "step": 7860
    },
    {
      "epoch": 3.5843682351600776,
      "grad_norm": 0.17629213631153107,
      "learning_rate": 0.0002830446672743847,
      "loss": 0.8384,
      "step": 7865
    },
    {
      "epoch": 3.5866469180813487,
      "grad_norm": 0.15369468927383423,
      "learning_rate": 0.0002825888787602552,
      "loss": 0.7884,
      "step": 7870
    },
    {
      "epoch": 3.5889256010026207,
      "grad_norm": 0.15805251896381378,
      "learning_rate": 0.00028213309024612584,
      "loss": 0.9267,
      "step": 7875
    },
    {
      "epoch": 3.591204283923892,
      "grad_norm": 0.17547616362571716,
      "learning_rate": 0.00028167730173199636,
      "loss": 0.852,
      "step": 7880
    },
    {
      "epoch": 3.5934829668451638,
      "grad_norm": 0.17309030890464783,
      "learning_rate": 0.0002812215132178669,
      "loss": 0.8176,
      "step": 7885
    },
    {
      "epoch": 3.595761649766435,
      "grad_norm": 0.1888445019721985,
      "learning_rate": 0.00028076572470373744,
      "loss": 0.803,
      "step": 7890
    },
    {
      "epoch": 3.5980403326877064,
      "grad_norm": 0.17961061000823975,
      "learning_rate": 0.00028030993618960806,
      "loss": 0.8795,
      "step": 7895
    },
    {
      "epoch": 3.600319015608978,
      "grad_norm": 0.1856195479631424,
      "learning_rate": 0.00027985414767547857,
      "loss": 0.8795,
      "step": 7900
    },
    {
      "epoch": 3.6025976985302495,
      "grad_norm": 0.18679280579090118,
      "learning_rate": 0.00027939835916134914,
      "loss": 0.8173,
      "step": 7905
    },
    {
      "epoch": 3.604876381451521,
      "grad_norm": 0.17084769904613495,
      "learning_rate": 0.00027894257064721965,
      "loss": 0.7887,
      "step": 7910
    },
    {
      "epoch": 3.6071550643727925,
      "grad_norm": 0.1826997697353363,
      "learning_rate": 0.00027848678213309027,
      "loss": 0.8213,
      "step": 7915
    },
    {
      "epoch": 3.609433747294064,
      "grad_norm": 0.1679689884185791,
      "learning_rate": 0.0002780309936189608,
      "loss": 0.8637,
      "step": 7920
    },
    {
      "epoch": 3.6117124302153356,
      "grad_norm": 0.18376709520816803,
      "learning_rate": 0.00027757520510483135,
      "loss": 0.8284,
      "step": 7925
    },
    {
      "epoch": 3.613991113136607,
      "grad_norm": 0.16097676753997803,
      "learning_rate": 0.0002771194165907019,
      "loss": 0.783,
      "step": 7930
    },
    {
      "epoch": 3.6162697960578787,
      "grad_norm": 0.16876955330371857,
      "learning_rate": 0.0002766636280765725,
      "loss": 0.787,
      "step": 7935
    },
    {
      "epoch": 3.61854847897915,
      "grad_norm": 0.20722417533397675,
      "learning_rate": 0.00027620783956244305,
      "loss": 0.8575,
      "step": 7940
    },
    {
      "epoch": 3.6208271619004213,
      "grad_norm": 0.19012098014354706,
      "learning_rate": 0.00027575205104831356,
      "loss": 0.8847,
      "step": 7945
    },
    {
      "epoch": 3.6231058448216933,
      "grad_norm": 0.19217821955680847,
      "learning_rate": 0.0002752962625341842,
      "loss": 0.885,
      "step": 7950
    },
    {
      "epoch": 3.6253845277429644,
      "grad_norm": 0.20706585049629211,
      "learning_rate": 0.0002748404740200547,
      "loss": 0.8664,
      "step": 7955
    },
    {
      "epoch": 3.627663210664236,
      "grad_norm": 0.1876341551542282,
      "learning_rate": 0.00027438468550592527,
      "loss": 0.8458,
      "step": 7960
    },
    {
      "epoch": 3.6299418935855075,
      "grad_norm": 0.188504159450531,
      "learning_rate": 0.0002739288969917958,
      "loss": 0.8762,
      "step": 7965
    },
    {
      "epoch": 3.632220576506779,
      "grad_norm": 0.2015724778175354,
      "learning_rate": 0.0002734731084776664,
      "loss": 0.8822,
      "step": 7970
    },
    {
      "epoch": 3.6344992594280505,
      "grad_norm": 0.18637846410274506,
      "learning_rate": 0.0002730173199635369,
      "loss": 0.8015,
      "step": 7975
    },
    {
      "epoch": 3.636777942349322,
      "grad_norm": 0.1691243201494217,
      "learning_rate": 0.0002725615314494075,
      "loss": 0.879,
      "step": 7980
    },
    {
      "epoch": 3.6390566252705936,
      "grad_norm": 0.19498766958713531,
      "learning_rate": 0.000272105742935278,
      "loss": 0.8843,
      "step": 7985
    },
    {
      "epoch": 3.641335308191865,
      "grad_norm": 0.17332491278648376,
      "learning_rate": 0.0002716499544211486,
      "loss": 0.8731,
      "step": 7990
    },
    {
      "epoch": 3.6436139911131367,
      "grad_norm": 0.18759846687316895,
      "learning_rate": 0.0002711941659070192,
      "loss": 0.8261,
      "step": 7995
    },
    {
      "epoch": 3.645892674034408,
      "grad_norm": 0.18666920065879822,
      "learning_rate": 0.0002707383773928897,
      "loss": 0.8522,
      "step": 8000
    },
    {
      "epoch": 3.6481713569556797,
      "grad_norm": 0.19045822322368622,
      "learning_rate": 0.00027028258887876026,
      "loss": 0.8824,
      "step": 8005
    },
    {
      "epoch": 3.6504500398769513,
      "grad_norm": 0.17267994582653046,
      "learning_rate": 0.00026982680036463083,
      "loss": 0.8552,
      "step": 8010
    },
    {
      "epoch": 3.652728722798223,
      "grad_norm": 0.19306449592113495,
      "learning_rate": 0.0002693710118505014,
      "loss": 0.87,
      "step": 8015
    },
    {
      "epoch": 3.655007405719494,
      "grad_norm": 0.169300839304924,
      "learning_rate": 0.0002689152233363719,
      "loss": 0.7964,
      "step": 8020
    },
    {
      "epoch": 3.657286088640766,
      "grad_norm": 0.19427992403507233,
      "learning_rate": 0.0002684594348222425,
      "loss": 0.8407,
      "step": 8025
    },
    {
      "epoch": 3.659564771562037,
      "grad_norm": 0.17119558155536652,
      "learning_rate": 0.00026800364630811304,
      "loss": 0.8216,
      "step": 8030
    },
    {
      "epoch": 3.6618434544833085,
      "grad_norm": 0.1836119443178177,
      "learning_rate": 0.0002675478577939836,
      "loss": 0.8186,
      "step": 8035
    },
    {
      "epoch": 3.66412213740458,
      "grad_norm": 0.18092674016952515,
      "learning_rate": 0.0002670920692798541,
      "loss": 0.8758,
      "step": 8040
    },
    {
      "epoch": 3.6664008203258516,
      "grad_norm": 0.1749640256166458,
      "learning_rate": 0.0002666362807657247,
      "loss": 0.7573,
      "step": 8045
    },
    {
      "epoch": 3.668679503247123,
      "grad_norm": 0.17482008039951324,
      "learning_rate": 0.00026618049225159526,
      "loss": 0.8075,
      "step": 8050
    },
    {
      "epoch": 3.6709581861683946,
      "grad_norm": 0.18441112339496613,
      "learning_rate": 0.0002657247037374658,
      "loss": 0.7149,
      "step": 8055
    },
    {
      "epoch": 3.673236869089666,
      "grad_norm": 0.1768266260623932,
      "learning_rate": 0.0002652689152233364,
      "loss": 0.8498,
      "step": 8060
    },
    {
      "epoch": 3.6755155520109377,
      "grad_norm": 0.1883305013179779,
      "learning_rate": 0.00026481312670920696,
      "loss": 0.8797,
      "step": 8065
    },
    {
      "epoch": 3.6777942349322092,
      "grad_norm": 0.15634243190288544,
      "learning_rate": 0.0002643573381950775,
      "loss": 0.8615,
      "step": 8070
    },
    {
      "epoch": 3.680072917853481,
      "grad_norm": 0.18401265144348145,
      "learning_rate": 0.00026390154968094804,
      "loss": 0.8146,
      "step": 8075
    },
    {
      "epoch": 3.6823516007747523,
      "grad_norm": 0.17973095178604126,
      "learning_rate": 0.0002634457611668186,
      "loss": 0.8297,
      "step": 8080
    },
    {
      "epoch": 3.684630283696024,
      "grad_norm": 0.2078225463628769,
      "learning_rate": 0.00026298997265268917,
      "loss": 0.8672,
      "step": 8085
    },
    {
      "epoch": 3.6869089666172954,
      "grad_norm": 0.17063632607460022,
      "learning_rate": 0.00026253418413855974,
      "loss": 0.9179,
      "step": 8090
    },
    {
      "epoch": 3.6891876495385665,
      "grad_norm": 0.18659284710884094,
      "learning_rate": 0.00026207839562443025,
      "loss": 0.9327,
      "step": 8095
    },
    {
      "epoch": 3.6914663324598385,
      "grad_norm": 0.18236403167247772,
      "learning_rate": 0.0002616226071103008,
      "loss": 0.9112,
      "step": 8100
    },
    {
      "epoch": 3.6937450153811096,
      "grad_norm": 0.1764300912618637,
      "learning_rate": 0.0002611668185961714,
      "loss": 0.8502,
      "step": 8105
    },
    {
      "epoch": 3.696023698302381,
      "grad_norm": 0.18083027005195618,
      "learning_rate": 0.00026071103008204195,
      "loss": 0.8338,
      "step": 8110
    },
    {
      "epoch": 3.6983023812236526,
      "grad_norm": 0.1589631587266922,
      "learning_rate": 0.00026025524156791246,
      "loss": 0.82,
      "step": 8115
    },
    {
      "epoch": 3.700581064144924,
      "grad_norm": 0.1834665685892105,
      "learning_rate": 0.00025979945305378303,
      "loss": 0.7795,
      "step": 8120
    },
    {
      "epoch": 3.7028597470661957,
      "grad_norm": 0.16263988614082336,
      "learning_rate": 0.0002593436645396536,
      "loss": 0.8348,
      "step": 8125
    },
    {
      "epoch": 3.7051384299874672,
      "grad_norm": 0.18269987404346466,
      "learning_rate": 0.00025888787602552417,
      "loss": 0.8769,
      "step": 8130
    },
    {
      "epoch": 3.7074171129087388,
      "grad_norm": 0.16722466051578522,
      "learning_rate": 0.00025843208751139473,
      "loss": 0.8391,
      "step": 8135
    },
    {
      "epoch": 3.7096957958300103,
      "grad_norm": 0.20195002853870392,
      "learning_rate": 0.00025797629899726524,
      "loss": 0.8028,
      "step": 8140
    },
    {
      "epoch": 3.711974478751282,
      "grad_norm": 0.20108185708522797,
      "learning_rate": 0.00025752051048313587,
      "loss": 0.7925,
      "step": 8145
    },
    {
      "epoch": 3.7142531616725534,
      "grad_norm": 0.1528719663619995,
      "learning_rate": 0.0002570647219690064,
      "loss": 0.8656,
      "step": 8150
    },
    {
      "epoch": 3.716531844593825,
      "grad_norm": 0.198292076587677,
      "learning_rate": 0.00025660893345487695,
      "loss": 0.8188,
      "step": 8155
    },
    {
      "epoch": 3.718810527515096,
      "grad_norm": 0.1598397195339203,
      "learning_rate": 0.00025615314494074746,
      "loss": 0.7793,
      "step": 8160
    },
    {
      "epoch": 3.721089210436368,
      "grad_norm": 0.19170287251472473,
      "learning_rate": 0.0002556973564266181,
      "loss": 0.79,
      "step": 8165
    },
    {
      "epoch": 3.723367893357639,
      "grad_norm": 0.19286800920963287,
      "learning_rate": 0.0002552415679124886,
      "loss": 0.8081,
      "step": 8170
    },
    {
      "epoch": 3.725646576278911,
      "grad_norm": 0.16804996132850647,
      "learning_rate": 0.00025478577939835916,
      "loss": 0.8159,
      "step": 8175
    },
    {
      "epoch": 3.727925259200182,
      "grad_norm": 0.18083879351615906,
      "learning_rate": 0.0002543299908842297,
      "loss": 0.8788,
      "step": 8180
    },
    {
      "epoch": 3.7302039421214537,
      "grad_norm": 0.19202920794487,
      "learning_rate": 0.0002538742023701003,
      "loss": 0.8569,
      "step": 8185
    },
    {
      "epoch": 3.732482625042725,
      "grad_norm": 0.18852154910564423,
      "learning_rate": 0.0002534184138559708,
      "loss": 0.8328,
      "step": 8190
    },
    {
      "epoch": 3.7347613079639967,
      "grad_norm": 0.18698497116565704,
      "learning_rate": 0.0002529626253418414,
      "loss": 0.8843,
      "step": 8195
    },
    {
      "epoch": 3.7370399908852683,
      "grad_norm": 0.16052554547786713,
      "learning_rate": 0.000252506836827712,
      "loss": 0.8322,
      "step": 8200
    },
    {
      "epoch": 3.73931867380654,
      "grad_norm": 0.18194729089736938,
      "learning_rate": 0.0002520510483135825,
      "loss": 0.8408,
      "step": 8205
    },
    {
      "epoch": 3.7415973567278114,
      "grad_norm": 0.18190599977970123,
      "learning_rate": 0.0002515952597994531,
      "loss": 0.848,
      "step": 8210
    },
    {
      "epoch": 3.743876039649083,
      "grad_norm": 0.1879444122314453,
      "learning_rate": 0.0002511394712853236,
      "loss": 0.848,
      "step": 8215
    },
    {
      "epoch": 3.7461547225703544,
      "grad_norm": 0.16597680747509003,
      "learning_rate": 0.0002506836827711942,
      "loss": 0.8609,
      "step": 8220
    },
    {
      "epoch": 3.748433405491626,
      "grad_norm": 0.17286206781864166,
      "learning_rate": 0.0002502278942570647,
      "loss": 0.8506,
      "step": 8225
    },
    {
      "epoch": 3.7507120884128975,
      "grad_norm": 0.18784482777118683,
      "learning_rate": 0.0002497721057429353,
      "loss": 0.7439,
      "step": 8230
    },
    {
      "epoch": 3.7529907713341686,
      "grad_norm": 0.1842438131570816,
      "learning_rate": 0.00024931631722880586,
      "loss": 0.8722,
      "step": 8235
    },
    {
      "epoch": 3.7552694542554406,
      "grad_norm": 0.19690805673599243,
      "learning_rate": 0.0002488605287146764,
      "loss": 0.8795,
      "step": 8240
    },
    {
      "epoch": 3.7575481371767117,
      "grad_norm": 0.21892353892326355,
      "learning_rate": 0.00024840474020054694,
      "loss": 0.9107,
      "step": 8245
    },
    {
      "epoch": 3.7598268200979836,
      "grad_norm": 0.19650040566921234,
      "learning_rate": 0.0002479489516864175,
      "loss": 0.8689,
      "step": 8250
    },
    {
      "epoch": 3.7621055030192547,
      "grad_norm": 0.18089176714420319,
      "learning_rate": 0.00024749316317228807,
      "loss": 0.8282,
      "step": 8255
    },
    {
      "epoch": 3.7643841859405263,
      "grad_norm": 0.18270553648471832,
      "learning_rate": 0.00024703737465815864,
      "loss": 0.8427,
      "step": 8260
    },
    {
      "epoch": 3.766662868861798,
      "grad_norm": 0.18771220743656158,
      "learning_rate": 0.00024658158614402915,
      "loss": 0.8951,
      "step": 8265
    },
    {
      "epoch": 3.7689415517830693,
      "grad_norm": 0.19755645096302032,
      "learning_rate": 0.0002461257976298997,
      "loss": 0.8554,
      "step": 8270
    },
    {
      "epoch": 3.771220234704341,
      "grad_norm": 0.18742850422859192,
      "learning_rate": 0.0002456700091157703,
      "loss": 0.7788,
      "step": 8275
    },
    {
      "epoch": 3.7734989176256124,
      "grad_norm": 0.16801556944847107,
      "learning_rate": 0.00024521422060164085,
      "loss": 0.8636,
      "step": 8280
    },
    {
      "epoch": 3.775777600546884,
      "grad_norm": 0.17455226182937622,
      "learning_rate": 0.00024475843208751136,
      "loss": 0.8324,
      "step": 8285
    },
    {
      "epoch": 3.7780562834681555,
      "grad_norm": 0.18022027611732483,
      "learning_rate": 0.00024430264357338193,
      "loss": 0.8152,
      "step": 8290
    },
    {
      "epoch": 3.780334966389427,
      "grad_norm": 0.1536175012588501,
      "learning_rate": 0.0002438468550592525,
      "loss": 0.8442,
      "step": 8295
    },
    {
      "epoch": 3.7826136493106985,
      "grad_norm": 0.16683083772659302,
      "learning_rate": 0.0002433910665451231,
      "loss": 0.844,
      "step": 8300
    },
    {
      "epoch": 3.78489233223197,
      "grad_norm": 0.16978566348552704,
      "learning_rate": 0.00024293527803099363,
      "loss": 0.7587,
      "step": 8305
    },
    {
      "epoch": 3.787171015153241,
      "grad_norm": 0.17689189314842224,
      "learning_rate": 0.0002424794895168642,
      "loss": 0.834,
      "step": 8310
    },
    {
      "epoch": 3.789449698074513,
      "grad_norm": 0.1837530881166458,
      "learning_rate": 0.00024202370100273474,
      "loss": 0.8944,
      "step": 8315
    },
    {
      "epoch": 3.7917283809957842,
      "grad_norm": 0.17141003906726837,
      "learning_rate": 0.0002415679124886053,
      "loss": 0.8011,
      "step": 8320
    },
    {
      "epoch": 3.7940070639170558,
      "grad_norm": 0.1988060176372528,
      "learning_rate": 0.00024111212397447585,
      "loss": 0.7937,
      "step": 8325
    },
    {
      "epoch": 3.7962857468383273,
      "grad_norm": 0.1942230612039566,
      "learning_rate": 0.0002406563354603464,
      "loss": 0.8853,
      "step": 8330
    },
    {
      "epoch": 3.798564429759599,
      "grad_norm": 0.16461649537086487,
      "learning_rate": 0.00024020054694621695,
      "loss": 0.8618,
      "step": 8335
    },
    {
      "epoch": 3.8008431126808704,
      "grad_norm": 0.16504935920238495,
      "learning_rate": 0.00023974475843208752,
      "loss": 0.8646,
      "step": 8340
    },
    {
      "epoch": 3.803121795602142,
      "grad_norm": 0.1696190983057022,
      "learning_rate": 0.00023928896991795806,
      "loss": 0.8479,
      "step": 8345
    },
    {
      "epoch": 3.8054004785234135,
      "grad_norm": 0.16833461821079254,
      "learning_rate": 0.00023883318140382863,
      "loss": 0.8384,
      "step": 8350
    },
    {
      "epoch": 3.807679161444685,
      "grad_norm": 0.18702462315559387,
      "learning_rate": 0.00023837739288969917,
      "loss": 0.8467,
      "step": 8355
    },
    {
      "epoch": 3.8099578443659565,
      "grad_norm": 0.17367692291736603,
      "learning_rate": 0.00023792160437556973,
      "loss": 0.7936,
      "step": 8360
    },
    {
      "epoch": 3.812236527287228,
      "grad_norm": 0.17527440190315247,
      "learning_rate": 0.0002374658158614403,
      "loss": 0.8296,
      "step": 8365
    },
    {
      "epoch": 3.8145152102084996,
      "grad_norm": 0.20611771941184998,
      "learning_rate": 0.00023701002734731087,
      "loss": 0.8573,
      "step": 8370
    },
    {
      "epoch": 3.816793893129771,
      "grad_norm": 0.1805962175130844,
      "learning_rate": 0.0002365542388331814,
      "loss": 0.8748,
      "step": 8375
    },
    {
      "epoch": 3.8190725760510427,
      "grad_norm": 0.17390696704387665,
      "learning_rate": 0.00023609845031905197,
      "loss": 0.8965,
      "step": 8380
    },
    {
      "epoch": 3.8213512589723138,
      "grad_norm": 0.17227798700332642,
      "learning_rate": 0.00023564266180492251,
      "loss": 0.8242,
      "step": 8385
    },
    {
      "epoch": 3.8236299418935857,
      "grad_norm": 0.17917434871196747,
      "learning_rate": 0.00023518687329079308,
      "loss": 0.8172,
      "step": 8390
    },
    {
      "epoch": 3.825908624814857,
      "grad_norm": 0.1735120415687561,
      "learning_rate": 0.00023473108477666362,
      "loss": 0.8149,
      "step": 8395
    },
    {
      "epoch": 3.8281873077361284,
      "grad_norm": 0.1998315453529358,
      "learning_rate": 0.0002342752962625342,
      "loss": 0.8723,
      "step": 8400
    },
    {
      "epoch": 3.8304659906574,
      "grad_norm": 0.1633707731962204,
      "learning_rate": 0.00023381950774840476,
      "loss": 0.8408,
      "step": 8405
    },
    {
      "epoch": 3.8327446735786714,
      "grad_norm": 0.19647769629955292,
      "learning_rate": 0.0002333637192342753,
      "loss": 0.8348,
      "step": 8410
    },
    {
      "epoch": 3.835023356499943,
      "grad_norm": 0.1923302412033081,
      "learning_rate": 0.00023290793072014586,
      "loss": 0.8323,
      "step": 8415
    },
    {
      "epoch": 3.8373020394212145,
      "grad_norm": 0.19160564243793488,
      "learning_rate": 0.0002324521422060164,
      "loss": 0.8532,
      "step": 8420
    },
    {
      "epoch": 3.839580722342486,
      "grad_norm": 0.1830836832523346,
      "learning_rate": 0.00023199635369188697,
      "loss": 0.8911,
      "step": 8425
    },
    {
      "epoch": 3.8418594052637576,
      "grad_norm": 0.1723337024450302,
      "learning_rate": 0.0002315405651777575,
      "loss": 0.8199,
      "step": 8430
    },
    {
      "epoch": 3.844138088185029,
      "grad_norm": 0.17885075509548187,
      "learning_rate": 0.0002310847766636281,
      "loss": 0.8305,
      "step": 8435
    },
    {
      "epoch": 3.8464167711063006,
      "grad_norm": 0.19725334644317627,
      "learning_rate": 0.00023062898814949864,
      "loss": 0.9317,
      "step": 8440
    },
    {
      "epoch": 3.848695454027572,
      "grad_norm": 0.1818491667509079,
      "learning_rate": 0.0002301731996353692,
      "loss": 0.8302,
      "step": 8445
    },
    {
      "epoch": 3.8509741369488437,
      "grad_norm": 0.16033047437667847,
      "learning_rate": 0.00022971741112123975,
      "loss": 0.8418,
      "step": 8450
    },
    {
      "epoch": 3.8532528198701153,
      "grad_norm": 0.1593276858329773,
      "learning_rate": 0.00022926162260711032,
      "loss": 0.7911,
      "step": 8455
    },
    {
      "epoch": 3.8555315027913863,
      "grad_norm": 0.1655227541923523,
      "learning_rate": 0.00022880583409298086,
      "loss": 0.8377,
      "step": 8460
    },
    {
      "epoch": 3.8578101857126583,
      "grad_norm": 0.18833346664905548,
      "learning_rate": 0.00022835004557885142,
      "loss": 0.9005,
      "step": 8465
    },
    {
      "epoch": 3.8600888686339294,
      "grad_norm": 0.19594430923461914,
      "learning_rate": 0.00022789425706472196,
      "loss": 0.8207,
      "step": 8470
    },
    {
      "epoch": 3.862367551555201,
      "grad_norm": 0.17661471664905548,
      "learning_rate": 0.00022743846855059253,
      "loss": 0.8343,
      "step": 8475
    },
    {
      "epoch": 3.8646462344764725,
      "grad_norm": 0.20816302299499512,
      "learning_rate": 0.00022698268003646307,
      "loss": 0.918,
      "step": 8480
    },
    {
      "epoch": 3.866924917397744,
      "grad_norm": 0.17894767224788666,
      "learning_rate": 0.00022652689152233364,
      "loss": 0.8742,
      "step": 8485
    },
    {
      "epoch": 3.8692036003190156,
      "grad_norm": 0.23167183995246887,
      "learning_rate": 0.00022607110300820418,
      "loss": 0.8582,
      "step": 8490
    },
    {
      "epoch": 3.871482283240287,
      "grad_norm": 0.16146284341812134,
      "learning_rate": 0.00022561531449407474,
      "loss": 0.7865,
      "step": 8495
    },
    {
      "epoch": 3.8737609661615586,
      "grad_norm": 0.1646948903799057,
      "learning_rate": 0.0002251595259799453,
      "loss": 0.8745,
      "step": 8500
    },
    {
      "epoch": 3.87603964908283,
      "grad_norm": 0.18733786046504974,
      "learning_rate": 0.00022470373746581588,
      "loss": 0.8054,
      "step": 8505
    },
    {
      "epoch": 3.8783183320041017,
      "grad_norm": 0.17528021335601807,
      "learning_rate": 0.00022424794895168642,
      "loss": 0.7669,
      "step": 8510
    },
    {
      "epoch": 3.8805970149253732,
      "grad_norm": 0.20528219640254974,
      "learning_rate": 0.00022379216043755699,
      "loss": 0.8191,
      "step": 8515
    },
    {
      "epoch": 3.8828756978466448,
      "grad_norm": 0.16978628933429718,
      "learning_rate": 0.00022333637192342755,
      "loss": 0.7856,
      "step": 8520
    },
    {
      "epoch": 3.8851543807679163,
      "grad_norm": 0.18069742619991302,
      "learning_rate": 0.0002228805834092981,
      "loss": 0.7958,
      "step": 8525
    },
    {
      "epoch": 3.887433063689188,
      "grad_norm": 0.18486732244491577,
      "learning_rate": 0.00022242479489516866,
      "loss": 0.8037,
      "step": 8530
    },
    {
      "epoch": 3.889711746610459,
      "grad_norm": 0.15001651644706726,
      "learning_rate": 0.0002219690063810392,
      "loss": 0.8136,
      "step": 8535
    },
    {
      "epoch": 3.891990429531731,
      "grad_norm": 0.1834731549024582,
      "learning_rate": 0.00022151321786690977,
      "loss": 0.8613,
      "step": 8540
    },
    {
      "epoch": 3.894269112453002,
      "grad_norm": 0.1797124445438385,
      "learning_rate": 0.0002210574293527803,
      "loss": 0.8056,
      "step": 8545
    },
    {
      "epoch": 3.8965477953742735,
      "grad_norm": 0.18840685486793518,
      "learning_rate": 0.00022060164083865087,
      "loss": 0.8657,
      "step": 8550
    },
    {
      "epoch": 3.898826478295545,
      "grad_norm": 0.17756539583206177,
      "learning_rate": 0.0002201458523245214,
      "loss": 0.8439,
      "step": 8555
    },
    {
      "epoch": 3.9011051612168166,
      "grad_norm": 0.17939543724060059,
      "learning_rate": 0.00021969006381039198,
      "loss": 0.8523,
      "step": 8560
    },
    {
      "epoch": 3.903383844138088,
      "grad_norm": 0.1767106056213379,
      "learning_rate": 0.00021923427529626252,
      "loss": 0.841,
      "step": 8565
    },
    {
      "epoch": 3.9056625270593597,
      "grad_norm": 0.16926240921020508,
      "learning_rate": 0.00021877848678213311,
      "loss": 0.8737,
      "step": 8570
    },
    {
      "epoch": 3.907941209980631,
      "grad_norm": 0.20402567088603973,
      "learning_rate": 0.00021832269826800365,
      "loss": 0.8468,
      "step": 8575
    },
    {
      "epoch": 3.9102198929019027,
      "grad_norm": 0.19919316470623016,
      "learning_rate": 0.00021786690975387422,
      "loss": 0.8239,
      "step": 8580
    },
    {
      "epoch": 3.9124985758231743,
      "grad_norm": 0.17336805164813995,
      "learning_rate": 0.00021741112123974476,
      "loss": 0.851,
      "step": 8585
    },
    {
      "epoch": 3.914777258744446,
      "grad_norm": 0.18800006806850433,
      "learning_rate": 0.00021695533272561533,
      "loss": 0.8872,
      "step": 8590
    },
    {
      "epoch": 3.9170559416657174,
      "grad_norm": 0.16579197347164154,
      "learning_rate": 0.00021649954421148587,
      "loss": 0.8605,
      "step": 8595
    },
    {
      "epoch": 3.9193346245869884,
      "grad_norm": 0.1848798543214798,
      "learning_rate": 0.00021604375569735644,
      "loss": 0.8122,
      "step": 8600
    },
    {
      "epoch": 3.9216133075082604,
      "grad_norm": 0.1715572476387024,
      "learning_rate": 0.00021558796718322698,
      "loss": 0.8867,
      "step": 8605
    },
    {
      "epoch": 3.9238919904295315,
      "grad_norm": 0.17736175656318665,
      "learning_rate": 0.00021513217866909754,
      "loss": 0.8934,
      "step": 8610
    },
    {
      "epoch": 3.9261706733508035,
      "grad_norm": 0.1863432377576828,
      "learning_rate": 0.00021467639015496808,
      "loss": 0.8369,
      "step": 8615
    },
    {
      "epoch": 3.9284493562720746,
      "grad_norm": 0.16865292191505432,
      "learning_rate": 0.00021422060164083865,
      "loss": 0.8387,
      "step": 8620
    },
    {
      "epoch": 3.930728039193346,
      "grad_norm": 0.18354590237140656,
      "learning_rate": 0.0002137648131267092,
      "loss": 0.8289,
      "step": 8625
    },
    {
      "epoch": 3.9330067221146177,
      "grad_norm": 0.1884162724018097,
      "learning_rate": 0.00021330902461257976,
      "loss": 0.8699,
      "step": 8630
    },
    {
      "epoch": 3.935285405035889,
      "grad_norm": 0.17863315343856812,
      "learning_rate": 0.00021285323609845035,
      "loss": 0.8395,
      "step": 8635
    },
    {
      "epoch": 3.9375640879571607,
      "grad_norm": 0.18130315840244293,
      "learning_rate": 0.0002123974475843209,
      "loss": 0.835,
      "step": 8640
    },
    {
      "epoch": 3.9398427708784323,
      "grad_norm": 0.1916694939136505,
      "learning_rate": 0.00021194165907019146,
      "loss": 0.8441,
      "step": 8645
    },
    {
      "epoch": 3.942121453799704,
      "grad_norm": 0.18689808249473572,
      "learning_rate": 0.000211485870556062,
      "loss": 0.8741,
      "step": 8650
    },
    {
      "epoch": 3.9444001367209753,
      "grad_norm": 0.17851081490516663,
      "learning_rate": 0.00021103008204193256,
      "loss": 0.8529,
      "step": 8655
    },
    {
      "epoch": 3.946678819642247,
      "grad_norm": 0.15438543260097504,
      "learning_rate": 0.0002105742935278031,
      "loss": 0.8009,
      "step": 8660
    },
    {
      "epoch": 3.9489575025635184,
      "grad_norm": 0.17360129952430725,
      "learning_rate": 0.00021011850501367367,
      "loss": 0.8175,
      "step": 8665
    },
    {
      "epoch": 3.95123618548479,
      "grad_norm": 0.19002574682235718,
      "learning_rate": 0.0002096627164995442,
      "loss": 0.7987,
      "step": 8670
    },
    {
      "epoch": 3.953514868406061,
      "grad_norm": 0.16407836973667145,
      "learning_rate": 0.00020920692798541478,
      "loss": 0.7581,
      "step": 8675
    },
    {
      "epoch": 3.955793551327333,
      "grad_norm": 0.18114429712295532,
      "learning_rate": 0.00020875113947128532,
      "loss": 0.8476,
      "step": 8680
    },
    {
      "epoch": 3.958072234248604,
      "grad_norm": 0.16951987147331238,
      "learning_rate": 0.00020829535095715588,
      "loss": 0.8828,
      "step": 8685
    },
    {
      "epoch": 3.960350917169876,
      "grad_norm": 0.19870038330554962,
      "learning_rate": 0.00020783956244302642,
      "loss": 0.8599,
      "step": 8690
    },
    {
      "epoch": 3.962629600091147,
      "grad_norm": 0.16787981986999512,
      "learning_rate": 0.000207383773928897,
      "loss": 0.7963,
      "step": 8695
    },
    {
      "epoch": 3.9649082830124187,
      "grad_norm": 0.15915459394454956,
      "learning_rate": 0.00020692798541476753,
      "loss": 0.8134,
      "step": 8700
    },
    {
      "epoch": 3.9671869659336902,
      "grad_norm": 0.19632971286773682,
      "learning_rate": 0.00020647219690063813,
      "loss": 0.8038,
      "step": 8705
    },
    {
      "epoch": 3.969465648854962,
      "grad_norm": 0.1777215451002121,
      "learning_rate": 0.00020601640838650867,
      "loss": 0.8704,
      "step": 8710
    },
    {
      "epoch": 3.9717443317762333,
      "grad_norm": 0.1760595440864563,
      "learning_rate": 0.00020556061987237923,
      "loss": 0.8178,
      "step": 8715
    },
    {
      "epoch": 3.974023014697505,
      "grad_norm": 0.1871679723262787,
      "learning_rate": 0.00020510483135824977,
      "loss": 0.8151,
      "step": 8720
    },
    {
      "epoch": 3.9763016976187764,
      "grad_norm": 0.17749474942684174,
      "learning_rate": 0.00020464904284412034,
      "loss": 0.8786,
      "step": 8725
    },
    {
      "epoch": 3.978580380540048,
      "grad_norm": 0.1838557869195938,
      "learning_rate": 0.00020419325432999088,
      "loss": 0.8288,
      "step": 8730
    },
    {
      "epoch": 3.9808590634613195,
      "grad_norm": 0.18142004311084747,
      "learning_rate": 0.00020373746581586145,
      "loss": 0.8082,
      "step": 8735
    },
    {
      "epoch": 3.983137746382591,
      "grad_norm": 0.16987760365009308,
      "learning_rate": 0.000203281677301732,
      "loss": 0.8127,
      "step": 8740
    },
    {
      "epoch": 3.9854164293038625,
      "grad_norm": 0.1809203028678894,
      "learning_rate": 0.00020282588878760255,
      "loss": 0.7871,
      "step": 8745
    },
    {
      "epoch": 3.9876951122251336,
      "grad_norm": 0.18199078738689423,
      "learning_rate": 0.0002023701002734731,
      "loss": 0.8528,
      "step": 8750
    },
    {
      "epoch": 3.9899737951464056,
      "grad_norm": 0.19411638379096985,
      "learning_rate": 0.00020191431175934366,
      "loss": 0.8366,
      "step": 8755
    },
    {
      "epoch": 3.9922524780676767,
      "grad_norm": 0.19235068559646606,
      "learning_rate": 0.00020145852324521423,
      "loss": 0.8552,
      "step": 8760
    },
    {
      "epoch": 3.9945311609889482,
      "grad_norm": 0.18731968104839325,
      "learning_rate": 0.00020100273473108477,
      "loss": 0.8584,
      "step": 8765
    },
    {
      "epoch": 3.9968098439102198,
      "grad_norm": 0.20107612013816833,
      "learning_rate": 0.00020054694621695536,
      "loss": 0.8262,
      "step": 8770
    },
    {
      "epoch": 3.9990885268314913,
      "grad_norm": 0.18418264389038086,
      "learning_rate": 0.0002000911577028259,
      "loss": 0.8263,
      "step": 8775
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.7470741868019104,
      "eval_runtime": 794.6309,
      "eval_samples_per_second": 25.244,
      "eval_steps_per_second": 3.156,
      "step": 8777
    },
    {
      "epoch": 4.001367209752763,
      "grad_norm": 0.19029752910137177,
      "learning_rate": 0.00019963536918869647,
      "loss": 0.8498,
      "step": 8780
    },
    {
      "epoch": 4.003645892674035,
      "grad_norm": 0.1741892546415329,
      "learning_rate": 0.000199179580674567,
      "loss": 0.8553,
      "step": 8785
    },
    {
      "epoch": 4.005924575595306,
      "grad_norm": 0.16881954669952393,
      "learning_rate": 0.00019872379216043758,
      "loss": 0.8191,
      "step": 8790
    },
    {
      "epoch": 4.008203258516577,
      "grad_norm": 0.1746521294116974,
      "learning_rate": 0.00019826800364630812,
      "loss": 0.8152,
      "step": 8795
    },
    {
      "epoch": 4.010481941437849,
      "grad_norm": 0.19937977194786072,
      "learning_rate": 0.00019781221513217868,
      "loss": 0.865,
      "step": 8800
    },
    {
      "epoch": 4.01276062435912,
      "grad_norm": 0.19551680982112885,
      "learning_rate": 0.00019735642661804922,
      "loss": 0.813,
      "step": 8805
    },
    {
      "epoch": 4.015039307280392,
      "grad_norm": 0.17694947123527527,
      "learning_rate": 0.0001969006381039198,
      "loss": 0.9395,
      "step": 8810
    },
    {
      "epoch": 4.017317990201663,
      "grad_norm": 0.18587321043014526,
      "learning_rate": 0.00019644484958979033,
      "loss": 0.8239,
      "step": 8815
    },
    {
      "epoch": 4.019596673122935,
      "grad_norm": 0.1764264851808548,
      "learning_rate": 0.0001959890610756609,
      "loss": 0.9215,
      "step": 8820
    },
    {
      "epoch": 4.021875356044206,
      "grad_norm": 0.1977161020040512,
      "learning_rate": 0.00019553327256153144,
      "loss": 0.84,
      "step": 8825
    },
    {
      "epoch": 4.024154038965478,
      "grad_norm": 0.16781243681907654,
      "learning_rate": 0.000195077484047402,
      "loss": 0.7973,
      "step": 8830
    },
    {
      "epoch": 4.026432721886749,
      "grad_norm": 0.18611571192741394,
      "learning_rate": 0.00019462169553327254,
      "loss": 0.8111,
      "step": 8835
    },
    {
      "epoch": 4.028711404808021,
      "grad_norm": 0.17526650428771973,
      "learning_rate": 0.00019416590701914314,
      "loss": 0.8327,
      "step": 8840
    },
    {
      "epoch": 4.030990087729292,
      "grad_norm": 0.16238844394683838,
      "learning_rate": 0.00019371011850501368,
      "loss": 0.8048,
      "step": 8845
    },
    {
      "epoch": 4.033268770650564,
      "grad_norm": 0.19330525398254395,
      "learning_rate": 0.00019325432999088424,
      "loss": 0.8057,
      "step": 8850
    },
    {
      "epoch": 4.035547453571835,
      "grad_norm": 0.1784447878599167,
      "learning_rate": 0.00019279854147675478,
      "loss": 0.8285,
      "step": 8855
    },
    {
      "epoch": 4.037826136493107,
      "grad_norm": 0.17095302045345306,
      "learning_rate": 0.00019234275296262535,
      "loss": 0.8466,
      "step": 8860
    },
    {
      "epoch": 4.0401048194143785,
      "grad_norm": 0.18690873682498932,
      "learning_rate": 0.0001918869644484959,
      "loss": 0.8371,
      "step": 8865
    },
    {
      "epoch": 4.04238350233565,
      "grad_norm": 0.18869268894195557,
      "learning_rate": 0.00019143117593436646,
      "loss": 0.8817,
      "step": 8870
    },
    {
      "epoch": 4.044662185256922,
      "grad_norm": 0.18550221621990204,
      "learning_rate": 0.00019097538742023703,
      "loss": 0.7649,
      "step": 8875
    },
    {
      "epoch": 4.046940868178193,
      "grad_norm": 0.18718981742858887,
      "learning_rate": 0.00019051959890610757,
      "loss": 0.8657,
      "step": 8880
    },
    {
      "epoch": 4.049219551099465,
      "grad_norm": 0.1828107386827469,
      "learning_rate": 0.00019006381039197813,
      "loss": 0.7818,
      "step": 8885
    },
    {
      "epoch": 4.051498234020736,
      "grad_norm": 0.185623437166214,
      "learning_rate": 0.00018960802187784867,
      "loss": 0.8463,
      "step": 8890
    },
    {
      "epoch": 4.053776916942008,
      "grad_norm": 0.18279500305652618,
      "learning_rate": 0.00018915223336371924,
      "loss": 0.7967,
      "step": 8895
    },
    {
      "epoch": 4.056055599863279,
      "grad_norm": 0.17919620871543884,
      "learning_rate": 0.00018869644484958978,
      "loss": 0.8456,
      "step": 8900
    },
    {
      "epoch": 4.058334282784551,
      "grad_norm": 0.21470920741558075,
      "learning_rate": 0.00018824065633546037,
      "loss": 0.8398,
      "step": 8905
    },
    {
      "epoch": 4.060612965705822,
      "grad_norm": 0.2103222757577896,
      "learning_rate": 0.0001877848678213309,
      "loss": 0.8583,
      "step": 8910
    },
    {
      "epoch": 4.062891648627094,
      "grad_norm": 0.17209629714488983,
      "learning_rate": 0.00018732907930720148,
      "loss": 0.8207,
      "step": 8915
    },
    {
      "epoch": 4.065170331548365,
      "grad_norm": 0.18474876880645752,
      "learning_rate": 0.00018687329079307202,
      "loss": 0.8549,
      "step": 8920
    },
    {
      "epoch": 4.067449014469637,
      "grad_norm": 0.20275257527828217,
      "learning_rate": 0.0001864175022789426,
      "loss": 0.8235,
      "step": 8925
    },
    {
      "epoch": 4.069727697390908,
      "grad_norm": 0.18469376862049103,
      "learning_rate": 0.00018596171376481313,
      "loss": 0.881,
      "step": 8930
    },
    {
      "epoch": 4.07200638031218,
      "grad_norm": 0.1805209070444107,
      "learning_rate": 0.0001855059252506837,
      "loss": 0.8962,
      "step": 8935
    },
    {
      "epoch": 4.074285063233451,
      "grad_norm": 0.20800606906414032,
      "learning_rate": 0.00018505013673655423,
      "loss": 0.7649,
      "step": 8940
    },
    {
      "epoch": 4.076563746154722,
      "grad_norm": 0.17061437666416168,
      "learning_rate": 0.0001845943482224248,
      "loss": 0.8146,
      "step": 8945
    },
    {
      "epoch": 4.078842429075994,
      "grad_norm": 0.17930731177330017,
      "learning_rate": 0.00018413855970829534,
      "loss": 0.8125,
      "step": 8950
    },
    {
      "epoch": 4.081121111997265,
      "grad_norm": 0.1793581247329712,
      "learning_rate": 0.0001836827711941659,
      "loss": 0.803,
      "step": 8955
    },
    {
      "epoch": 4.083399794918537,
      "grad_norm": 0.1966649889945984,
      "learning_rate": 0.00018322698268003645,
      "loss": 0.8737,
      "step": 8960
    },
    {
      "epoch": 4.085678477839808,
      "grad_norm": 0.17808865010738373,
      "learning_rate": 0.00018277119416590701,
      "loss": 0.8502,
      "step": 8965
    },
    {
      "epoch": 4.08795716076108,
      "grad_norm": 0.19870468974113464,
      "learning_rate": 0.00018231540565177755,
      "loss": 0.8535,
      "step": 8970
    },
    {
      "epoch": 4.090235843682351,
      "grad_norm": 0.18786121904850006,
      "learning_rate": 0.00018185961713764815,
      "loss": 0.852,
      "step": 8975
    },
    {
      "epoch": 4.092514526603623,
      "grad_norm": 0.16580237448215485,
      "learning_rate": 0.0001814038286235187,
      "loss": 0.8127,
      "step": 8980
    },
    {
      "epoch": 4.0947932095248945,
      "grad_norm": 0.1795717179775238,
      "learning_rate": 0.00018094804010938926,
      "loss": 0.8306,
      "step": 8985
    },
    {
      "epoch": 4.097071892446166,
      "grad_norm": 0.22282817959785461,
      "learning_rate": 0.00018049225159525982,
      "loss": 0.8237,
      "step": 8990
    },
    {
      "epoch": 4.0993505753674375,
      "grad_norm": 0.18505170941352844,
      "learning_rate": 0.00018003646308113036,
      "loss": 0.7678,
      "step": 8995
    },
    {
      "epoch": 4.1016292582887095,
      "grad_norm": 0.18350151181221008,
      "learning_rate": 0.00017958067456700093,
      "loss": 0.8127,
      "step": 9000
    },
    {
      "epoch": 4.103907941209981,
      "grad_norm": 0.17333126068115234,
      "learning_rate": 0.00017912488605287147,
      "loss": 0.7861,
      "step": 9005
    },
    {
      "epoch": 4.106186624131253,
      "grad_norm": 0.18199901282787323,
      "learning_rate": 0.00017866909753874204,
      "loss": 0.8759,
      "step": 9010
    },
    {
      "epoch": 4.108465307052524,
      "grad_norm": 0.1730213612318039,
      "learning_rate": 0.00017821330902461258,
      "loss": 0.8005,
      "step": 9015
    },
    {
      "epoch": 4.110743989973795,
      "grad_norm": 0.18480423092842102,
      "learning_rate": 0.00017775752051048314,
      "loss": 0.7499,
      "step": 9020
    },
    {
      "epoch": 4.113022672895067,
      "grad_norm": 0.18238893151283264,
      "learning_rate": 0.00017730173199635368,
      "loss": 0.7949,
      "step": 9025
    },
    {
      "epoch": 4.115301355816338,
      "grad_norm": 0.18559129536151886,
      "learning_rate": 0.00017684594348222425,
      "loss": 0.7866,
      "step": 9030
    },
    {
      "epoch": 4.11758003873761,
      "grad_norm": 0.1623350828886032,
      "learning_rate": 0.0001763901549680948,
      "loss": 0.8562,
      "step": 9035
    },
    {
      "epoch": 4.119858721658881,
      "grad_norm": 0.1878800094127655,
      "learning_rate": 0.00017593436645396536,
      "loss": 0.8463,
      "step": 9040
    },
    {
      "epoch": 4.122137404580153,
      "grad_norm": 0.18353424966335297,
      "learning_rate": 0.00017547857793983592,
      "loss": 0.8257,
      "step": 9045
    },
    {
      "epoch": 4.124416087501424,
      "grad_norm": 0.17742106318473816,
      "learning_rate": 0.0001750227894257065,
      "loss": 0.7915,
      "step": 9050
    },
    {
      "epoch": 4.126694770422696,
      "grad_norm": 0.18406862020492554,
      "learning_rate": 0.00017456700091157703,
      "loss": 0.8374,
      "step": 9055
    },
    {
      "epoch": 4.128973453343967,
      "grad_norm": 0.18440315127372742,
      "learning_rate": 0.0001741112123974476,
      "loss": 0.8377,
      "step": 9060
    },
    {
      "epoch": 4.131252136265239,
      "grad_norm": 0.1797102987766266,
      "learning_rate": 0.00017365542388331814,
      "loss": 0.867,
      "step": 9065
    },
    {
      "epoch": 4.13353081918651,
      "grad_norm": 0.17432665824890137,
      "learning_rate": 0.0001731996353691887,
      "loss": 0.8642,
      "step": 9070
    },
    {
      "epoch": 4.135809502107782,
      "grad_norm": 0.17496414482593536,
      "learning_rate": 0.00017274384685505925,
      "loss": 0.8352,
      "step": 9075
    },
    {
      "epoch": 4.138088185029053,
      "grad_norm": 0.1700500249862671,
      "learning_rate": 0.0001722880583409298,
      "loss": 0.8212,
      "step": 9080
    },
    {
      "epoch": 4.140366867950325,
      "grad_norm": 0.17438136041164398,
      "learning_rate": 0.00017183226982680035,
      "loss": 0.8846,
      "step": 9085
    },
    {
      "epoch": 4.142645550871596,
      "grad_norm": 0.1893509328365326,
      "learning_rate": 0.00017137648131267092,
      "loss": 0.8175,
      "step": 9090
    },
    {
      "epoch": 4.144924233792867,
      "grad_norm": 0.17594751715660095,
      "learning_rate": 0.00017092069279854146,
      "loss": 0.7756,
      "step": 9095
    },
    {
      "epoch": 4.147202916714139,
      "grad_norm": 0.20622262358665466,
      "learning_rate": 0.00017046490428441203,
      "loss": 0.9024,
      "step": 9100
    },
    {
      "epoch": 4.14948159963541,
      "grad_norm": 0.15709814429283142,
      "learning_rate": 0.00017000911577028257,
      "loss": 0.859,
      "step": 9105
    },
    {
      "epoch": 4.151760282556682,
      "grad_norm": 0.18894395232200623,
      "learning_rate": 0.00016955332725615316,
      "loss": 0.8273,
      "step": 9110
    },
    {
      "epoch": 4.1540389654779535,
      "grad_norm": 0.1707872450351715,
      "learning_rate": 0.00016909753874202373,
      "loss": 0.8076,
      "step": 9115
    },
    {
      "epoch": 4.1563176483992255,
      "grad_norm": 0.15559318661689758,
      "learning_rate": 0.00016864175022789427,
      "loss": 0.7924,
      "step": 9120
    },
    {
      "epoch": 4.1585963313204966,
      "grad_norm": 0.176588773727417,
      "learning_rate": 0.00016818596171376483,
      "loss": 0.7537,
      "step": 9125
    },
    {
      "epoch": 4.1608750142417685,
      "grad_norm": 0.18731415271759033,
      "learning_rate": 0.00016773017319963537,
      "loss": 0.876,
      "step": 9130
    },
    {
      "epoch": 4.16315369716304,
      "grad_norm": 0.16432952880859375,
      "learning_rate": 0.00016727438468550594,
      "loss": 0.7982,
      "step": 9135
    },
    {
      "epoch": 4.165432380084312,
      "grad_norm": 0.18451498448848724,
      "learning_rate": 0.00016681859617137648,
      "loss": 0.8507,
      "step": 9140
    },
    {
      "epoch": 4.167711063005583,
      "grad_norm": 0.15803813934326172,
      "learning_rate": 0.00016636280765724705,
      "loss": 0.7908,
      "step": 9145
    },
    {
      "epoch": 4.169989745926855,
      "grad_norm": 0.17968593537807465,
      "learning_rate": 0.0001659070191431176,
      "loss": 0.8267,
      "step": 9150
    },
    {
      "epoch": 4.172268428848126,
      "grad_norm": 0.1575472205877304,
      "learning_rate": 0.00016545123062898816,
      "loss": 0.8588,
      "step": 9155
    },
    {
      "epoch": 4.174547111769398,
      "grad_norm": 0.20908397436141968,
      "learning_rate": 0.0001649954421148587,
      "loss": 0.7974,
      "step": 9160
    },
    {
      "epoch": 4.176825794690669,
      "grad_norm": 0.1824655532836914,
      "learning_rate": 0.00016453965360072926,
      "loss": 0.8669,
      "step": 9165
    },
    {
      "epoch": 4.17910447761194,
      "grad_norm": 0.16668587923049927,
      "learning_rate": 0.0001640838650865998,
      "loss": 0.8351,
      "step": 9170
    },
    {
      "epoch": 4.181383160533212,
      "grad_norm": 0.1982879787683487,
      "learning_rate": 0.00016362807657247037,
      "loss": 0.8038,
      "step": 9175
    },
    {
      "epoch": 4.183661843454483,
      "grad_norm": 0.18184949457645416,
      "learning_rate": 0.00016317228805834094,
      "loss": 0.8042,
      "step": 9180
    },
    {
      "epoch": 4.185940526375755,
      "grad_norm": 0.19091767072677612,
      "learning_rate": 0.0001627164995442115,
      "loss": 0.8485,
      "step": 9185
    },
    {
      "epoch": 4.188219209297026,
      "grad_norm": 0.17855039238929749,
      "learning_rate": 0.00016226071103008204,
      "loss": 0.839,
      "step": 9190
    },
    {
      "epoch": 4.190497892218298,
      "grad_norm": 0.17781591415405273,
      "learning_rate": 0.0001618049225159526,
      "loss": 0.8665,
      "step": 9195
    },
    {
      "epoch": 4.192776575139569,
      "grad_norm": 0.16936133801937103,
      "learning_rate": 0.00016134913400182315,
      "loss": 0.821,
      "step": 9200
    },
    {
      "epoch": 4.195055258060841,
      "grad_norm": 0.17586655914783478,
      "learning_rate": 0.00016089334548769372,
      "loss": 0.83,
      "step": 9205
    },
    {
      "epoch": 4.197333940982112,
      "grad_norm": 0.1753382831811905,
      "learning_rate": 0.00016043755697356426,
      "loss": 0.8511,
      "step": 9210
    },
    {
      "epoch": 4.199612623903384,
      "grad_norm": 0.18277999758720398,
      "learning_rate": 0.00015998176845943482,
      "loss": 0.7812,
      "step": 9215
    },
    {
      "epoch": 4.201891306824655,
      "grad_norm": 0.1742212325334549,
      "learning_rate": 0.00015952597994530536,
      "loss": 0.805,
      "step": 9220
    },
    {
      "epoch": 4.204169989745927,
      "grad_norm": 0.16928629577159882,
      "learning_rate": 0.00015907019143117593,
      "loss": 0.8356,
      "step": 9225
    },
    {
      "epoch": 4.206448672667198,
      "grad_norm": 0.23613794147968292,
      "learning_rate": 0.0001586144029170465,
      "loss": 0.877,
      "step": 9230
    },
    {
      "epoch": 4.208727355588469,
      "grad_norm": 0.17246589064598083,
      "learning_rate": 0.00015815861440291704,
      "loss": 0.7413,
      "step": 9235
    },
    {
      "epoch": 4.211006038509741,
      "grad_norm": 0.19607821106910706,
      "learning_rate": 0.0001577028258887876,
      "loss": 0.8409,
      "step": 9240
    },
    {
      "epoch": 4.2132847214310125,
      "grad_norm": 0.19203054904937744,
      "learning_rate": 0.00015724703737465817,
      "loss": 0.822,
      "step": 9245
    },
    {
      "epoch": 4.2155634043522845,
      "grad_norm": 0.1702718436717987,
      "learning_rate": 0.00015679124886052874,
      "loss": 0.7932,
      "step": 9250
    },
    {
      "epoch": 4.217842087273556,
      "grad_norm": 0.19085180759429932,
      "learning_rate": 0.00015633546034639928,
      "loss": 0.7793,
      "step": 9255
    },
    {
      "epoch": 4.220120770194828,
      "grad_norm": 0.1943570226430893,
      "learning_rate": 0.00015587967183226985,
      "loss": 0.8037,
      "step": 9260
    },
    {
      "epoch": 4.222399453116099,
      "grad_norm": 0.16054585576057434,
      "learning_rate": 0.00015542388331814039,
      "loss": 0.8155,
      "step": 9265
    },
    {
      "epoch": 4.224678136037371,
      "grad_norm": 0.17095889151096344,
      "learning_rate": 0.00015496809480401095,
      "loss": 0.7815,
      "step": 9270
    },
    {
      "epoch": 4.226956818958642,
      "grad_norm": 0.2029910832643509,
      "learning_rate": 0.0001545123062898815,
      "loss": 0.8349,
      "step": 9275
    },
    {
      "epoch": 4.229235501879914,
      "grad_norm": 0.18448610603809357,
      "learning_rate": 0.00015405651777575206,
      "loss": 0.7642,
      "step": 9280
    },
    {
      "epoch": 4.231514184801185,
      "grad_norm": 0.15807679295539856,
      "learning_rate": 0.0001536007292616226,
      "loss": 0.7597,
      "step": 9285
    },
    {
      "epoch": 4.233792867722457,
      "grad_norm": 0.15311497449874878,
      "learning_rate": 0.00015314494074749317,
      "loss": 0.8593,
      "step": 9290
    },
    {
      "epoch": 4.236071550643728,
      "grad_norm": 0.18336221575737,
      "learning_rate": 0.0001526891522333637,
      "loss": 0.8218,
      "step": 9295
    },
    {
      "epoch": 4.238350233565,
      "grad_norm": 0.18087482452392578,
      "learning_rate": 0.00015223336371923427,
      "loss": 0.7722,
      "step": 9300
    },
    {
      "epoch": 4.240628916486271,
      "grad_norm": 0.18911714851856232,
      "learning_rate": 0.0001517775752051048,
      "loss": 0.861,
      "step": 9305
    },
    {
      "epoch": 4.242907599407542,
      "grad_norm": 0.17387908697128296,
      "learning_rate": 0.00015132178669097538,
      "loss": 0.7877,
      "step": 9310
    },
    {
      "epoch": 4.245186282328814,
      "grad_norm": 0.1873881220817566,
      "learning_rate": 0.00015086599817684595,
      "loss": 0.8623,
      "step": 9315
    },
    {
      "epoch": 4.247464965250085,
      "grad_norm": 0.17613820731639862,
      "learning_rate": 0.00015041020966271651,
      "loss": 0.8448,
      "step": 9320
    },
    {
      "epoch": 4.249743648171357,
      "grad_norm": 0.1752026528120041,
      "learning_rate": 0.00014995442114858705,
      "loss": 0.7995,
      "step": 9325
    },
    {
      "epoch": 4.252022331092628,
      "grad_norm": 0.19483810663223267,
      "learning_rate": 0.00014949863263445762,
      "loss": 0.826,
      "step": 9330
    },
    {
      "epoch": 4.2543010140139,
      "grad_norm": 0.18092602491378784,
      "learning_rate": 0.00014904284412032816,
      "loss": 0.825,
      "step": 9335
    },
    {
      "epoch": 4.256579696935171,
      "grad_norm": 0.16866624355316162,
      "learning_rate": 0.00014858705560619873,
      "loss": 0.8197,
      "step": 9340
    },
    {
      "epoch": 4.258858379856443,
      "grad_norm": 0.2161816507577896,
      "learning_rate": 0.0001481312670920693,
      "loss": 0.82,
      "step": 9345
    },
    {
      "epoch": 4.261137062777714,
      "grad_norm": 0.1675489842891693,
      "learning_rate": 0.00014767547857793984,
      "loss": 0.7994,
      "step": 9350
    },
    {
      "epoch": 4.263415745698986,
      "grad_norm": 0.17865759134292603,
      "learning_rate": 0.0001472196900638104,
      "loss": 0.8192,
      "step": 9355
    },
    {
      "epoch": 4.265694428620257,
      "grad_norm": 0.18685442209243774,
      "learning_rate": 0.00014676390154968094,
      "loss": 0.8587,
      "step": 9360
    },
    {
      "epoch": 4.267973111541529,
      "grad_norm": 0.16514408588409424,
      "learning_rate": 0.0001463081130355515,
      "loss": 0.7988,
      "step": 9365
    },
    {
      "epoch": 4.2702517944628005,
      "grad_norm": 0.18680590391159058,
      "learning_rate": 0.00014585232452142205,
      "loss": 0.8419,
      "step": 9370
    },
    {
      "epoch": 4.2725304773840715,
      "grad_norm": 0.1876188963651657,
      "learning_rate": 0.00014539653600729262,
      "loss": 0.8728,
      "step": 9375
    },
    {
      "epoch": 4.2748091603053435,
      "grad_norm": 0.1813155859708786,
      "learning_rate": 0.00014494074749316318,
      "loss": 0.833,
      "step": 9380
    },
    {
      "epoch": 4.277087843226615,
      "grad_norm": 0.17100279033184052,
      "learning_rate": 0.00014448495897903375,
      "loss": 0.8301,
      "step": 9385
    },
    {
      "epoch": 4.279366526147887,
      "grad_norm": 0.186355859041214,
      "learning_rate": 0.0001440291704649043,
      "loss": 0.8651,
      "step": 9390
    },
    {
      "epoch": 4.281645209069158,
      "grad_norm": 0.1894243210554123,
      "learning_rate": 0.00014357338195077486,
      "loss": 0.8434,
      "step": 9395
    },
    {
      "epoch": 4.28392389199043,
      "grad_norm": 0.16681967675685883,
      "learning_rate": 0.0001431175934366454,
      "loss": 0.7775,
      "step": 9400
    },
    {
      "epoch": 4.286202574911701,
      "grad_norm": 0.19262784719467163,
      "learning_rate": 0.00014266180492251596,
      "loss": 0.9096,
      "step": 9405
    },
    {
      "epoch": 4.288481257832973,
      "grad_norm": 0.17956483364105225,
      "learning_rate": 0.0001422060164083865,
      "loss": 0.7981,
      "step": 9410
    },
    {
      "epoch": 4.290759940754244,
      "grad_norm": 0.18412183225154877,
      "learning_rate": 0.00014175022789425707,
      "loss": 0.8452,
      "step": 9415
    },
    {
      "epoch": 4.293038623675516,
      "grad_norm": 0.1749558001756668,
      "learning_rate": 0.0001412944393801276,
      "loss": 0.8786,
      "step": 9420
    },
    {
      "epoch": 4.295317306596787,
      "grad_norm": 0.17551018297672272,
      "learning_rate": 0.00014083865086599818,
      "loss": 0.7803,
      "step": 9425
    },
    {
      "epoch": 4.297595989518059,
      "grad_norm": 0.17705483734607697,
      "learning_rate": 0.00014038286235186872,
      "loss": 0.783,
      "step": 9430
    },
    {
      "epoch": 4.29987467243933,
      "grad_norm": 0.17874541878700256,
      "learning_rate": 0.00013992707383773928,
      "loss": 0.7975,
      "step": 9435
    },
    {
      "epoch": 4.302153355360602,
      "grad_norm": 0.18513038754463196,
      "learning_rate": 0.00013947128532360982,
      "loss": 0.8655,
      "step": 9440
    },
    {
      "epoch": 4.304432038281873,
      "grad_norm": 0.17509318888187408,
      "learning_rate": 0.0001390154968094804,
      "loss": 0.8372,
      "step": 9445
    },
    {
      "epoch": 4.306710721203144,
      "grad_norm": 0.18519024550914764,
      "learning_rate": 0.00013855970829535096,
      "loss": 0.8308,
      "step": 9450
    },
    {
      "epoch": 4.308989404124416,
      "grad_norm": 0.16768653690814972,
      "learning_rate": 0.00013810391978122153,
      "loss": 0.7912,
      "step": 9455
    },
    {
      "epoch": 4.311268087045687,
      "grad_norm": 0.1787024885416031,
      "learning_rate": 0.0001376481312670921,
      "loss": 0.8021,
      "step": 9460
    },
    {
      "epoch": 4.313546769966959,
      "grad_norm": 0.18435265123844147,
      "learning_rate": 0.00013719234275296263,
      "loss": 0.8784,
      "step": 9465
    },
    {
      "epoch": 4.31582545288823,
      "grad_norm": 0.18321801722049713,
      "learning_rate": 0.0001367365542388332,
      "loss": 0.8529,
      "step": 9470
    },
    {
      "epoch": 4.318104135809502,
      "grad_norm": 0.1827247589826584,
      "learning_rate": 0.00013628076572470374,
      "loss": 0.9092,
      "step": 9475
    },
    {
      "epoch": 4.320382818730773,
      "grad_norm": 0.17484533786773682,
      "learning_rate": 0.0001358249772105743,
      "loss": 0.8456,
      "step": 9480
    },
    {
      "epoch": 4.322661501652045,
      "grad_norm": 0.17606642842292786,
      "learning_rate": 0.00013536918869644485,
      "loss": 0.7837,
      "step": 9485
    },
    {
      "epoch": 4.324940184573316,
      "grad_norm": 0.19051121175289154,
      "learning_rate": 0.00013491340018231541,
      "loss": 0.8091,
      "step": 9490
    },
    {
      "epoch": 4.327218867494588,
      "grad_norm": 0.18818999826908112,
      "learning_rate": 0.00013445761166818595,
      "loss": 0.8404,
      "step": 9495
    },
    {
      "epoch": 4.3294975504158595,
      "grad_norm": 0.1676165759563446,
      "learning_rate": 0.00013400182315405652,
      "loss": 0.8006,
      "step": 9500
    },
    {
      "epoch": 4.3317762333371315,
      "grad_norm": 0.17469856142997742,
      "learning_rate": 0.00013354603463992706,
      "loss": 0.8697,
      "step": 9505
    },
    {
      "epoch": 4.334054916258403,
      "grad_norm": 0.1760493963956833,
      "learning_rate": 0.00013309024612579763,
      "loss": 0.7982,
      "step": 9510
    },
    {
      "epoch": 4.3363335991796745,
      "grad_norm": 0.22120381891727448,
      "learning_rate": 0.0001326344576116682,
      "loss": 0.8424,
      "step": 9515
    },
    {
      "epoch": 4.338612282100946,
      "grad_norm": 0.20276203751564026,
      "learning_rate": 0.00013217866909753876,
      "loss": 0.8434,
      "step": 9520
    },
    {
      "epoch": 4.340890965022217,
      "grad_norm": 0.19468219578266144,
      "learning_rate": 0.0001317228805834093,
      "loss": 0.8191,
      "step": 9525
    },
    {
      "epoch": 4.343169647943489,
      "grad_norm": 0.1725711226463318,
      "learning_rate": 0.00013126709206927987,
      "loss": 0.8225,
      "step": 9530
    },
    {
      "epoch": 4.34544833086476,
      "grad_norm": 0.19840258359909058,
      "learning_rate": 0.0001308113035551504,
      "loss": 0.7866,
      "step": 9535
    },
    {
      "epoch": 4.347727013786032,
      "grad_norm": 0.18109069764614105,
      "learning_rate": 0.00013035551504102098,
      "loss": 0.8065,
      "step": 9540
    },
    {
      "epoch": 4.350005696707303,
      "grad_norm": 0.1900944709777832,
      "learning_rate": 0.00012989972652689152,
      "loss": 0.8601,
      "step": 9545
    },
    {
      "epoch": 4.352284379628575,
      "grad_norm": 0.21845093369483948,
      "learning_rate": 0.00012944393801276208,
      "loss": 0.8972,
      "step": 9550
    },
    {
      "epoch": 4.354563062549846,
      "grad_norm": 0.20092202723026276,
      "learning_rate": 0.00012898814949863262,
      "loss": 0.8695,
      "step": 9555
    },
    {
      "epoch": 4.356841745471118,
      "grad_norm": 0.17793568968772888,
      "learning_rate": 0.0001285323609845032,
      "loss": 0.8078,
      "step": 9560
    },
    {
      "epoch": 4.359120428392389,
      "grad_norm": 0.1792505979537964,
      "learning_rate": 0.00012807657247037373,
      "loss": 0.7844,
      "step": 9565
    },
    {
      "epoch": 4.361399111313661,
      "grad_norm": 0.1570105105638504,
      "learning_rate": 0.0001276207839562443,
      "loss": 0.8566,
      "step": 9570
    },
    {
      "epoch": 4.363677794234932,
      "grad_norm": 0.16725100576877594,
      "learning_rate": 0.00012716499544211486,
      "loss": 0.8243,
      "step": 9575
    },
    {
      "epoch": 4.365956477156204,
      "grad_norm": 0.22178982198238373,
      "learning_rate": 0.0001267092069279854,
      "loss": 0.8286,
      "step": 9580
    },
    {
      "epoch": 4.368235160077475,
      "grad_norm": 0.16472968459129333,
      "learning_rate": 0.000126253418413856,
      "loss": 0.8614,
      "step": 9585
    },
    {
      "epoch": 4.370513842998747,
      "grad_norm": 0.17538954317569733,
      "learning_rate": 0.00012579762989972654,
      "loss": 0.8958,
      "step": 9590
    },
    {
      "epoch": 4.372792525920018,
      "grad_norm": 0.1743236482143402,
      "learning_rate": 0.0001253418413855971,
      "loss": 0.8459,
      "step": 9595
    },
    {
      "epoch": 4.375071208841289,
      "grad_norm": 0.1674027293920517,
      "learning_rate": 0.00012488605287146764,
      "loss": 0.8414,
      "step": 9600
    },
    {
      "epoch": 4.377349891762561,
      "grad_norm": 0.2248488813638687,
      "learning_rate": 0.0001244302643573382,
      "loss": 0.8788,
      "step": 9605
    },
    {
      "epoch": 4.379628574683832,
      "grad_norm": 0.19859351217746735,
      "learning_rate": 0.00012397447584320875,
      "loss": 0.8131,
      "step": 9610
    },
    {
      "epoch": 4.381907257605104,
      "grad_norm": 0.1726653128862381,
      "learning_rate": 0.00012351868732907932,
      "loss": 0.8059,
      "step": 9615
    },
    {
      "epoch": 4.3841859405263754,
      "grad_norm": 0.1614210456609726,
      "learning_rate": 0.00012306289881494986,
      "loss": 0.7794,
      "step": 9620
    },
    {
      "epoch": 4.386464623447647,
      "grad_norm": 0.17743781208992004,
      "learning_rate": 0.00012260711030082043,
      "loss": 0.8765,
      "step": 9625
    },
    {
      "epoch": 4.3887433063689185,
      "grad_norm": 0.17545515298843384,
      "learning_rate": 0.00012215132178669097,
      "loss": 0.7876,
      "step": 9630
    },
    {
      "epoch": 4.3910219892901905,
      "grad_norm": 0.17720447480678558,
      "learning_rate": 0.00012169553327256155,
      "loss": 0.8083,
      "step": 9635
    },
    {
      "epoch": 4.393300672211462,
      "grad_norm": 0.17674699425697327,
      "learning_rate": 0.0001212397447584321,
      "loss": 0.784,
      "step": 9640
    },
    {
      "epoch": 4.395579355132734,
      "grad_norm": 0.1878485381603241,
      "learning_rate": 0.00012078395624430265,
      "loss": 0.8061,
      "step": 9645
    },
    {
      "epoch": 4.397858038054005,
      "grad_norm": 0.1828060746192932,
      "learning_rate": 0.0001203281677301732,
      "loss": 0.8038,
      "step": 9650
    },
    {
      "epoch": 4.400136720975277,
      "grad_norm": 0.18593211472034454,
      "learning_rate": 0.00011987237921604376,
      "loss": 0.8772,
      "step": 9655
    },
    {
      "epoch": 4.402415403896548,
      "grad_norm": 0.1638880968093872,
      "learning_rate": 0.00011941659070191431,
      "loss": 0.8211,
      "step": 9660
    },
    {
      "epoch": 4.40469408681782,
      "grad_norm": 0.19766922295093536,
      "learning_rate": 0.00011896080218778487,
      "loss": 0.8301,
      "step": 9665
    },
    {
      "epoch": 4.406972769739091,
      "grad_norm": 0.16962513327598572,
      "learning_rate": 0.00011850501367365543,
      "loss": 0.7858,
      "step": 9670
    },
    {
      "epoch": 4.409251452660362,
      "grad_norm": 0.18450653553009033,
      "learning_rate": 0.00011804922515952599,
      "loss": 0.8062,
      "step": 9675
    },
    {
      "epoch": 4.411530135581634,
      "grad_norm": 0.16855522990226746,
      "learning_rate": 0.00011759343664539654,
      "loss": 0.8525,
      "step": 9680
    },
    {
      "epoch": 4.413808818502905,
      "grad_norm": 0.21785090863704681,
      "learning_rate": 0.0001171376481312671,
      "loss": 0.8534,
      "step": 9685
    },
    {
      "epoch": 4.416087501424177,
      "grad_norm": 0.2083253264427185,
      "learning_rate": 0.00011668185961713765,
      "loss": 0.8873,
      "step": 9690
    },
    {
      "epoch": 4.418366184345448,
      "grad_norm": 0.19362817704677582,
      "learning_rate": 0.0001162260711030082,
      "loss": 0.8575,
      "step": 9695
    },
    {
      "epoch": 4.42064486726672,
      "grad_norm": 0.16750477254390717,
      "learning_rate": 0.00011577028258887875,
      "loss": 0.8384,
      "step": 9700
    },
    {
      "epoch": 4.422923550187991,
      "grad_norm": 0.19376641511917114,
      "learning_rate": 0.00011531449407474932,
      "loss": 0.8506,
      "step": 9705
    },
    {
      "epoch": 4.425202233109263,
      "grad_norm": 0.16972379386425018,
      "learning_rate": 0.00011485870556061987,
      "loss": 0.7881,
      "step": 9710
    },
    {
      "epoch": 4.427480916030534,
      "grad_norm": 0.16027118265628815,
      "learning_rate": 0.00011440291704649043,
      "loss": 0.787,
      "step": 9715
    },
    {
      "epoch": 4.429759598951806,
      "grad_norm": 0.18857191503047943,
      "learning_rate": 0.00011394712853236098,
      "loss": 0.8251,
      "step": 9720
    },
    {
      "epoch": 4.432038281873077,
      "grad_norm": 0.1726696938276291,
      "learning_rate": 0.00011349134001823154,
      "loss": 0.8379,
      "step": 9725
    },
    {
      "epoch": 4.434316964794349,
      "grad_norm": 0.1552259624004364,
      "learning_rate": 0.00011303555150410209,
      "loss": 0.7754,
      "step": 9730
    },
    {
      "epoch": 4.43659564771562,
      "grad_norm": 0.19261911511421204,
      "learning_rate": 0.00011257976298997266,
      "loss": 0.7978,
      "step": 9735
    },
    {
      "epoch": 4.438874330636892,
      "grad_norm": 0.16522543132305145,
      "learning_rate": 0.00011212397447584321,
      "loss": 0.857,
      "step": 9740
    },
    {
      "epoch": 4.441153013558163,
      "grad_norm": 0.19573067128658295,
      "learning_rate": 0.00011166818596171378,
      "loss": 0.8276,
      "step": 9745
    },
    {
      "epoch": 4.4434316964794345,
      "grad_norm": 0.1890554428100586,
      "learning_rate": 0.00011121239744758433,
      "loss": 0.8262,
      "step": 9750
    },
    {
      "epoch": 4.4457103794007065,
      "grad_norm": 0.18922683596611023,
      "learning_rate": 0.00011075660893345488,
      "loss": 0.8242,
      "step": 9755
    },
    {
      "epoch": 4.4479890623219775,
      "grad_norm": 0.1861734539270401,
      "learning_rate": 0.00011030082041932544,
      "loss": 0.8064,
      "step": 9760
    },
    {
      "epoch": 4.4502677452432495,
      "grad_norm": 0.19384731352329254,
      "learning_rate": 0.00010984503190519599,
      "loss": 0.8495,
      "step": 9765
    },
    {
      "epoch": 4.452546428164521,
      "grad_norm": 0.17946359515190125,
      "learning_rate": 0.00010938924339106656,
      "loss": 0.8212,
      "step": 9770
    },
    {
      "epoch": 4.454825111085793,
      "grad_norm": 0.1850689947605133,
      "learning_rate": 0.00010893345487693711,
      "loss": 0.8749,
      "step": 9775
    },
    {
      "epoch": 4.457103794007064,
      "grad_norm": 0.1811646670103073,
      "learning_rate": 0.00010847766636280766,
      "loss": 0.8739,
      "step": 9780
    },
    {
      "epoch": 4.459382476928336,
      "grad_norm": 0.15192686021327972,
      "learning_rate": 0.00010802187784867822,
      "loss": 0.7914,
      "step": 9785
    },
    {
      "epoch": 4.461661159849607,
      "grad_norm": 0.1827811300754547,
      "learning_rate": 0.00010756608933454877,
      "loss": 0.8218,
      "step": 9790
    },
    {
      "epoch": 4.463939842770879,
      "grad_norm": 0.18305237591266632,
      "learning_rate": 0.00010711030082041932,
      "loss": 0.8175,
      "step": 9795
    },
    {
      "epoch": 4.46621852569215,
      "grad_norm": 0.1826995313167572,
      "learning_rate": 0.00010665451230628988,
      "loss": 0.8053,
      "step": 9800
    },
    {
      "epoch": 4.468497208613422,
      "grad_norm": 0.19112928211688995,
      "learning_rate": 0.00010619872379216045,
      "loss": 0.8377,
      "step": 9805
    },
    {
      "epoch": 4.470775891534693,
      "grad_norm": 0.1850341111421585,
      "learning_rate": 0.000105742935278031,
      "loss": 0.8134,
      "step": 9810
    },
    {
      "epoch": 4.473054574455965,
      "grad_norm": 0.18187415599822998,
      "learning_rate": 0.00010528714676390155,
      "loss": 0.8563,
      "step": 9815
    },
    {
      "epoch": 4.475333257377236,
      "grad_norm": 0.18300871551036835,
      "learning_rate": 0.0001048313582497721,
      "loss": 0.8324,
      "step": 9820
    },
    {
      "epoch": 4.477611940298507,
      "grad_norm": 0.19691036641597748,
      "learning_rate": 0.00010437556973564266,
      "loss": 0.8456,
      "step": 9825
    },
    {
      "epoch": 4.479890623219779,
      "grad_norm": 0.20064175128936768,
      "learning_rate": 0.00010391978122151321,
      "loss": 0.8816,
      "step": 9830
    },
    {
      "epoch": 4.48216930614105,
      "grad_norm": 0.17225605249404907,
      "learning_rate": 0.00010346399270738377,
      "loss": 0.7916,
      "step": 9835
    },
    {
      "epoch": 4.484447989062322,
      "grad_norm": 0.19524458050727844,
      "learning_rate": 0.00010300820419325433,
      "loss": 0.822,
      "step": 9840
    },
    {
      "epoch": 4.486726671983593,
      "grad_norm": 0.17560045421123505,
      "learning_rate": 0.00010255241567912489,
      "loss": 0.8391,
      "step": 9845
    },
    {
      "epoch": 4.489005354904865,
      "grad_norm": 0.23052529990673065,
      "learning_rate": 0.00010209662716499544,
      "loss": 0.8722,
      "step": 9850
    },
    {
      "epoch": 4.491284037826136,
      "grad_norm": 0.19411146640777588,
      "learning_rate": 0.000101640838650866,
      "loss": 0.828,
      "step": 9855
    },
    {
      "epoch": 4.493562720747408,
      "grad_norm": 0.17349903285503387,
      "learning_rate": 0.00010118505013673655,
      "loss": 0.8234,
      "step": 9860
    },
    {
      "epoch": 4.495841403668679,
      "grad_norm": 0.19133688509464264,
      "learning_rate": 0.00010072926162260711,
      "loss": 0.793,
      "step": 9865
    },
    {
      "epoch": 4.498120086589951,
      "grad_norm": 0.18037515878677368,
      "learning_rate": 0.00010027347310847768,
      "loss": 0.7982,
      "step": 9870
    },
    {
      "epoch": 4.500398769511222,
      "grad_norm": 0.17116524279117584,
      "learning_rate": 9.981768459434823e-05,
      "loss": 0.8838,
      "step": 9875
    },
    {
      "epoch": 4.502677452432494,
      "grad_norm": 0.18019509315490723,
      "learning_rate": 9.936189608021879e-05,
      "loss": 0.8332,
      "step": 9880
    },
    {
      "epoch": 4.5049561353537655,
      "grad_norm": 0.1766669899225235,
      "learning_rate": 9.890610756608934e-05,
      "loss": 0.8039,
      "step": 9885
    },
    {
      "epoch": 4.5072348182750375,
      "grad_norm": 0.18578612804412842,
      "learning_rate": 9.84503190519599e-05,
      "loss": 0.8437,
      "step": 9890
    },
    {
      "epoch": 4.509513501196309,
      "grad_norm": 0.1630038321018219,
      "learning_rate": 9.799453053783045e-05,
      "loss": 0.8253,
      "step": 9895
    },
    {
      "epoch": 4.51179218411758,
      "grad_norm": 0.16596679389476776,
      "learning_rate": 9.7538742023701e-05,
      "loss": 0.8205,
      "step": 9900
    },
    {
      "epoch": 4.514070867038852,
      "grad_norm": 0.19896851480007172,
      "learning_rate": 9.708295350957157e-05,
      "loss": 0.8458,
      "step": 9905
    },
    {
      "epoch": 4.516349549960123,
      "grad_norm": 0.19719058275222778,
      "learning_rate": 9.662716499544212e-05,
      "loss": 0.808,
      "step": 9910
    },
    {
      "epoch": 4.518628232881395,
      "grad_norm": 0.1741683930158615,
      "learning_rate": 9.617137648131268e-05,
      "loss": 0.7641,
      "step": 9915
    },
    {
      "epoch": 4.520906915802666,
      "grad_norm": 0.16006912291049957,
      "learning_rate": 9.571558796718323e-05,
      "loss": 0.8036,
      "step": 9920
    },
    {
      "epoch": 4.523185598723938,
      "grad_norm": 0.20567239820957184,
      "learning_rate": 9.525979945305378e-05,
      "loss": 0.8619,
      "step": 9925
    },
    {
      "epoch": 4.525464281645209,
      "grad_norm": 0.17897933721542358,
      "learning_rate": 9.480401093892434e-05,
      "loss": 0.8505,
      "step": 9930
    },
    {
      "epoch": 4.527742964566481,
      "grad_norm": 0.1987430304288864,
      "learning_rate": 9.434822242479489e-05,
      "loss": 0.7917,
      "step": 9935
    },
    {
      "epoch": 4.530021647487752,
      "grad_norm": 0.193313330411911,
      "learning_rate": 9.389243391066546e-05,
      "loss": 0.8538,
      "step": 9940
    },
    {
      "epoch": 4.532300330409024,
      "grad_norm": 0.1797620952129364,
      "learning_rate": 9.343664539653601e-05,
      "loss": 0.8632,
      "step": 9945
    },
    {
      "epoch": 4.534579013330295,
      "grad_norm": 0.17489705979824066,
      "learning_rate": 9.298085688240656e-05,
      "loss": 0.79,
      "step": 9950
    },
    {
      "epoch": 4.536857696251566,
      "grad_norm": 0.19115522503852844,
      "learning_rate": 9.252506836827712e-05,
      "loss": 0.8849,
      "step": 9955
    },
    {
      "epoch": 4.539136379172838,
      "grad_norm": 0.17879976332187653,
      "learning_rate": 9.206927985414767e-05,
      "loss": 0.7769,
      "step": 9960
    },
    {
      "epoch": 4.54141506209411,
      "grad_norm": 0.16648758947849274,
      "learning_rate": 9.161349134001822e-05,
      "loss": 0.8309,
      "step": 9965
    },
    {
      "epoch": 4.543693745015381,
      "grad_norm": 0.18971163034439087,
      "learning_rate": 9.115770282588878e-05,
      "loss": 0.7958,
      "step": 9970
    },
    {
      "epoch": 4.545972427936652,
      "grad_norm": 0.18261291086673737,
      "learning_rate": 9.070191431175934e-05,
      "loss": 0.7919,
      "step": 9975
    },
    {
      "epoch": 4.548251110857924,
      "grad_norm": 0.2117445170879364,
      "learning_rate": 9.024612579762991e-05,
      "loss": 0.8378,
      "step": 9980
    },
    {
      "epoch": 4.550529793779195,
      "grad_norm": 0.1762104332447052,
      "learning_rate": 8.979033728350046e-05,
      "loss": 0.8012,
      "step": 9985
    },
    {
      "epoch": 4.552808476700467,
      "grad_norm": 0.18984951078891754,
      "learning_rate": 8.933454876937102e-05,
      "loss": 0.8397,
      "step": 9990
    },
    {
      "epoch": 4.555087159621738,
      "grad_norm": 0.16704082489013672,
      "learning_rate": 8.887876025524157e-05,
      "loss": 0.7447,
      "step": 9995
    },
    {
      "epoch": 4.55736584254301,
      "grad_norm": 0.19733473658561707,
      "learning_rate": 8.842297174111213e-05,
      "loss": 0.7961,
      "step": 10000
    },
    {
      "epoch": 4.5596445254642815,
      "grad_norm": 0.17192573845386505,
      "learning_rate": 8.796718322698268e-05,
      "loss": 0.8259,
      "step": 10005
    },
    {
      "epoch": 4.561923208385553,
      "grad_norm": 0.17491742968559265,
      "learning_rate": 8.751139471285325e-05,
      "loss": 0.8098,
      "step": 10010
    },
    {
      "epoch": 4.5642018913068245,
      "grad_norm": 0.1707138568162918,
      "learning_rate": 8.70556061987238e-05,
      "loss": 0.8581,
      "step": 10015
    },
    {
      "epoch": 4.5664805742280965,
      "grad_norm": 0.19103436172008514,
      "learning_rate": 8.659981768459435e-05,
      "loss": 0.8899,
      "step": 10020
    },
    {
      "epoch": 4.568759257149368,
      "grad_norm": 0.2055070549249649,
      "learning_rate": 8.61440291704649e-05,
      "loss": 0.8623,
      "step": 10025
    },
    {
      "epoch": 4.571037940070639,
      "grad_norm": 0.1631503701210022,
      "learning_rate": 8.568824065633546e-05,
      "loss": 0.7827,
      "step": 10030
    },
    {
      "epoch": 4.573316622991911,
      "grad_norm": 0.18106138706207275,
      "learning_rate": 8.523245214220601e-05,
      "loss": 0.7994,
      "step": 10035
    },
    {
      "epoch": 4.575595305913183,
      "grad_norm": 0.1667422205209732,
      "learning_rate": 8.477666362807658e-05,
      "loss": 0.7874,
      "step": 10040
    },
    {
      "epoch": 4.577873988834454,
      "grad_norm": 0.18885737657546997,
      "learning_rate": 8.432087511394713e-05,
      "loss": 0.7735,
      "step": 10045
    },
    {
      "epoch": 4.580152671755725,
      "grad_norm": 0.18636168539524078,
      "learning_rate": 8.386508659981769e-05,
      "loss": 0.8122,
      "step": 10050
    },
    {
      "epoch": 4.582431354676997,
      "grad_norm": 0.18700680136680603,
      "learning_rate": 8.340929808568824e-05,
      "loss": 0.835,
      "step": 10055
    },
    {
      "epoch": 4.584710037598268,
      "grad_norm": 0.19353905320167542,
      "learning_rate": 8.29535095715588e-05,
      "loss": 0.8683,
      "step": 10060
    },
    {
      "epoch": 4.58698872051954,
      "grad_norm": 0.17613224685192108,
      "learning_rate": 8.249772105742935e-05,
      "loss": 0.8158,
      "step": 10065
    },
    {
      "epoch": 4.589267403440811,
      "grad_norm": 0.1839166283607483,
      "learning_rate": 8.20419325432999e-05,
      "loss": 0.8376,
      "step": 10070
    },
    {
      "epoch": 4.591546086362083,
      "grad_norm": 0.1991286426782608,
      "learning_rate": 8.158614402917047e-05,
      "loss": 0.921,
      "step": 10075
    },
    {
      "epoch": 4.593824769283354,
      "grad_norm": 0.17961129546165466,
      "learning_rate": 8.113035551504102e-05,
      "loss": 0.8721,
      "step": 10080
    },
    {
      "epoch": 4.596103452204626,
      "grad_norm": 0.17509886622428894,
      "learning_rate": 8.067456700091157e-05,
      "loss": 0.8401,
      "step": 10085
    },
    {
      "epoch": 4.598382135125897,
      "grad_norm": 0.23544302582740784,
      "learning_rate": 8.021877848678213e-05,
      "loss": 0.856,
      "step": 10090
    },
    {
      "epoch": 4.600660818047169,
      "grad_norm": 0.1902661770582199,
      "learning_rate": 7.976298997265268e-05,
      "loss": 0.8486,
      "step": 10095
    },
    {
      "epoch": 4.60293950096844,
      "grad_norm": 0.17137624323368073,
      "learning_rate": 7.930720145852325e-05,
      "loss": 0.8624,
      "step": 10100
    },
    {
      "epoch": 4.605218183889711,
      "grad_norm": 0.1693725287914276,
      "learning_rate": 7.88514129443938e-05,
      "loss": 0.8076,
      "step": 10105
    },
    {
      "epoch": 4.607496866810983,
      "grad_norm": 0.2043016403913498,
      "learning_rate": 7.839562443026437e-05,
      "loss": 0.8399,
      "step": 10110
    },
    {
      "epoch": 4.609775549732255,
      "grad_norm": 0.18917468190193176,
      "learning_rate": 7.793983591613492e-05,
      "loss": 0.8719,
      "step": 10115
    },
    {
      "epoch": 4.612054232653526,
      "grad_norm": 0.1593846082687378,
      "learning_rate": 7.748404740200548e-05,
      "loss": 0.8493,
      "step": 10120
    },
    {
      "epoch": 4.614332915574797,
      "grad_norm": 0.18620948493480682,
      "learning_rate": 7.702825888787603e-05,
      "loss": 0.7939,
      "step": 10125
    },
    {
      "epoch": 4.616611598496069,
      "grad_norm": 0.18214431405067444,
      "learning_rate": 7.657247037374658e-05,
      "loss": 0.8389,
      "step": 10130
    },
    {
      "epoch": 4.6188902814173405,
      "grad_norm": 0.20612673461437225,
      "learning_rate": 7.611668185961714e-05,
      "loss": 0.862,
      "step": 10135
    },
    {
      "epoch": 4.6211689643386125,
      "grad_norm": 0.18603543937206268,
      "learning_rate": 7.566089334548769e-05,
      "loss": 0.8311,
      "step": 10140
    },
    {
      "epoch": 4.6234476472598836,
      "grad_norm": 0.1680108904838562,
      "learning_rate": 7.520510483135826e-05,
      "loss": 0.859,
      "step": 10145
    },
    {
      "epoch": 4.6257263301811555,
      "grad_norm": 0.184977188706398,
      "learning_rate": 7.474931631722881e-05,
      "loss": 0.8312,
      "step": 10150
    },
    {
      "epoch": 4.628005013102427,
      "grad_norm": 0.17858120799064636,
      "learning_rate": 7.429352780309936e-05,
      "loss": 0.8081,
      "step": 10155
    },
    {
      "epoch": 4.630283696023699,
      "grad_norm": 0.2071787416934967,
      "learning_rate": 7.383773928896992e-05,
      "loss": 0.9005,
      "step": 10160
    },
    {
      "epoch": 4.63256237894497,
      "grad_norm": 0.17864938080310822,
      "learning_rate": 7.338195077484047e-05,
      "loss": 0.7993,
      "step": 10165
    },
    {
      "epoch": 4.634841061866242,
      "grad_norm": 0.17596809566020966,
      "learning_rate": 7.292616226071102e-05,
      "loss": 0.8549,
      "step": 10170
    },
    {
      "epoch": 4.637119744787513,
      "grad_norm": 0.18140828609466553,
      "learning_rate": 7.247037374658159e-05,
      "loss": 0.903,
      "step": 10175
    },
    {
      "epoch": 4.639398427708784,
      "grad_norm": 0.16755911707878113,
      "learning_rate": 7.201458523245215e-05,
      "loss": 0.7994,
      "step": 10180
    },
    {
      "epoch": 4.641677110630056,
      "grad_norm": 0.17691545188426971,
      "learning_rate": 7.15587967183227e-05,
      "loss": 0.817,
      "step": 10185
    },
    {
      "epoch": 4.643955793551328,
      "grad_norm": 0.19300591945648193,
      "learning_rate": 7.110300820419325e-05,
      "loss": 0.8275,
      "step": 10190
    },
    {
      "epoch": 4.646234476472599,
      "grad_norm": 0.17830288410186768,
      "learning_rate": 7.06472196900638e-05,
      "loss": 0.786,
      "step": 10195
    },
    {
      "epoch": 4.64851315939387,
      "grad_norm": 0.1734190732240677,
      "learning_rate": 7.019143117593436e-05,
      "loss": 0.8194,
      "step": 10200
    },
    {
      "epoch": 4.650791842315142,
      "grad_norm": 0.181319460272789,
      "learning_rate": 6.973564266180491e-05,
      "loss": 0.8499,
      "step": 10205
    },
    {
      "epoch": 4.653070525236413,
      "grad_norm": 0.2083442360162735,
      "learning_rate": 6.927985414767548e-05,
      "loss": 0.8144,
      "step": 10210
    },
    {
      "epoch": 4.655349208157685,
      "grad_norm": 0.19463296234607697,
      "learning_rate": 6.882406563354605e-05,
      "loss": 0.7984,
      "step": 10215
    },
    {
      "epoch": 4.657627891078956,
      "grad_norm": 0.19197028875350952,
      "learning_rate": 6.83682771194166e-05,
      "loss": 0.7823,
      "step": 10220
    },
    {
      "epoch": 4.659906574000228,
      "grad_norm": 0.18751037120819092,
      "learning_rate": 6.791248860528715e-05,
      "loss": 0.8083,
      "step": 10225
    },
    {
      "epoch": 4.662185256921499,
      "grad_norm": 0.19883166253566742,
      "learning_rate": 6.745670009115771e-05,
      "loss": 0.7827,
      "step": 10230
    },
    {
      "epoch": 4.664463939842771,
      "grad_norm": 0.18569867312908173,
      "learning_rate": 6.700091157702826e-05,
      "loss": 0.84,
      "step": 10235
    },
    {
      "epoch": 4.666742622764042,
      "grad_norm": 0.19154202938079834,
      "learning_rate": 6.654512306289881e-05,
      "loss": 0.8283,
      "step": 10240
    },
    {
      "epoch": 4.669021305685314,
      "grad_norm": 0.18431200087070465,
      "learning_rate": 6.608933454876938e-05,
      "loss": 0.8525,
      "step": 10245
    },
    {
      "epoch": 4.671299988606585,
      "grad_norm": 0.19071419537067413,
      "learning_rate": 6.563354603463993e-05,
      "loss": 0.8525,
      "step": 10250
    },
    {
      "epoch": 4.673578671527856,
      "grad_norm": 0.21122372150421143,
      "learning_rate": 6.517775752051049e-05,
      "loss": 0.8287,
      "step": 10255
    },
    {
      "epoch": 4.675857354449128,
      "grad_norm": 0.18204668164253235,
      "learning_rate": 6.472196900638104e-05,
      "loss": 0.829,
      "step": 10260
    },
    {
      "epoch": 4.6781360373704,
      "grad_norm": 0.19629186391830444,
      "learning_rate": 6.42661804922516e-05,
      "loss": 0.8305,
      "step": 10265
    },
    {
      "epoch": 4.6804147202916715,
      "grad_norm": 0.19567157328128815,
      "learning_rate": 6.381039197812215e-05,
      "loss": 0.8703,
      "step": 10270
    },
    {
      "epoch": 4.682693403212943,
      "grad_norm": 0.18306781351566315,
      "learning_rate": 6.33546034639927e-05,
      "loss": 0.8527,
      "step": 10275
    },
    {
      "epoch": 4.684972086134215,
      "grad_norm": 0.20590610802173615,
      "learning_rate": 6.289881494986327e-05,
      "loss": 0.7917,
      "step": 10280
    },
    {
      "epoch": 4.687250769055486,
      "grad_norm": 0.19598935544490814,
      "learning_rate": 6.244302643573382e-05,
      "loss": 0.8569,
      "step": 10285
    },
    {
      "epoch": 4.689529451976758,
      "grad_norm": 0.1926594376564026,
      "learning_rate": 6.198723792160438e-05,
      "loss": 0.8315,
      "step": 10290
    },
    {
      "epoch": 4.691808134898029,
      "grad_norm": 0.19779673218727112,
      "learning_rate": 6.153144940747493e-05,
      "loss": 0.8829,
      "step": 10295
    },
    {
      "epoch": 4.694086817819301,
      "grad_norm": 0.19464269280433655,
      "learning_rate": 6.107566089334548e-05,
      "loss": 0.8106,
      "step": 10300
    },
    {
      "epoch": 4.696365500740572,
      "grad_norm": 0.17617645859718323,
      "learning_rate": 6.061987237921605e-05,
      "loss": 0.8123,
      "step": 10305
    },
    {
      "epoch": 4.698644183661844,
      "grad_norm": 0.22727768123149872,
      "learning_rate": 6.01640838650866e-05,
      "loss": 0.8421,
      "step": 10310
    },
    {
      "epoch": 4.700922866583115,
      "grad_norm": 0.17559124529361725,
      "learning_rate": 5.9708295350957157e-05,
      "loss": 0.7783,
      "step": 10315
    },
    {
      "epoch": 4.703201549504387,
      "grad_norm": 0.19325780868530273,
      "learning_rate": 5.925250683682772e-05,
      "loss": 0.8555,
      "step": 10320
    },
    {
      "epoch": 4.705480232425658,
      "grad_norm": 0.1761147677898407,
      "learning_rate": 5.879671832269827e-05,
      "loss": 0.8366,
      "step": 10325
    },
    {
      "epoch": 4.707758915346929,
      "grad_norm": 0.17982342839241028,
      "learning_rate": 5.8340929808568824e-05,
      "loss": 0.7585,
      "step": 10330
    },
    {
      "epoch": 4.710037598268201,
      "grad_norm": 0.17203789949417114,
      "learning_rate": 5.788514129443938e-05,
      "loss": 0.8326,
      "step": 10335
    },
    {
      "epoch": 4.712316281189472,
      "grad_norm": 0.16661511361598969,
      "learning_rate": 5.742935278030994e-05,
      "loss": 0.8224,
      "step": 10340
    },
    {
      "epoch": 4.714594964110744,
      "grad_norm": 0.17619678378105164,
      "learning_rate": 5.697356426618049e-05,
      "loss": 0.7873,
      "step": 10345
    },
    {
      "epoch": 4.716873647032015,
      "grad_norm": 0.1666593998670578,
      "learning_rate": 5.6517775752051044e-05,
      "loss": 0.771,
      "step": 10350
    },
    {
      "epoch": 4.719152329953287,
      "grad_norm": 0.1927005797624588,
      "learning_rate": 5.6061987237921605e-05,
      "loss": 0.8178,
      "step": 10355
    },
    {
      "epoch": 4.721431012874558,
      "grad_norm": 0.18103308975696564,
      "learning_rate": 5.5606198723792165e-05,
      "loss": 0.8118,
      "step": 10360
    },
    {
      "epoch": 4.72370969579583,
      "grad_norm": 0.18032096326351166,
      "learning_rate": 5.515041020966272e-05,
      "loss": 0.8261,
      "step": 10365
    },
    {
      "epoch": 4.725988378717101,
      "grad_norm": 0.1923678070306778,
      "learning_rate": 5.469462169553328e-05,
      "loss": 0.8483,
      "step": 10370
    },
    {
      "epoch": 4.728267061638373,
      "grad_norm": 0.1650107204914093,
      "learning_rate": 5.423883318140383e-05,
      "loss": 0.8538,
      "step": 10375
    },
    {
      "epoch": 4.730545744559644,
      "grad_norm": 0.18184247612953186,
      "learning_rate": 5.3783044667274386e-05,
      "loss": 0.8245,
      "step": 10380
    },
    {
      "epoch": 4.732824427480916,
      "grad_norm": 0.19928117096424103,
      "learning_rate": 5.332725615314494e-05,
      "loss": 0.8731,
      "step": 10385
    },
    {
      "epoch": 4.7351031104021875,
      "grad_norm": 0.1807079315185547,
      "learning_rate": 5.28714676390155e-05,
      "loss": 0.8568,
      "step": 10390
    },
    {
      "epoch": 4.737381793323459,
      "grad_norm": 0.18701311945915222,
      "learning_rate": 5.241567912488605e-05,
      "loss": 0.8502,
      "step": 10395
    },
    {
      "epoch": 4.7396604762447305,
      "grad_norm": 0.18352331221103668,
      "learning_rate": 5.1959890610756606e-05,
      "loss": 0.8936,
      "step": 10400
    },
    {
      "epoch": 4.741939159166002,
      "grad_norm": 0.1809464693069458,
      "learning_rate": 5.1504102096627166e-05,
      "loss": 0.8619,
      "step": 10405
    },
    {
      "epoch": 4.744217842087274,
      "grad_norm": 0.16739191114902496,
      "learning_rate": 5.104831358249772e-05,
      "loss": 0.7728,
      "step": 10410
    },
    {
      "epoch": 4.746496525008545,
      "grad_norm": 0.1833992898464203,
      "learning_rate": 5.0592525068368273e-05,
      "loss": 0.7999,
      "step": 10415
    },
    {
      "epoch": 4.748775207929817,
      "grad_norm": 0.19519081711769104,
      "learning_rate": 5.013673655423884e-05,
      "loss": 0.8439,
      "step": 10420
    },
    {
      "epoch": 4.751053890851088,
      "grad_norm": 0.1757572889328003,
      "learning_rate": 4.9680948040109394e-05,
      "loss": 0.8016,
      "step": 10425
    },
    {
      "epoch": 4.75333257377236,
      "grad_norm": 0.19150720536708832,
      "learning_rate": 4.922515952597995e-05,
      "loss": 0.7972,
      "step": 10430
    },
    {
      "epoch": 4.755611256693631,
      "grad_norm": 0.1607058048248291,
      "learning_rate": 4.87693710118505e-05,
      "loss": 0.7946,
      "step": 10435
    },
    {
      "epoch": 4.757889939614903,
      "grad_norm": 0.1749904751777649,
      "learning_rate": 4.831358249772106e-05,
      "loss": 0.8516,
      "step": 10440
    },
    {
      "epoch": 4.760168622536174,
      "grad_norm": 0.16832660138607025,
      "learning_rate": 4.7857793983591615e-05,
      "loss": 0.8076,
      "step": 10445
    },
    {
      "epoch": 4.762447305457446,
      "grad_norm": 0.16453346610069275,
      "learning_rate": 4.740200546946217e-05,
      "loss": 0.8172,
      "step": 10450
    },
    {
      "epoch": 4.764725988378717,
      "grad_norm": 0.16757628321647644,
      "learning_rate": 4.694621695533273e-05,
      "loss": 0.8434,
      "step": 10455
    },
    {
      "epoch": 4.767004671299989,
      "grad_norm": 0.19327093660831451,
      "learning_rate": 4.649042844120328e-05,
      "loss": 0.8475,
      "step": 10460
    },
    {
      "epoch": 4.76928335422126,
      "grad_norm": 0.19048812985420227,
      "learning_rate": 4.6034639927073835e-05,
      "loss": 0.8719,
      "step": 10465
    },
    {
      "epoch": 4.771562037142532,
      "grad_norm": 0.19233687222003937,
      "learning_rate": 4.557885141294439e-05,
      "loss": 0.8002,
      "step": 10470
    },
    {
      "epoch": 4.773840720063803,
      "grad_norm": 0.18196560442447662,
      "learning_rate": 4.5123062898814956e-05,
      "loss": 0.7885,
      "step": 10475
    },
    {
      "epoch": 4.776119402985074,
      "grad_norm": 0.16699162125587463,
      "learning_rate": 4.466727438468551e-05,
      "loss": 0.8639,
      "step": 10480
    },
    {
      "epoch": 4.778398085906346,
      "grad_norm": 0.18181566894054413,
      "learning_rate": 4.421148587055606e-05,
      "loss": 0.8955,
      "step": 10485
    },
    {
      "epoch": 4.780676768827617,
      "grad_norm": 0.17770512402057648,
      "learning_rate": 4.375569735642662e-05,
      "loss": 0.8116,
      "step": 10490
    },
    {
      "epoch": 4.782955451748889,
      "grad_norm": 0.2011858969926834,
      "learning_rate": 4.3299908842297176e-05,
      "loss": 0.8162,
      "step": 10495
    },
    {
      "epoch": 4.78523413467016,
      "grad_norm": 0.18425360321998596,
      "learning_rate": 4.284412032816773e-05,
      "loss": 0.8437,
      "step": 10500
    },
    {
      "epoch": 4.787512817591432,
      "grad_norm": 0.15419547259807587,
      "learning_rate": 4.238833181403829e-05,
      "loss": 0.8402,
      "step": 10505
    },
    {
      "epoch": 4.789791500512703,
      "grad_norm": 0.18397247791290283,
      "learning_rate": 4.1932543299908844e-05,
      "loss": 0.8167,
      "step": 10510
    },
    {
      "epoch": 4.792070183433975,
      "grad_norm": 0.1653788685798645,
      "learning_rate": 4.14767547857794e-05,
      "loss": 0.7431,
      "step": 10515
    },
    {
      "epoch": 4.7943488663552465,
      "grad_norm": 0.1652180552482605,
      "learning_rate": 4.102096627164995e-05,
      "loss": 0.8892,
      "step": 10520
    },
    {
      "epoch": 4.7966275492765185,
      "grad_norm": 0.1904815137386322,
      "learning_rate": 4.056517775752051e-05,
      "loss": 0.8512,
      "step": 10525
    },
    {
      "epoch": 4.79890623219779,
      "grad_norm": 0.17694447934627533,
      "learning_rate": 4.0109389243391064e-05,
      "loss": 0.8112,
      "step": 10530
    },
    {
      "epoch": 4.8011849151190615,
      "grad_norm": 0.1902289092540741,
      "learning_rate": 3.9653600729261624e-05,
      "loss": 0.8319,
      "step": 10535
    },
    {
      "epoch": 4.803463598040333,
      "grad_norm": 0.1824997365474701,
      "learning_rate": 3.9197812215132185e-05,
      "loss": 0.8601,
      "step": 10540
    },
    {
      "epoch": 4.805742280961605,
      "grad_norm": 0.16243720054626465,
      "learning_rate": 3.874202370100274e-05,
      "loss": 0.8363,
      "step": 10545
    },
    {
      "epoch": 4.808020963882876,
      "grad_norm": 0.18771086633205414,
      "learning_rate": 3.828623518687329e-05,
      "loss": 0.8693,
      "step": 10550
    },
    {
      "epoch": 4.810299646804147,
      "grad_norm": 0.18158571422100067,
      "learning_rate": 3.7830446672743845e-05,
      "loss": 0.8636,
      "step": 10555
    },
    {
      "epoch": 4.812578329725419,
      "grad_norm": 0.18355689942836761,
      "learning_rate": 3.7374658158614405e-05,
      "loss": 0.833,
      "step": 10560
    },
    {
      "epoch": 4.81485701264669,
      "grad_norm": 0.17262579500675201,
      "learning_rate": 3.691886964448496e-05,
      "loss": 0.8696,
      "step": 10565
    },
    {
      "epoch": 4.817135695567962,
      "grad_norm": 0.18273082375526428,
      "learning_rate": 3.646308113035551e-05,
      "loss": 0.8602,
      "step": 10570
    },
    {
      "epoch": 4.819414378489233,
      "grad_norm": 0.17184078693389893,
      "learning_rate": 3.600729261622607e-05,
      "loss": 0.8165,
      "step": 10575
    },
    {
      "epoch": 4.821693061410505,
      "grad_norm": 0.1888938546180725,
      "learning_rate": 3.5551504102096626e-05,
      "loss": 0.8174,
      "step": 10580
    },
    {
      "epoch": 4.823971744331776,
      "grad_norm": 0.18209131062030792,
      "learning_rate": 3.509571558796718e-05,
      "loss": 0.8068,
      "step": 10585
    },
    {
      "epoch": 4.826250427253048,
      "grad_norm": 0.18976564705371857,
      "learning_rate": 3.463992707383774e-05,
      "loss": 0.8296,
      "step": 10590
    },
    {
      "epoch": 4.828529110174319,
      "grad_norm": 0.1745283007621765,
      "learning_rate": 3.41841385597083e-05,
      "loss": 0.8143,
      "step": 10595
    },
    {
      "epoch": 4.830807793095591,
      "grad_norm": 0.20261551439762115,
      "learning_rate": 3.3728350045578853e-05,
      "loss": 0.8458,
      "step": 10600
    },
    {
      "epoch": 4.833086476016862,
      "grad_norm": 0.18078888952732086,
      "learning_rate": 3.327256153144941e-05,
      "loss": 0.834,
      "step": 10605
    },
    {
      "epoch": 4.835365158938134,
      "grad_norm": 0.17768771946430206,
      "learning_rate": 3.281677301731997e-05,
      "loss": 0.8559,
      "step": 10610
    },
    {
      "epoch": 4.837643841859405,
      "grad_norm": 0.19946660101413727,
      "learning_rate": 3.236098450319052e-05,
      "loss": 0.8318,
      "step": 10615
    },
    {
      "epoch": 4.839922524780677,
      "grad_norm": 0.20275159180164337,
      "learning_rate": 3.1905195989061074e-05,
      "loss": 0.8766,
      "step": 10620
    },
    {
      "epoch": 4.842201207701948,
      "grad_norm": 0.19333268702030182,
      "learning_rate": 3.1449407474931634e-05,
      "loss": 0.8378,
      "step": 10625
    },
    {
      "epoch": 4.844479890623219,
      "grad_norm": 0.18250887095928192,
      "learning_rate": 3.099361896080219e-05,
      "loss": 0.8875,
      "step": 10630
    },
    {
      "epoch": 4.846758573544491,
      "grad_norm": 0.1868085414171219,
      "learning_rate": 3.053783044667274e-05,
      "loss": 0.8226,
      "step": 10635
    },
    {
      "epoch": 4.8490372564657624,
      "grad_norm": 0.18075305223464966,
      "learning_rate": 3.00820419325433e-05,
      "loss": 0.8043,
      "step": 10640
    },
    {
      "epoch": 4.851315939387034,
      "grad_norm": 0.1826295554637909,
      "learning_rate": 2.962625341841386e-05,
      "loss": 0.8741,
      "step": 10645
    },
    {
      "epoch": 4.8535946223083055,
      "grad_norm": 0.16613811254501343,
      "learning_rate": 2.9170464904284412e-05,
      "loss": 0.8043,
      "step": 10650
    },
    {
      "epoch": 4.8558733052295775,
      "grad_norm": 0.17268231511116028,
      "learning_rate": 2.871467639015497e-05,
      "loss": 0.7744,
      "step": 10655
    },
    {
      "epoch": 4.858151988150849,
      "grad_norm": 0.18848837912082672,
      "learning_rate": 2.8258887876025522e-05,
      "loss": 0.8462,
      "step": 10660
    },
    {
      "epoch": 4.860430671072121,
      "grad_norm": 0.18367135524749756,
      "learning_rate": 2.7803099361896082e-05,
      "loss": 0.818,
      "step": 10665
    },
    {
      "epoch": 4.862709353993392,
      "grad_norm": 0.19367250800132751,
      "learning_rate": 2.734731084776664e-05,
      "loss": 0.886,
      "step": 10670
    },
    {
      "epoch": 4.864988036914664,
      "grad_norm": 0.196439728140831,
      "learning_rate": 2.6891522333637193e-05,
      "loss": 0.8126,
      "step": 10675
    },
    {
      "epoch": 4.867266719835935,
      "grad_norm": 0.17785406112670898,
      "learning_rate": 2.643573381950775e-05,
      "loss": 0.7732,
      "step": 10680
    },
    {
      "epoch": 4.869545402757207,
      "grad_norm": 0.19337040185928345,
      "learning_rate": 2.5979945305378303e-05,
      "loss": 0.8471,
      "step": 10685
    },
    {
      "epoch": 4.871824085678478,
      "grad_norm": 0.19359618425369263,
      "learning_rate": 2.552415679124886e-05,
      "loss": 0.8134,
      "step": 10690
    },
    {
      "epoch": 4.87410276859975,
      "grad_norm": 0.1824466437101364,
      "learning_rate": 2.506836827711942e-05,
      "loss": 0.8777,
      "step": 10695
    },
    {
      "epoch": 4.876381451521021,
      "grad_norm": 0.1797725260257721,
      "learning_rate": 2.4612579762989974e-05,
      "loss": 0.8012,
      "step": 10700
    },
    {
      "epoch": 4.878660134442292,
      "grad_norm": 0.17753474414348602,
      "learning_rate": 2.415679124886053e-05,
      "loss": 0.8079,
      "step": 10705
    },
    {
      "epoch": 4.880938817363564,
      "grad_norm": 0.18086473643779755,
      "learning_rate": 2.3701002734731084e-05,
      "loss": 0.8382,
      "step": 10710
    },
    {
      "epoch": 4.883217500284835,
      "grad_norm": 0.191620334982872,
      "learning_rate": 2.324521422060164e-05,
      "loss": 0.7923,
      "step": 10715
    },
    {
      "epoch": 4.885496183206107,
      "grad_norm": 0.18055059015750885,
      "learning_rate": 2.2789425706472194e-05,
      "loss": 0.7672,
      "step": 10720
    },
    {
      "epoch": 4.887774866127378,
      "grad_norm": 0.18485866487026215,
      "learning_rate": 2.2333637192342755e-05,
      "loss": 0.7828,
      "step": 10725
    },
    {
      "epoch": 4.89005354904865,
      "grad_norm": 0.2228936105966568,
      "learning_rate": 2.187784867821331e-05,
      "loss": 0.866,
      "step": 10730
    },
    {
      "epoch": 4.892332231969921,
      "grad_norm": 0.19736209511756897,
      "learning_rate": 2.1422060164083865e-05,
      "loss": 0.8253,
      "step": 10735
    },
    {
      "epoch": 4.894610914891193,
      "grad_norm": 0.17617130279541016,
      "learning_rate": 2.0966271649954422e-05,
      "loss": 0.8268,
      "step": 10740
    },
    {
      "epoch": 4.896889597812464,
      "grad_norm": 0.20876391232013702,
      "learning_rate": 2.0510483135824975e-05,
      "loss": 0.8251,
      "step": 10745
    },
    {
      "epoch": 4.899168280733736,
      "grad_norm": 0.1791290044784546,
      "learning_rate": 2.0054694621695532e-05,
      "loss": 0.8635,
      "step": 10750
    },
    {
      "epoch": 4.901446963655007,
      "grad_norm": 0.1794680953025818,
      "learning_rate": 1.9598906107566092e-05,
      "loss": 0.7953,
      "step": 10755
    },
    {
      "epoch": 4.903725646576279,
      "grad_norm": 0.2138482928276062,
      "learning_rate": 1.9143117593436646e-05,
      "loss": 0.8572,
      "step": 10760
    },
    {
      "epoch": 4.90600432949755,
      "grad_norm": 0.19097377359867096,
      "learning_rate": 1.8687329079307203e-05,
      "loss": 0.8272,
      "step": 10765
    },
    {
      "epoch": 4.908283012418822,
      "grad_norm": 0.17957405745983124,
      "learning_rate": 1.8231540565177756e-05,
      "loss": 0.803,
      "step": 10770
    },
    {
      "epoch": 4.9105616953400935,
      "grad_norm": 0.19583602249622345,
      "learning_rate": 1.7775752051048313e-05,
      "loss": 0.8653,
      "step": 10775
    },
    {
      "epoch": 4.9128403782613645,
      "grad_norm": 0.17067211866378784,
      "learning_rate": 1.731996353691887e-05,
      "loss": 0.8416,
      "step": 10780
    },
    {
      "epoch": 4.9151190611826365,
      "grad_norm": 0.18744057416915894,
      "learning_rate": 1.6864175022789427e-05,
      "loss": 0.8023,
      "step": 10785
    },
    {
      "epoch": 4.917397744103908,
      "grad_norm": 0.1756388247013092,
      "learning_rate": 1.6408386508659984e-05,
      "loss": 0.8077,
      "step": 10790
    },
    {
      "epoch": 4.91967642702518,
      "grad_norm": 0.19470854103565216,
      "learning_rate": 1.5952597994530537e-05,
      "loss": 0.7785,
      "step": 10795
    },
    {
      "epoch": 4.921955109946451,
      "grad_norm": 0.19238117337226868,
      "learning_rate": 1.5496809480401094e-05,
      "loss": 0.825,
      "step": 10800
    },
    {
      "epoch": 4.924233792867723,
      "grad_norm": 0.16658160090446472,
      "learning_rate": 1.504102096627165e-05,
      "loss": 0.7569,
      "step": 10805
    },
    {
      "epoch": 4.926512475788994,
      "grad_norm": 0.16437645256519318,
      "learning_rate": 1.4585232452142206e-05,
      "loss": 0.8018,
      "step": 10810
    },
    {
      "epoch": 4.928791158710266,
      "grad_norm": 0.16457602381706238,
      "learning_rate": 1.4129443938012761e-05,
      "loss": 0.8546,
      "step": 10815
    },
    {
      "epoch": 4.931069841631537,
      "grad_norm": 0.21734020113945007,
      "learning_rate": 1.367365542388332e-05,
      "loss": 0.8265,
      "step": 10820
    },
    {
      "epoch": 4.933348524552809,
      "grad_norm": 0.18055862188339233,
      "learning_rate": 1.3217866909753875e-05,
      "loss": 0.8203,
      "step": 10825
    },
    {
      "epoch": 4.93562720747408,
      "grad_norm": 0.2037704586982727,
      "learning_rate": 1.276207839562443e-05,
      "loss": 0.8046,
      "step": 10830
    },
    {
      "epoch": 4.937905890395351,
      "grad_norm": 0.17599517107009888,
      "learning_rate": 1.2306289881494987e-05,
      "loss": 0.8273,
      "step": 10835
    },
    {
      "epoch": 4.940184573316623,
      "grad_norm": 0.19200168550014496,
      "learning_rate": 1.1850501367365542e-05,
      "loss": 0.8507,
      "step": 10840
    },
    {
      "epoch": 4.942463256237895,
      "grad_norm": 0.16561955213546753,
      "learning_rate": 1.1394712853236097e-05,
      "loss": 0.8514,
      "step": 10845
    },
    {
      "epoch": 4.944741939159166,
      "grad_norm": 0.1784801483154297,
      "learning_rate": 1.0938924339106656e-05,
      "loss": 0.8376,
      "step": 10850
    },
    {
      "epoch": 4.947020622080437,
      "grad_norm": 0.17233732342720032,
      "learning_rate": 1.0483135824977211e-05,
      "loss": 0.799,
      "step": 10855
    },
    {
      "epoch": 4.949299305001709,
      "grad_norm": 0.18520256876945496,
      "learning_rate": 1.0027347310847766e-05,
      "loss": 0.7932,
      "step": 10860
    },
    {
      "epoch": 4.95157798792298,
      "grad_norm": 0.1792483627796173,
      "learning_rate": 9.571558796718323e-06,
      "loss": 0.8176,
      "step": 10865
    },
    {
      "epoch": 4.953856670844252,
      "grad_norm": 0.173030287027359,
      "learning_rate": 9.115770282588878e-06,
      "loss": 0.8753,
      "step": 10870
    },
    {
      "epoch": 4.956135353765523,
      "grad_norm": 0.16925226151943207,
      "learning_rate": 8.659981768459435e-06,
      "loss": 0.8137,
      "step": 10875
    },
    {
      "epoch": 4.958414036686795,
      "grad_norm": 0.17893558740615845,
      "learning_rate": 8.204193254329992e-06,
      "loss": 0.807,
      "step": 10880
    },
    {
      "epoch": 4.960692719608066,
      "grad_norm": 0.19043362140655518,
      "learning_rate": 7.748404740200547e-06,
      "loss": 0.8108,
      "step": 10885
    },
    {
      "epoch": 4.962971402529338,
      "grad_norm": 0.17098551988601685,
      "learning_rate": 7.292616226071103e-06,
      "loss": 0.8231,
      "step": 10890
    },
    {
      "epoch": 4.965250085450609,
      "grad_norm": 0.19716201722621918,
      "learning_rate": 6.83682771194166e-06,
      "loss": 0.795,
      "step": 10895
    },
    {
      "epoch": 4.967528768371881,
      "grad_norm": 0.18908436596393585,
      "learning_rate": 6.381039197812215e-06,
      "loss": 0.8379,
      "step": 10900
    },
    {
      "epoch": 4.9698074512931525,
      "grad_norm": 0.20879265666007996,
      "learning_rate": 5.925250683682771e-06,
      "loss": 0.8534,
      "step": 10905
    },
    {
      "epoch": 4.972086134214424,
      "grad_norm": 0.1639297753572464,
      "learning_rate": 5.469462169553328e-06,
      "loss": 0.8299,
      "step": 10910
    },
    {
      "epoch": 4.974364817135696,
      "grad_norm": 0.19965295493602753,
      "learning_rate": 5.013673655423883e-06,
      "loss": 0.8074,
      "step": 10915
    },
    {
      "epoch": 4.9766435000569675,
      "grad_norm": 0.18139038980007172,
      "learning_rate": 4.557885141294439e-06,
      "loss": 0.8422,
      "step": 10920
    },
    {
      "epoch": 4.978922182978239,
      "grad_norm": 0.19232496619224548,
      "learning_rate": 4.102096627164996e-06,
      "loss": 0.8181,
      "step": 10925
    },
    {
      "epoch": 4.98120086589951,
      "grad_norm": 0.1665109246969223,
      "learning_rate": 3.6463081130355515e-06,
      "loss": 0.8121,
      "step": 10930
    },
    {
      "epoch": 4.983479548820782,
      "grad_norm": 0.19574657082557678,
      "learning_rate": 3.1905195989061075e-06,
      "loss": 0.8765,
      "step": 10935
    },
    {
      "epoch": 4.985758231742053,
      "grad_norm": 0.17163331806659698,
      "learning_rate": 2.734731084776664e-06,
      "loss": 0.8318,
      "step": 10940
    },
    {
      "epoch": 4.988036914663325,
      "grad_norm": 0.18075959384441376,
      "learning_rate": 2.2789425706472195e-06,
      "loss": 0.8302,
      "step": 10945
    },
    {
      "epoch": 4.990315597584596,
      "grad_norm": 0.16896826028823853,
      "learning_rate": 1.8231540565177757e-06,
      "loss": 0.8311,
      "step": 10950
    },
    {
      "epoch": 4.992594280505868,
      "grad_norm": 0.1740105301141739,
      "learning_rate": 1.367365542388332e-06,
      "loss": 0.8115,
      "step": 10955
    },
    {
      "epoch": 4.994872963427139,
      "grad_norm": 0.18940825760364532,
      "learning_rate": 9.115770282588879e-07,
      "loss": 0.7852,
      "step": 10960
    },
    {
      "epoch": 4.997151646348411,
      "grad_norm": 0.18168926239013672,
      "learning_rate": 4.5578851412944394e-07,
      "loss": 0.8034,
      "step": 10965
    },
    {
      "epoch": 4.999430329269682,
      "grad_norm": 0.1746973693370819,
      "learning_rate": 0.0,
      "loss": 0.9106,
      "step": 10970
    },
    {
      "epoch": 4.999430329269682,
      "eval_loss": 0.7422646284103394,
      "eval_runtime": 794.1528,
      "eval_samples_per_second": 25.26,
      "eval_steps_per_second": 3.158,
      "step": 10970
    }
  ],
  "logging_steps": 5,
  "max_steps": 10970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0220600466830131e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
