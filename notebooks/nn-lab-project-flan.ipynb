{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Installing Some required dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:31:30.309111Z",
     "iopub.status.busy": "2024-11-17T07:31:30.308649Z",
     "iopub.status.idle": "2024-11-17T07:31:45.669556Z",
     "shell.execute_reply": "2024-11-17T07:31:45.668414Z",
     "shell.execute_reply.started": "2024-11-17T07:31:30.309067Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_score --quiet\n",
    "# !pip install peft --quiet\n",
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing the required dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-17T07:31:45.672092Z",
     "iopub.status.busy": "2024-11-17T07:31:45.671725Z",
     "iopub.status.idle": "2024-11-17T07:32:20.162663Z",
     "shell.execute_reply": "2024-11-17T07:32:20.161835Z",
     "shell.execute_reply.started": "2024-11-17T07:31:45.672050Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import gc\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import dask.dataframe as dd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing the dataset and removing all the samples with output length of 256**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:20.164636Z",
     "iopub.status.busy": "2024-11-17T07:32:20.163860Z",
     "iopub.status.idle": "2024-11-17T07:32:27.249698Z",
     "shell.execute_reply": "2024-11-17T07:32:27.248320Z",
     "shell.execute_reply.started": "2024-11-17T07:32:20.164596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>What's the best on-line calender/scheduling/bo...</td>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>What is the most effective on-line calendar/sc...</td>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>What is the most efficient on-line calendar/sc...</td>\n",
       "      <td>Which on-line calendar/scheduling/booking syst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     s1  \\\n",
       "4779  What's the best on-line calender/scheduling/bo...   \n",
       "4783  Which on-line calendar/scheduling/booking syst...   \n",
       "4786  What is the most effective on-line calendar/sc...   \n",
       "4788  Which on-line calendar/scheduling/booking syst...   \n",
       "4789  What is the most efficient on-line calendar/sc...   \n",
       "\n",
       "                                                     s2  \n",
       "4779  Which on-line calendar/scheduling/booking syst...  \n",
       "4783  Which on-line calendar/scheduling/booking syst...  \n",
       "4786  Which on-line calendar/scheduling/booking syst...  \n",
       "4788  Which on-line calendar/scheduling/booking syst...  \n",
       "4789  Which on-line calendar/scheduling/booking syst...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dd.read_parquet(\"hf://datasets/sharad/chatgpt-paraphrases-simple/data/train-*-of-*.parquet\").compute()\n",
    "dataset = dataset[dataset['s2'].str.len() > 256]\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Splitting the dataset into train test and validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:27.382201Z",
     "iopub.status.busy": "2024-11-17T07:32:27.381464Z",
     "iopub.status.idle": "2024-11-17T07:32:27.484712Z",
     "shell.execute_reply": "2024-11-17T07:32:27.483768Z",
     "shell.execute_reply.started": "2024-11-17T07:32:27.382149Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 70209\n",
      "Validation set size: 20060\n",
      "Test set size: 10030\n"
     ]
    }
   ],
   "source": [
    "dataset.rename(columns={'s1': 'input', 's2': 'output'}, inplace=True)\n",
    "trainData, tempDF = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "valData, testData = train_test_split(tempDF, test_size=1/3, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(trainData)}\")\n",
    "print(f\"Validation set size: {len(valData)}\")\n",
    "print(f\"Test set size: {len(testData)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a model friendly transformers dataset dictionary for smooth training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:27.486327Z",
     "iopub.status.busy": "2024-11-17T07:32:27.485930Z",
     "iopub.status.idle": "2024-11-17T07:32:27.852051Z",
     "shell.execute_reply": "2024-11-17T07:32:27.851132Z",
     "shell.execute_reply.started": "2024-11-17T07:32:27.486287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 70209\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 20060\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'output'],\n",
       "        num_rows: 10030\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = Dataset.from_pandas(trainData.reset_index(drop=True))\n",
    "valData = Dataset.from_pandas(valData.reset_index(drop=True))\n",
    "testData = Dataset.from_pandas(testData.reset_index(drop=True))\n",
    "\n",
    "datasetDict = DatasetDict({\n",
    "    'train': trainData,\n",
    "    'validation': valData,\n",
    "    'test': testData\n",
    "})\n",
    "del trainData, valData, testData\n",
    "datasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing the model and the tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:27.853595Z",
     "iopub.status.busy": "2024-11-17T07:32:27.853282Z",
     "iopub.status.idle": "2024-11-17T07:32:34.949592Z",
     "shell.execute_reply": "2024-11-17T07:32:34.948645Z",
     "shell.execute_reply.started": "2024-11-17T07:32:27.853563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b272c02345646708f7090c30be46b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db29de8e9c4f4a128ee41457e020811f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450ad529c0ee4dfdbf358d799eb12c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e888359206814691be2a932964bd2b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3a0d9a1498416b8212265acd370a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33195c4c4df7422bb80ed0b5f67ac336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0951a074ea984808aaa6867784cef0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log ---------------- Model and tokenizer Loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"google/flan-t5-base\"\n",
    "def initModelAndTokenizer(modelID: str):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        modelID,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelID)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return model, tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseModel, tokenizer = initModelAndTokenizer(MODEL_ID)\n",
    "baseModel.to(device)\n",
    "print(\"Log ---------------- Model and tokenizer Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Defining a dataset preprocessor function that takes in input a sample of examples and outputs their `input_ids`, `attention_masks` and `labels`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:34.951839Z",
     "iopub.status.busy": "2024-11-17T07:32:34.951124Z",
     "iopub.status.idle": "2024-11-17T07:32:34.958903Z",
     "shell.execute_reply": "2024-11-17T07:32:34.958043Z",
     "shell.execute_reply.started": "2024-11-17T07:32:34.951787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenizeInputText(sample):\n",
    "    prompt = [\n",
    "        f'Paraphrase this sentence without changing its meaning: \\\"{inp}\\\"' for inp in sample[\"input\"]\n",
    "    ]\n",
    "    # print(prompt)\n",
    "    tokenizedSample = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=216,\n",
    "    )\n",
    "\n",
    "    sample[\"input_ids\"] = tokenizedSample[\"input_ids\"]\n",
    "    sample[\"attention_mask\"] = tokenizedSample[\"attention_mask\"]\n",
    "    # print(sample[\"output\"])\n",
    "    labels = tokenizer(\n",
    "        sample[\"output\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=216,\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    sample[\"labels\"] = labels\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Applying the tokenizer function in batches to exploit multiprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:32:34.960684Z",
     "iopub.status.busy": "2024-11-17T07:32:34.960276Z",
     "iopub.status.idle": "2024-11-17T07:33:34.476842Z",
     "shell.execute_reply": "2024-11-17T07:33:34.475909Z",
     "shell.execute_reply.started": "2024-11-17T07:32:34.960640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d251feab71cc4cd39964c619eaa732ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038fc4035e324b5cbd3865aab18d2b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20060 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06df25ee22784ac996f3e62c9ab41375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizedData = datasetDict.map(tokenizeInputText, batched=True)\n",
    "tokenizedData = tokenizedData.remove_columns(['input', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:33:34.482057Z",
     "iopub.status.busy": "2024-11-17T07:33:34.481689Z",
     "iopub.status.idle": "2024-11-17T07:33:34.496332Z",
     "shell.execute_reply": "2024-11-17T07:33:34.495156Z",
     "shell.execute_reply.started": "2024-11-17T07:33:34.482009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4734, 27111, 48, 7142, 406, 2839, 165, 2530, 10, 96, 7825, 986, 31842, 47, 2650, 6381, 17, 106, 31, 7, 2743, 21, 8, 1025, 97, 16, 6622, 6, 68, 8, 1357, 737, 31, 17, 726, 326, 38, 8, 372, 31, 7, 30552, 189, 18, 4687, 1992, 16, 8, 6552, 2009, 3679, 79, 163, 3, 28423, 3, 60, 5772, 257, 57, 578, 3, 9, 4784, 1288, 1750, 12, 1491, 7377, 12832, 277, 535, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "[3, 4868, 271, 7817, 38, 6381, 17, 106, 31, 7, 2743, 21, 8, 1025, 97, 16, 6622, 6, 13816, 31842, 31, 7, 20752, 47, 26684, 6, 28, 8, 372, 8619, 30552, 189, 16, 8, 6552, 2009, 11, 163, 3, 16217, 3, 60, 5772, 257, 788, 12, 70, 394, 1288, 1750, 145, 1491, 7377, 12832, 277, 5, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "Paraphrase this sentence without changing its meaning: \"Howard Kendall was named Everton's manager for the third time in 1997, but the decision didn't pay off as the team's seventeenth-place finish in the Premiership meant they only escaped relegation by having a superior goal difference to Bolton Wanderers.\"</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "t = 1452\n",
    "print(tokenizedData[\"train\"][t][\"input_ids\"])\n",
    "print()\n",
    "print(tokenizedData[\"train\"][t][\"labels\"])\n",
    "print()\n",
    "print(tokenizer.decode(tokenizedData[\"train\"][t][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:33:34.497830Z",
     "iopub.status.busy": "2024-11-17T07:33:34.497513Z",
     "iopub.status.idle": "2024-11-17T07:33:34.509427Z",
     "shell.execute_reply": "2024-11-17T07:33:34.508449Z",
     "shell.execute_reply.started": "2024-11-17T07:33:34.497795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def trainableParams(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:38:02.616602Z",
     "iopub.status.busy": "2024-11-17T07:38:02.615623Z",
     "iopub.status.idle": "2024-11-17T07:38:04.298729Z",
     "shell.execute_reply": "2024-11-17T07:38:04.297788Z",
     "shell.execute_reply.started": "2024-11-17T07:38:02.616527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OUPUT_DIR = f'/kaggle/working/flan-peft-train-V3'\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "loraConfig = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=['q', 'v'],\n",
    "    lora_dropout=DROPOUT_RATE,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "peftModel = get_peft_model(baseModel, loraConfig)\n",
    "\n",
    "trainArgs = TrainingArguments(\n",
    "    output_dir=OUPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    logging_strategy=\"steps\", \n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peftModel,\n",
    "    args=trainArgs,\n",
    "    train_dataset=tokenizedData['train'],\n",
    "    eval_dataset=tokenizedData['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:38:07.222162Z",
     "iopub.status.busy": "2024-11-17T07:38:07.221725Z",
     "iopub.status.idle": "2024-11-17T07:38:07.233322Z",
     "shell.execute_reply": "2024-11-17T07:38:07.232360Z",
     "shell.execute_reply.started": "2024-11-17T07:38:07.222124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 1769472\n",
      "all model parameters: 249347328\n",
      "percentage of trainable model parameters: 0.71%\n"
     ]
    }
   ],
   "source": [
    "print(trainableParams(peftModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T07:38:16.015907Z",
     "iopub.status.busy": "2024-11-17T07:38:16.015065Z",
     "iopub.status.idle": "2024-11-17T16:12:42.503733Z",
     "shell.execute_reply": "2024-11-17T16:12:42.502778Z",
     "shell.execute_reply.started": "2024-11-17T07:38:16.015869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2ff57bea4d4438aacaa2514b736707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112971711109922, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20241117_073823-rustszei</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cosmickay/huggingface/runs/rustszei' target=\"_blank\">/kaggle/working/flan-peft-train-V3</a></strong> to <a href='https://wandb.ai/cosmickay/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cosmickay/huggingface' target=\"_blank\">https://wandb.ai/cosmickay/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cosmickay/huggingface/runs/rustszei' target=\"_blank\">https://wandb.ai/cosmickay/huggingface/runs/rustszei</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10970' max='10970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10970/10970 8:34:09, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>0.802390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>0.771587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.754733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.910600</td>\n",
       "      <td>0.742265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 30864.1301, 'train_samples_per_second': 11.374, 'train_steps_per_second': 0.355, 'total_flos': 1.0220600466830131e+17, 'train_loss': 0.8701929144350745, 'epoch': 4.999430329269682}\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# 0df9404f6d8f654feeb2066151fcad924d9d4363\n",
    "trainStats = trainer.train()\n",
    "print(trainStats.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:37:20.823651Z",
     "iopub.status.busy": "2024-11-17T16:37:20.822885Z",
     "iopub.status.idle": "2024-11-17T16:37:23.072361Z",
     "shell.execute_reply": "2024-11-17T16:37:23.071238Z",
     "shell.execute_reply.started": "2024-11-17T16:37:20.823604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "PEFT_MODEL_ID = \"/kaggle/working/flan-peft-train-V3/checkpoint-10970\"\n",
    "baseModelImport = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "peftModelTest = PeftModel.from_pretrained(baseModelImport,PEFT_MODEL_ID,is_trainable=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:38:42.968217Z",
     "iopub.status.busy": "2024-11-17T16:38:42.967459Z",
     "iopub.status.idle": "2024-11-17T16:38:48.405667Z",
     "shell.execute_reply": "2024-11-17T16:38:48.404714Z",
     "shell.execute_reply.started": "2024-11-17T16:38:42.968175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4734, 27111,    48,  7142,   406,  2839,   165,  2530,    10,    96,\n",
      "         11889,    15,    15,  9351,     7,    43,   118,  4792,    11,     3,\n",
      "         14903,    72,  5830,  7532,    16, 10748,  6032,   437,  1671,  2628,\n",
      "            28, 15721,     7, 11214,     3, 23606,   844,    95,    12,  1283,\n",
      "         20325,    41,  1828,  2286,    61,  1096,  9351,  9964,    11,     3,\n",
      "          3131,  8640,     6,  2313,  9351,     7,    16,  5129,     6,     8,\n",
      "           934,   243,   535,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1]], device='cuda:0')\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "tensor([[    0,    37,   934,  4466,    24, 10748,  6032,    30,  3352,    43,\n",
      "           741,    15,    26,    16,     8, 14319,    13,   386,  9351,     7,\n",
      "            11,     8,  2261,  5157,    13,     3, 14903,    72,     6,    28,\n",
      "         15721,     7, 10849,     3, 23606,   844,    95,    12,  1283, 20325,\n",
      "            41,  1828,  2286,    61,  1096,  9351,  9964,    11,     3,  3131,\n",
      "          8640,     6,  2313,  9351,     7,    16,  5129,     5,    37,   934,\n",
      "            92,  4466,    24, 15721,     7,    43,   118, 10849,     3, 23606,\n",
      "           844,    95,    12,  1283, 20325,    41,  1828,  2286,    61,  1096,\n",
      "          9351,  9964,     5,    37,   934,    92,  4466,    24,     8,  9351,\n",
      "           789,    65,   118,     3,  6319,    12,  1535,     8, 10748,     7,\n",
      "             5,     1]], device='cuda:0')\n",
      "Prompt-------------\n",
      "Paraphrase this sentence without changing its meaning: \"Three Israelis have been killed and dozens more seriously injured in Palestinian attacks since November 2008 with rockets striking populated areas up to 40 kilometers (25 miles) inside Israeli territory and putting 800,000 Israelis in danger, the report said.\"\n",
      "\n",
      "Model Output-------\n",
      "The report noted that Palestinian attacks on Israel have resulted in the deaths of three Israelis and the serious injuries of dozens more, with rockets hitting populated areas up to 40 kilometers (25 miles) inside Israeli territory and putting 800,000 Israelis in danger. The report also noted that rockets have been hitting populated areas up to 40 kilometers (25 miles) inside Israeli territory. The report also noted that the Israeli government has been unable to reach the Palestinians.\n"
     ]
    }
   ],
   "source": [
    "evalIndex = 13\n",
    "inputIds = torch.stack([torch.tensor(ids) for ids in tokenizedData[\"test\"][\"input_ids\"][evalIndex:evalIndex+1]]).to(device)\n",
    "attentionMask = torch.stack([torch.tensor(mask) for mask in tokenizedData[\"test\"][\"attention_mask\"][evalIndex:evalIndex+1]]).to(device)\n",
    "print(inputIds)\n",
    "print(attentionMask)\n",
    "peftModel.eval()\n",
    "\n",
    "outputs = peftModelTest.generate(input_ids=inputIds,\n",
    "                             max_new_tokens=216,\n",
    "                             temperature=0.8,\n",
    "                             attention_mask=attentionMask,\n",
    "                             pad_token_id=tokenizer.pad_token_id,\n",
    "                             top_k=50,\n",
    "                             top_p=0.9,\n",
    "\n",
    "                            )\n",
    "print(outputs)\n",
    "textedOutput = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt-------------\")\n",
    "print(tokenizer.decode(tokenizedData[\"test\"][evalIndex][\"input_ids\"], skip_special_tokens=True))\n",
    "print(\"\\nModel Output-------\")\n",
    "print(textedOutput)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
